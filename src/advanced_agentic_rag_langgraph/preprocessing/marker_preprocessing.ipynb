{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marker PDF Preprocessing for RAG Pipeline\n",
    "\n",
    "**Purpose**: Process PDFs with GPU-accelerated Marker for improved table/figure extraction.\n",
    "\n",
    "**Workflow**:\n",
    "1. Upload PDFs to Colab\n",
    "2. Process with Marker (GPU) -> Markdown with tables/figures\n",
    "3. Chunk markdown for RAG\n",
    "4. Export as JSON\n",
    "5. Download for local use\n",
    "\n",
    "**Why Marker over PyMuPDF?**\n",
    "- Table extraction: 0% (PyMuPDF) -> ~96% (Marker)\n",
    "- Figure extraction with captions\n",
    "- Layout-aware processing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install marker-pdf and langchain text splitter (takes ~2-3 minutes)\n!pip install -q marker-pdf>=1.10.0 langchain-text-splitters"
  },
  {
   "cell_type": "code",
   "source": "# Setup OpenAI for Vision LLM (figure descriptions)\n# Add your API key to Colab secrets: Settings > Secrets > Add OPENAI_API_KEY\n!pip install -q openai\n\nimport openai\nfrom google.colab import userdata\n\ntry:\n    openai.api_key = userdata.get('OPENAI_API_KEY')\n    print(\"OpenAI API key loaded from Colab secrets\")\nexcept Exception as e:\n    print(f\"Warning: Could not load OPENAI_API_KEY from secrets: {e}\")\n    print(\"Figure descriptions will be skipped. Add key in Settings > Secrets\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Optional\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input directory\n",
    "INPUT_DIR = Path(\"/content/pdfs\")\n",
    "OUTPUT_DIR = Path(\"/content/output\")\n",
    "INPUT_DIR.mkdir(exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Upload PDFs to: {INPUT_DIR}\")\n",
    "print(f\"Output will be saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload PDFs (interactive file picker)\n",
    "print(\"Select PDF files to upload...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Move uploaded files to input directory\n",
    "for filename, content in uploaded.items():\n",
    "    if filename.lower().endswith('.pdf'):\n",
    "        filepath = INPUT_DIR / filename\n",
    "        with open(filepath, 'wb') as f:\n",
    "            f.write(content)\n",
    "        print(f\"Saved: {filepath}\")\n",
    "    else:\n",
    "        print(f\"Skipped (not PDF): {filename}\")\n",
    "\n",
    "# List uploaded PDFs\n",
    "pdf_files = list(INPUT_DIR.glob(\"*.pdf\"))\n",
    "print(f\"\\nTotal PDFs ready for processing: {len(pdf_files)}\")\n",
    "for pdf in pdf_files:\n",
    "    print(f\"  - {pdf.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Marker Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data classes for structured output\n",
    "@dataclass\n",
    "class TableInfo:\n",
    "    \"\"\"Extracted table information.\"\"\"\n",
    "    table_id: str\n",
    "    headers: list\n",
    "    rows: list\n",
    "    markdown: str\n",
    "\n",
    "@dataclass\n",
    "class FigureInfo:\n",
    "    \"\"\"Extracted figure information.\"\"\"\n",
    "    figure_id: str\n",
    "    image_key: str\n",
    "    caption: str\n",
    "    alt_text: str\n",
    "    context_before: str = \"\"\n",
    "    context_after: str = \"\"\n",
    "\n",
    "@dataclass\n",
    "class ChunkInfo:\n",
    "    \"\"\"Chunk with metadata.\"\"\"\n",
    "    id: str\n",
    "    content: str\n",
    "    metadata: dict\n",
    "\n",
    "@dataclass\n",
    "class ProcessedDocument:\n",
    "    \"\"\"Full processed document output.\"\"\"\n",
    "    source: str\n",
    "    processor: str\n",
    "    processed_date: str\n",
    "    markdown: str\n",
    "    chunks: list\n",
    "    tables: list\n",
    "    figures: list\n",
    "    stats: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables(markdown: str) -> list:\n",
    "    \"\"\"Extract structured table information from markdown.\"\"\"\n",
    "    tables = []\n",
    "    # Pattern for markdown tables: header row, separator row, data rows\n",
    "    table_pattern = r\"(\\|[^\\n]+\\|\\n)(\\|[-:\\s|]+\\|\\n)((?:\\|[^\\n]+\\|\\n?)+)\"\n",
    "\n",
    "    for i, match in enumerate(re.finditer(table_pattern, markdown)):\n",
    "        try:\n",
    "            header_row = match.group(1).strip()\n",
    "            headers = [c.strip() for c in header_row.split(\"|\")[1:-1]]\n",
    "\n",
    "            data_section = match.group(3).strip()\n",
    "            rows = []\n",
    "            for row_line in data_section.split(\"\\n\"):\n",
    "                if row_line.strip():\n",
    "                    cells = [c.strip() for c in row_line.split(\"|\")[1:-1]]\n",
    "                    if cells:\n",
    "                        rows.append(cells)\n",
    "\n",
    "            tables.append(TableInfo(\n",
    "                table_id=f\"table_{i + 1}\",\n",
    "                headers=headers,\n",
    "                rows=rows,\n",
    "                markdown=match.group(0),\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to parse table {i + 1}: {e}\")\n",
    "\n",
    "    return tables\n",
    "\n",
    "\n",
    "def extract_figures(markdown: str) -> list:\n",
    "    \"\"\"Extract figure information with captions from markdown.\"\"\"\n",
    "    figures = []\n",
    "    # Pattern for image references with optional caption\n",
    "    figure_pattern = r\"!\\[([^\\]]*)\\]\\(([^)]+)\\)(?:\\s*\\n\\s*(?:\\*\\*)?(?:Figure\\s*\\d+[.:]?)?\\s*([^\\n]+)(?:\\*\\*)?)?\"\n",
    "\n",
    "    for i, match in enumerate(re.finditer(figure_pattern, markdown, re.IGNORECASE)):\n",
    "        alt_text = match.group(1) or \"\"\n",
    "        image_path = match.group(2) or \"\"\n",
    "        caption = match.group(3) or alt_text\n",
    "\n",
    "        # Get surrounding context\n",
    "        start = max(0, match.start() - 150)\n",
    "        end = min(len(markdown), match.end() + 150)\n",
    "        context_before = markdown[start:match.start()].split(\"\\n\")[-1].strip()\n",
    "        context_after = markdown[match.end():end].split(\"\\n\")[0].strip()\n",
    "\n",
    "        figures.append(FigureInfo(\n",
    "            figure_id=f\"figure_{i + 1}\",\n",
    "            image_key=image_path,\n",
    "            caption=caption.strip() if caption else \"\",\n",
    "            alt_text=alt_text,\n",
    "            context_before=context_before,\n",
    "            context_after=context_after,\n",
    "        ))\n",
    "\n",
    "    return figures\n",
    "\n",
    "print(\"Extraction functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "import base64\nimport io\nimport time\n\ndef describe_figure(image, caption: str = \"\", max_retries: int = 3) -> str:\n    \"\"\"\n    Generate description of figure using GPT-4o-mini vision.\n\n    Args:\n        image: PIL Image object from Marker output\n        caption: Figure caption for context\n        max_retries: Maximum retry attempts for rate limit errors\n\n    Returns:\n        AI-generated description of the visual content\n    \"\"\"\n    if not openai.api_key:\n        return \"\"  # Skip if no API key\n\n    # Convert PIL Image to bytes (do this once, outside retry loop)\n    buffer = io.BytesIO()\n    image.save(buffer, format='PNG')\n    image_bytes = buffer.getvalue()\n    base64_image = base64.b64encode(image_bytes).decode('utf-8')\n\n    for attempt in range(max_retries):\n        try:\n            response = openai.chat.completions.create(\n                model=\"gpt-4o-mini\",\n                messages=[{\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": f\"Describe this academic figure in 2-3 sentences. Include: axes labels, data trends, key findings. Caption context: {caption}\"\n                        },\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"}\n                        }\n                    ]\n                }],\n                max_tokens=150\n            )\n            return response.choices[0].message.content.strip()\n\n        except openai.RateLimitError as e:\n            # Exponential backoff: 1s, 2s, 4s\n            wait_time = 2 ** attempt\n            print(f\"  Rate limit hit, retrying in {wait_time}s... (attempt {attempt + 1}/{max_retries})\")\n            time.sleep(wait_time)\n\n        except Exception as e:\n            print(f\"  Warning: Vision API failed: {e}\")\n            return \"\"\n\n    print(f\"  Warning: Max retries ({max_retries}) exceeded for vision API\")\n    return \"\"\n\nprint(\"Vision description function defined (GPT-4o-mini with retry logic).\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_text_splitters import RecursiveCharacterTextSplitter\n\ndef create_text_splitter(chunk_size: int = 1000, chunk_overlap: int = 200):\n    \"\"\"Create markdown-aware text splitter with proper overlap.\"\"\"\n    return RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,  # 20% overlap - matches local marker_processor.py\n        separators=[\n            \"\\n## \",   # H2 headers\n            \"\\n### \",  # H3 headers\n            \"\\n#### \", # H4 headers\n            \"\\n\\n\",    # Paragraphs\n            \"\\n\",      # Lines\n            \". \",      # Sentences\n            \" \",       # Words\n            \"\",\n        ],\n    )\n\ndef chunk_markdown(\n    markdown: str,\n    source: str,\n    chunk_size: int = 1000,\n    chunk_overlap: int = 200,\n) -> list:\n    \"\"\"\n    Chunk markdown text using LangChain's RecursiveCharacterTextSplitter.\n    \n    Uses markdown-aware separators and proper overlap between chunks.\n    Default: 1000 chars with 200 char overlap (20%).\n    \"\"\"\n    splitter = create_text_splitter(chunk_size, chunk_overlap)\n    raw_chunks = splitter.split_text(markdown)\n\n    # Create ChunkInfo objects with metadata\n    chunks = []\n    for i, content in enumerate(raw_chunks):\n        # Detect if chunk contains table or figure\n        has_table = bool(re.search(r\"\\|[^|]+\\|.*\\n\\|[-:]+\\|\", content))\n        has_figure = bool(re.search(r\"!\\[[^\\]]*\\]\\([^)]+\\)\", content))\n\n        chunk = ChunkInfo(\n            id=f\"{source}_chunk_{i}\",\n            content=content.strip(),\n            metadata={\n                \"chunk_index\": i,\n                \"content_type\": \"text\",\n                \"has_table\": has_table,\n                \"has_figure\": has_figure,\n                \"char_count\": len(content),\n            }\n        )\n        chunks.append(chunk)\n\n    return chunks\n\nprint(\"Chunking function defined (using RecursiveCharacterTextSplitter with 20% overlap).\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Marker converter (loads models - takes ~1-2 minutes first time)\n",
    "print(\"Loading Marker models (this may take 1-2 minutes)...\")\n",
    "\n",
    "from marker.converters.pdf import PdfConverter\n",
    "from marker.models import create_model_dict\n",
    "from marker.output import text_from_rendered\n",
    "\n",
    "# Create model dict with GPU\n",
    "model_dict = create_model_dict(device=\"cuda\")\n",
    "converter = PdfConverter(artifact_dict=model_dict)\n",
    "\n",
    "print(\"Marker models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def process_pdf(pdf_path: Path, converter) -> ProcessedDocument:\n    \"\"\"\n    Process a single PDF with Marker and return structured output.\n    Includes Vision LLM descriptions for figures.\n    \"\"\"\n    print(f\"\\nProcessing: {pdf_path.name}\")\n    start_time = datetime.now()\n\n    # Convert PDF to markdown\n    rendered = converter(str(pdf_path))\n    markdown_text, _, images = text_from_rendered(rendered)\n\n    # Get native image count from Marker (more accurate than regex)\n    native_image_count = len(images) if images else 0\n\n    # Extract structured elements (for creating dedicated chunks)\n    tables = extract_tables(markdown_text)\n    figures = extract_figures(markdown_text)  # For figure chunks with context\n\n    # Chunk the markdown\n    chunks = chunk_markdown(markdown_text, pdf_path.name)\n\n    # Add dedicated table chunks\n    for table in tables:\n        table_chunk = ChunkInfo(\n            id=f\"{pdf_path.name}_{table.table_id}\",\n            content=table.markdown,\n            metadata={\n                \"content_type\": \"table\",\n                \"table_id\": table.table_id,\n                \"headers\": table.headers,\n                \"row_count\": len(table.rows),\n            }\n        )\n        chunks.append(table_chunk)\n\n    # Add dedicated figure chunks with Vision LLM descriptions\n    figures_described = 0\n    for figure in figures:\n        # Try to get vision description if image available\n        vision_description = \"\"\n        if images and figure.image_key in images:\n            image_bytes = images[figure.image_key]\n            vision_description = describe_figure(image_bytes, figure.caption)\n            if vision_description:\n                figures_described += 1\n\n        # Build figure chunk content\n        figure_text = \"\"\n        if figure.context_before:\n            figure_text += f\"{figure.context_before}\\n\\n\"\n        figure_text += f\"Figure: {figure.caption}\"\n        if vision_description:\n            figure_text += f\"\\n\\nVisual Description: {vision_description}\"\n        if figure.context_after:\n            figure_text += f\"\\n\\n{figure.context_after}\"\n\n        figure_chunk = ChunkInfo(\n            id=f\"{pdf_path.name}_{figure.figure_id}\",\n            content=figure_text,\n            metadata={\n                \"content_type\": \"figure\",\n                \"figure_id\": figure.figure_id,\n                \"caption\": figure.caption,\n                \"image_key\": figure.image_key,\n                \"has_vision_description\": bool(vision_description),\n            }\n        )\n        chunks.append(figure_chunk)\n\n    elapsed = (datetime.now() - start_time).total_seconds()\n\n    # Build output\n    doc = ProcessedDocument(\n        source=pdf_path.name,\n        processor=\"marker\",\n        processed_date=datetime.now().isoformat(),\n        markdown=markdown_text,\n        chunks=[asdict(c) for c in chunks],\n        tables=[asdict(t) for t in tables],\n        figures=[asdict(f) for f in figures],\n        stats={\n            \"total_chunks\": len(chunks),\n            \"text_chunks\": sum(1 for c in chunks if c.metadata.get(\"content_type\") == \"text\"),\n            \"tables_extracted\": len(tables),\n            \"images_extracted\": native_image_count,  # Native Marker count\n            \"figure_chunks_created\": len(figures),   # Regex-based chunks\n            \"figures_with_vision\": figures_described,  # Vision LLM descriptions\n            \"markdown_chars\": len(markdown_text),\n            \"processing_time_seconds\": round(elapsed, 2),\n        }\n    )\n\n    print(f\"  Completed in {elapsed:.1f}s\")\n    print(f\"  - {len(chunks)} total chunks\")\n    print(f\"  - {len(tables)} tables extracted\")\n    print(f\"  - {native_image_count} images extracted (Marker native)\")\n    print(f\"  - {figures_described}/{len(figures)} figures with vision descriptions\")\n\n    return doc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all uploaded PDFs\n",
    "processed_docs = []\n",
    "\n",
    "print(f\"Processing {len(pdf_files)} PDF(s)...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for pdf_path in pdf_files:\n",
    "    try:\n",
    "        doc = process_pdf(pdf_path, converter)\n",
    "        processed_docs.append(doc)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing {pdf_path.name}: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"Processed {len(processed_docs)}/{len(pdf_files)} PDFs successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Export each document as JSON\nfor doc in processed_docs:\n    output_file = OUTPUT_DIR / f\"{Path(doc.source).stem}_processed.json\"\n    with open(output_file, 'w', encoding='utf-8') as f:\n        json.dump(asdict(doc), f, indent=2, ensure_ascii=False)\n    print(f\"Saved: {output_file}\")\n\n# Also create a combined output file\ncombined_output = {\n    \"processed_date\": datetime.now().isoformat(),\n    \"processor\": \"marker\",\n    \"total_documents\": len(processed_docs),\n    \"documents\": [asdict(doc) for doc in processed_docs],\n    \"summary\": {\n        \"total_chunks\": sum(doc.stats[\"total_chunks\"] for doc in processed_docs),\n        \"total_tables\": sum(doc.stats[\"tables_extracted\"] for doc in processed_docs),\n        \"total_images\": sum(doc.stats[\"images_extracted\"] for doc in processed_docs),\n    }\n}\n\ncombined_file = OUTPUT_DIR / \"all_documents_processed.json\"\nwith open(combined_file, 'w', encoding='utf-8') as f:\n    json.dump(combined_output, f, indent=2, ensure_ascii=False)\n\nprint(f\"\\nCombined output saved: {combined_file}\")\nprint(f\"\\nSummary:\")\nprint(f\"  Total chunks: {combined_output['summary']['total_chunks']}\")\nprint(f\"  Total tables: {combined_output['summary']['total_tables']}\")\nprint(f\"  Total images: {combined_output['summary']['total_images']} (Marker native count)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download individual files\n",
    "print(\"Downloading JSON files...\\n\")\n",
    "\n",
    "for json_file in OUTPUT_DIR.glob(\"*.json\"):\n",
    "    print(f\"Downloading: {json_file.name}\")\n",
    "    files.download(str(json_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Download as ZIP\n",
    "import shutil\n",
    "\n",
    "zip_path = \"/content/marker_processed_output\"\n",
    "shutil.make_archive(zip_path, 'zip', OUTPUT_DIR)\n",
    "\n",
    "print(f\"Created: {zip_path}.zip\")\n",
    "files.download(f\"{zip_path}.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Preview Sample Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview first document's tables\n",
    "if processed_docs:\n",
    "    doc = processed_docs[0]\n",
    "    print(f\"\\n=== Tables from {doc.source} ===\")\n",
    "    for table in doc.tables[:3]:  # Show first 3 tables\n",
    "        print(f\"\\n{table['table_id']}:\")\n",
    "        print(f\"Headers: {table['headers']}\")\n",
    "        print(f\"Rows: {len(table['rows'])}\")\n",
    "        print(table['markdown'][:500] + \"...\" if len(table['markdown']) > 500 else table['markdown'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview first few chunks\n",
    "if processed_docs:\n",
    "    doc = processed_docs[0]\n",
    "    print(f\"\\n=== Sample Chunks from {doc.source} ===\")\n",
    "    for chunk in doc.chunks[:3]:\n",
    "        print(f\"\\n--- {chunk['id']} ---\")\n",
    "        print(f\"Type: {chunk['metadata'].get('content_type', 'text')}\")\n",
    "        print(f\"Content preview: {chunk['content'][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Usage Instructions\n",
    "\n",
    "1. **Run all cells** in order\n",
    "2. **Upload your PDFs** when prompted in cell 4\n",
    "3. **Wait for processing** (typically 1-3 minutes per PDF with GPU)\n",
    "4. **Download the JSON files** from the download cells\n",
    "5. **Place JSON files** in your local `data/preprocessed/` folder\n",
    "\n",
    "### Local Loading (Future Integration)\n",
    "\n",
    "```python\n",
    "import json\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def load_preprocessed_chunks(json_path: str) -> list[Document]:\n",
    "    with open(json_path) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    documents = []\n",
    "    for chunk in data['chunks']:\n",
    "        doc = Document(\n",
    "            page_content=chunk['content'],\n",
    "            metadata={\n",
    "                'id': chunk['id'],\n",
    "                'source': data['source'],\n",
    "                'processor': 'marker',\n",
    "                **chunk['metadata']\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    return documents\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}