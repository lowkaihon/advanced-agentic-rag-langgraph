{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-1-intro",
   "metadata": {},
   "source": [
    "# Advanced Agentic RAG with LangGraph\n",
    "\n",
    "**A portfolio project showcasing intelligent, adaptive retrieval pipelines**\n",
    "\n",
    "This system demonstrates production-grade RAG architecture patterns:\n",
    "\n",
    "- **Dynamic strategy selection** - Semantic, keyword, or hybrid retrieval based on query analysis\n",
    "- **Quality-driven self-correction** - Automatic query rewrites when retrieval quality is insufficient\n",
    "- **Multi-stage reranking** - CrossEncoder (top-10) + LLM-as-judge (top-4) for precision\n",
    "- **HHEM-based hallucination detection** - Claim decomposition with per-chunk HHEM-2.1-Open verification\n",
    "- **Multi-agent parallel retrieval** - Query decomposition with parallel workers for complex questions\n",
    "\n",
    "**Architecture**: 7-node StateGraph with distributed intelligence (no central orchestrator)  \n",
    "**Framework**: LangChain 1.0 & LangGraph 1.0  \n",
    "**Pattern**: Dynamic Planning and Execution Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2-setup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T07:55:37.058491Z",
     "iopub.status.busy": "2025-11-27T07:55:37.058491Z",
     "iopub.status.idle": "2025-11-27T07:56:34.893409Z",
     "shell.execute_reply": "2025-11-27T07:56:34.891880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG variants loaded:\n",
      "  - basic_rag_graph: Simplest RAG (semantic search only)\n",
      "  - intermediate_rag_graph: Query expansion + hybrid + reranking\n",
      "  - advanced_rag_graph: Full agentic RAG with self-correction\n",
      "  - multi_agent_rag_graph: Parallel retrieval workers\n"
     ]
    }
   ],
   "source": [
    "# Setup & Imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# All RAG graph variants\n",
    "from advanced_agentic_rag_langgraph.variants import (\n",
    "    basic_rag_graph,\n",
    "    intermediate_rag_graph,\n",
    "    advanced_rag_graph,\n",
    "    multi_agent_rag_graph,\n",
    ")\n",
    "\n",
    "print(\"RAG variants loaded:\")\n",
    "print(\"  - basic_rag_graph: Simplest RAG (semantic search only)\")\n",
    "print(\"  - intermediate_rag_graph: Query expansion + hybrid + reranking\")\n",
    "print(\"  - advanced_rag_graph: Full agentic RAG with self-correction\")\n",
    "print(\"  - multi_agent_rag_graph: Parallel retrieval workers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3-diagram",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T07:56:34.897447Z",
     "iopub.status.busy": "2025-11-27T07:56:34.897447Z",
     "iopub.status.idle": "2025-11-27T07:56:36.525070Z",
     "shell.execute_reply": "2025-11-27T07:56:36.524050Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Architecture Diagram - LangGraph Mermaid Visualization\n",
    "from IPython.display import Image\n",
    "\n",
    "# Render the Advanced RAG graph structure\n",
    "Image(advanced_rag_graph.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4-routing",
   "metadata": {},
   "source": [
    "## Routing Logic (What the Diagram Doesn't Show)\n",
    "\n",
    "The graph visualization shows nodes and edges, but the **conditional routing logic** is where the \"intelligence\" lives:\n",
    "\n",
    "### `route_after_retrieval` - Quality-Based Branching\n",
    "```python\n",
    "if quality >= 0.6:\n",
    "    return \"answer_generation\"  # Good enough - proceed\n",
    "\n",
    "if attempts >= 2:\n",
    "    return \"answer_generation\"  # Max attempts - proceed anyway\n",
    "\n",
    "if (\"off_topic\" in issues or \"wrong_domain\" in issues) and (attempts == 1):\n",
    "    return \"query_expansion\"    # Early strategy switch\n",
    "else:\n",
    "    return \"rewrite_and_refine\" # Semantic rewrite\n",
    "```\n",
    "\n",
    "### `route_after_evaluation` - Answer Quality Gate\n",
    "```python\n",
    "if is_refusal:\n",
    "    return END  # LLM refused - terminal state\n",
    "\n",
    "if is_answer_sufficient:\n",
    "    return END\n",
    "\n",
    "if generation_attempts < 3:\n",
    "    return \"answer_generation\"  # Retry with feedback\n",
    "else:\n",
    "    return END  # Max attempts reached\n",
    "```\n",
    "\n",
    "### Key Design Principles\n",
    "- **Fix generation with generation** - Don't re-retrieve for generation problems\n",
    "- **Single correction cycle** - Research shows diminishing returns after first retry\n",
    "- **Adaptive thresholds** - Lower quality bar (50%) when retrieval is poor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5-retriever",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T07:56:36.529086Z",
     "iopub.status.busy": "2025-11-27T07:56:36.529086Z",
     "iopub.status.idle": "2025-11-27T07:57:38.702131Z",
     "shell.execute_reply": "2025-11-27T07:57:38.700599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOADING FROM MARKER JSON\n",
      "============================================================\n",
      "\n",
      "Found 10 JSON files in C:\\Users\\kaiho\\Downloads\\LangChain\\advanced-agentic-rag-langgraph\\evaluation\\corpus_chunks\\marker_json_v2\n",
      "Loaded 10 documents, 1295 chunks\n",
      "\n",
      "============================================================\n",
      "STEP 1: Profiling documents with LLM\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "DOCUMENT PROFILING\n",
      "============================================================\n",
      "Profiling 10 documents...\n",
      "\n",
      "Document 1 (AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.80\n",
      "  Reading Level: advanced\n",
      "  Domain: computer_vision, transformers, deep_learning\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "Document 2 (Attention Is All You Need.pdf):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.90\n",
      "  Reading Level: advanced\n",
      "  Domain: machine_learning, deep_learning, nlp\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "Document 3 (BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.90\n",
      "  Reading Level: advanced\n",
      "  Domain: nlp, transformers, deep_learning\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "Document 4 (Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.90\n",
      "  Reading Level: advanced\n",
      "  Domain: machine_learning, generative_models, diffusion_models\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "Document 5 (Denoising Diffusion Probabilistic Models.pdf):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.90\n",
      "  Reading Level: advanced\n",
      "  Domain: machine_learning, deep_learning, computer_vision\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "Document 6 (Expanding Horizons in RAG - Exploring and Extending the Limits of RAPTOR.pdf):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.80\n",
      "  Reading Level: advanced\n",
      "  Domain: nlp, retrieval_augmentation, machine_learning\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "Document 7 (Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.80\n",
      "  Reading Level: advanced\n",
      "  Domain: machine_learning, computer_vision, generative_models\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "Document 8 (Improved Training of Wasserstein GANs.pdf):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.90\n",
      "  Reading Level: advanced\n",
      "  Domain: machine_learning, deep_learning, generative_adversarial_networks\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "Document 9 (Learning Transferable Visual Models From Natural Language Supervision.pdf):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.90\n",
      "  Reading Level: advanced\n",
      "  Domain: computer_vision, machine_learning, natural_language_processing\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "Document 10 (U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.80\n",
      "  Reading Level: advanced\n",
      "  Domain: deep_learning, computer_vision, biomedical_image_segmentation\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "============================================================\n",
      "CORPUS STATISTICS\n",
      "============================================================\n",
      "Total Documents: 10\n",
      "Average Technical Density: 0.86\n",
      "\n",
      "Document Types:\n",
      "  - conference_paper: 10 (100.0%)\n",
      "\n",
      "Domain Distribution:\n",
      "  - computer_vision: 5\n",
      "  - transformers: 2\n",
      "  - deep_learning: 6\n",
      "  - machine_learning: 7\n",
      "  - nlp: 3\n",
      "  - generative_models: 2\n",
      "  - diffusion_models: 1\n",
      "  - retrieval_augmentation: 1\n",
      "  - generative_adversarial_networks: 1\n",
      "  - natural_language_processing: 1\n",
      "  - biomedical_image_segmentation: 1\n",
      "\n",
      "Percentage with Code: 30.0%\n",
      "Percentage with Math: 90.0%\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "STEP 2: Enriching pre-existing chunks with profile metadata\n",
      "============================================================\n",
      "\n",
      "Enriched 1295 chunks from 10 documents\n",
      "\n",
      "============================================================\n",
      "CORPUS STATISTICS\n",
      "============================================================\n",
      "Total documents: 10\n",
      "Total chunks: 1295\n",
      "Avg technical density: 0.86\n",
      "Document types: {'conference_paper': 10}\n",
      "Has code: 30%\n",
      "Has math: 90%\n",
      "============================================================\n",
      "\n",
      "\n",
      "Retriever initialized with 10 research papers (1295 Marker chunks)\n",
      "Papers include: Transformers, BERT, ViT, DDPM, CLIP, U-Net, WGAN-GP, Consistency Models\n"
     ]
    }
   ],
   "source": [
    "# Initialize Retriever (one-time setup)\n",
    "# This loads pre-processed Marker JSON chunks (1295 chunks from 10 papers)\n",
    "from advanced_agentic_rag_langgraph.core import setup_retriever\n",
    "\n",
    "k_final=4\n",
    "retriever = setup_retriever(from_marker_json=True, k_final=k_final)\n",
    "print(\"\\nRetriever initialized with 10 research papers (1295 Marker chunks)\")\n",
    "print(\"Papers include: Transformers, BERT, ViT, DDPM, CLIP, U-Net, WGAN-GP, Consistency Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6-tiers",
   "metadata": {},
   "source": [
    "## 4-Tier Architecture Comparison\n",
    "\n",
    "Each tier adds capabilities while using the **same budget model tier** (GPT-4o-mini) to isolate architectural improvements from model quality:\n",
    "\n",
    "| Tier | Features | Key Additions |\n",
    "|------|----------|---------------|\n",
    "| **Basic** | 1 | Semantic search only, direct LLM generation |\n",
    "| **Intermediate** | 5 | + Query expansion, hybrid retrieval, CrossEncoder reranking, RRF fusion |\n",
    "| **Advanced** | 17 | + Strategy selection, two-stage reranking, HHEM detection, quality gates, self-correction loops |\n",
    "| **Multi-Agent** | 20 | + Query decomposition, parallel retrieval workers, cross-agent LLM relevance scoring |\n",
    "\n",
    "### Feature Progression\n",
    "- **Basic**: Pure semantic similarity - works well for simple, direct questions\n",
    "- **Intermediate**: Query variations improve recall, reranking improves precision\n",
    "- **Advanced**: Adapts strategy based on query type, retries on poor results\n",
    "- **Multi-Agent**: Decomposes complex questions, retrieves in parallel, merges results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38ccd786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the time complexity of self-attention in the Transformer?\n",
      "Ground Truth Chunks: 2 (from attention paper)\n",
      "================================================================================\n",
      "\n",
      "Running Basic RAG...\n",
      "\n",
      "============================================================\n",
      "BASIC RETRIEVAL\n",
      "Strategy: semantic only (vector similarity)\n",
      "Top-K: 4 chunks (no reranking)\n",
      "Retrieved: 4 documents\n",
      "\n",
      "All 4 chunk IDs (rank order):\n",
      "  1. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_2\n",
      "  2. Attention Is All You Need.pdf_chunk_6\n",
      "  3. Attention Is All You Need.pdf_chunk_9\n",
      "  4. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_112\n",
      "\n",
      "Expected chunks: ['Attention Is All You Need.pdf_chunk_25', 'Attention Is All You Need.pdf_chunk_30']\n",
      "Found: [] | Missing: ['Attention Is All You Need.pdf_chunk_25', 'Attention Is All You Need.pdf_chunk_30']\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Answer length: 371 chars\n",
      "Context docs: 4\n",
      "============================================================\n",
      "\n",
      "  F1@4: 0%\n",
      "\n",
      "Running Intermediate RAG...\n",
      "\n",
      "============================================================\n",
      "QUERY EXPANSION\n",
      "Original: What is the time complexity of self-attention in the Transformer?\n",
      "Generated 4 variations\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "HYBRID RETRIEVAL WITH RRF FUSION\n",
      "Strategy: hybrid (always)\n",
      "Query variants: 4\n",
      "Total retrievals: 51\n",
      "Unique docs after RRF: 19\n",
      "\n",
      "All 19 chunk IDs (RRF scores):\n",
      "  1. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_2 (0.0656)\n",
      "  2. Attention Is All You Need.pdf_chunk_6 (0.0643)\n",
      "  3. Attention Is All You Need.pdf_chunk_9 (0.0630)\n",
      "  4. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_112 (0.0615)\n",
      "  5. Attention Is All You Need.pdf_chunk_30 (0.0606)\n",
      "  6. Attention Is All You Need.pdf_chunk_28 (0.0593)\n",
      "  7. Attention Is All You Need.pdf_chunk_31 (0.0561)\n",
      "  8. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_7 (0.0474)\n",
      "  9. Attention Is All You Need.pdf_chunk_55 (0.0439)\n",
      "  10. Attention Is All You Need.pdf_chunk_10 (0.0437)\n",
      "  11. Attention Is All You Need.pdf_chunk_20 (0.0435)\n",
      "  12. Attention Is All You Need.pdf_chunk_1 (0.0296)\n",
      "  13. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_8 (0.0294)\n",
      "  14. Attention Is All You Need.pdf_chunk_21 (0.0280)\n",
      "  15. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_19 (0.0139)\n",
      "  16. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_117 (0.0139)\n",
      "  17. Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_52 (0.0137)\n",
      "  18. BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_93 (0.0137)\n",
      "  19. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_20 (0.0135)\n",
      "\n",
      "Expected chunks: ['Attention Is All You Need.pdf_chunk_25', 'Attention Is All You Need.pdf_chunk_30']\n",
      "Found: ['Attention Is All You Need.pdf_chunk_30'] | Missing: ['Attention Is All You Need.pdf_chunk_25']\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "CROSSENCODER RERANKING\n",
      "Input: 19 documents\n",
      "\n",
      "Chunk IDs sent to reranking (top-15):\n",
      "  1. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_2\n",
      "  2. Attention Is All You Need.pdf_chunk_6\n",
      "  3. Attention Is All You Need.pdf_chunk_9\n",
      "  4. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_112\n",
      "  5. Attention Is All You Need.pdf_chunk_30\n",
      "  6. Attention Is All You Need.pdf_chunk_28\n",
      "  7. Attention Is All You Need.pdf_chunk_31\n",
      "  8. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_7\n",
      "  9. Attention Is All You Need.pdf_chunk_55\n",
      "  10. Attention Is All You Need.pdf_chunk_10\n",
      "  ... and 5 more\n",
      "\n",
      "Expected chunks in reranking input:\n",
      "Found: ['Attention Is All You Need.pdf_chunk_30'] | Missing: ['Attention Is All You Need.pdf_chunk_25']\n",
      "\n",
      "Output: 4 documents after CrossEncoder reranking\n",
      "\n",
      "Final chunk IDs (after CrossEncoder reranking):\n",
      "  1. Attention Is All You Need.pdf_chunk_30 (score: 1.6689)\n",
      "  2. Attention Is All You Need.pdf_chunk_9 (score: 1.4595)\n",
      "  3. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_112 (score: 1.0619)\n",
      "  4. Attention Is All You Need.pdf_chunk_28 (score: -0.1438)\n",
      "\n",
      "Expected chunks in final results:\n",
      "Found: ['Attention Is All You Need.pdf_chunk_30'] | Missing: ['Attention Is All You Need.pdf_chunk_25']\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Answer length: 468 chars\n",
      "Context docs: 4\n",
      "============================================================\n",
      "\n",
      "  F1@4: 33%\n",
      "\n",
      "Running Advanced RAG...\n",
      "\n",
      "============================================================\n",
      "STRATEGY SELECTION\n",
      "Query: What is the time complexity of self-attention in the Transformer?\n",
      "Selected: SEMANTIC\n",
      "Confidence: 95%\n",
      "Reasoning: The user is seeking an explanation regarding the time complexity of self-attention in the Transformer model, which indicates a desire for conceptual understanding. This aligns with the semantic search strengths, as it can provide detailed explanations and handle variations in wording. The query does not focus on exact matching of terms but rather on understanding a concept, making semantic the optimal choice. Additionally, the corpus is technical and relevant to the query, further supporting the use of semantic search.\n",
      "Note: Query optimization will happen in query_expansion_node\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EXPANSION DECISION\n",
      "Query: What is the time complexity of self-attention in the Transformer?\n",
      "LLM decision: EXPAND query\n",
      "Reasoning: The query addresses a complex concept (time complexity of self-attention in Transformers) that could benefit from variations in phrasing and terminology. Users may use different terms or ask the question in various ways, such as 'How does self-attention scale in Transformers?' or 'What is the computational complexity of self-attention in a Transformer model?'. Expanding the query can improve retrieval by capturing these different expressions.\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "QUERY EXPANDED (Strategy-Agnostic)\n",
      "Source query: What is the time complexity of self-attention in the Transformer?\n",
      "Expansions: ['What are the core principles and mechanisms behind self-attention in the Transformer architecture?', 'What is the time complexity of the Transformer model as a whole, including all its components?', 'How does the time complexity of self-attention in the Transformer compare to other attention mechanisms?']\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "STRATEGY-SPECIFIC QUERY OPTIMIZATION\n",
      "Strategy: semantic\n",
      "Original query: What is the time complexity of self-attention in the Transformer?\n",
      "Optimized query: What are the computational efficiency and performance implications of the self-attention mechanism in the Transformer architecture?\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "RETRIEVAL EXECUTION START\n",
      "Using 4 query expansion(s)\n",
      "Expansions generated from: retrieval_query\n",
      "Retrieval strategy: semantic\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "RRF MULTI-QUERY RETRIEVAL\n",
      "Query variants: 4\n",
      "Total retrievals: 60\n",
      "Unique docs after RRF: 27\n",
      "\n",
      "All 27 chunk IDs (RRF scores):\n",
      "  1. Attention Is All You Need.pdf_chunk_6 (0.0651)\n",
      "  2. Attention Is All You Need.pdf_chunk_9 (0.0633)\n",
      "  3. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_2 (0.0630)\n",
      "  4. Attention Is All You Need.pdf_chunk_10 (0.0591)\n",
      "  5. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_112 (0.0581)\n",
      "  6. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_7 (0.0464)\n",
      "  7. Attention Is All You Need.pdf_chunk_20 (0.0451)\n",
      "  8. Attention Is All You Need.pdf_chunk_28 (0.0444)\n",
      "  9. Attention Is All You Need.pdf_chunk_55 (0.0438)\n",
      "  10. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_8 (0.0435)\n",
      "  11. Attention Is All You Need.pdf_chunk_1 (0.0434)\n",
      "  12. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_49 (0.0427)\n",
      "  13. Attention Is All You Need.pdf_chunk_30 (0.0286)\n",
      "  14. Attention Is All You Need.pdf_chunk_8 (0.0282)\n",
      "  15. BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_20 (0.0272)\n",
      "  16. Attention Is All You Need.pdf_chunk_56 (0.0268)\n",
      "  17. Attention Is All You Need.pdf_chunk_43 (0.0164)\n",
      "  18. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_14 (0.0149)\n",
      "  19. Attention Is All You Need.pdf_chunk_38 (0.0147)\n",
      "  20. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_26 (0.0145)\n",
      "  21. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_36 (0.0143)\n",
      "  22. Attention Is All You Need.pdf_chunk_32 (0.0141)\n",
      "  23. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_50 (0.0139)\n",
      "  24. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_380 (0.0139)\n",
      "  25. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_17 (0.0137)\n",
      "  26. BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_58 (0.0135)\n",
      "  27. Attention Is All You Need.pdf_chunk_3 (0.0133)\n",
      "\n",
      "Expected chunks: ['Attention Is All You Need.pdf_chunk_25', 'Attention Is All You Need.pdf_chunk_30']\n",
      "Found: ['Attention Is All You Need.pdf_chunk_30'] | Missing: ['Attention Is All You Need.pdf_chunk_25']\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "TWO-STAGE RERANKING (After RRF)\n",
      "Input: 27 docs (from RRF top-40)\n",
      "\n",
      "Chunk IDs sent to reranking (top-40):\n",
      "  1. Attention Is All You Need.pdf_chunk_6\n",
      "  2. Attention Is All You Need.pdf_chunk_9\n",
      "  3. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_2\n",
      "  4. Attention Is All You Need.pdf_chunk_10\n",
      "  5. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_112\n",
      "  6. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_7\n",
      "  7. Attention Is All You Need.pdf_chunk_20\n",
      "  8. Attention Is All You Need.pdf_chunk_28\n",
      "  9. Attention Is All You Need.pdf_chunk_55\n",
      "  10. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_8\n",
      "  ... and 17 more\n",
      "\n",
      "Expected chunks in reranking input:\n",
      "Found: ['Attention Is All You Need.pdf_chunk_30'] | Missing: ['Attention Is All You Need.pdf_chunk_25']\n",
      "\n",
      "============================================================\n",
      "RERANKING QUERY SOURCE\n",
      "Using: active_query (semantic, human-readable)\n",
      "Query: What is the time complexity of self-attention in the Transformer?\n",
      "Note: Reranking uses semantic query, NOT algorithm-optimized retrieval_query\n",
      "============================================================\n",
      "\n",
      "\n",
      "Output: 4 docs after two-stage reranking\n",
      "\n",
      "Final chunk IDs (after two-stage reranking):\n",
      "  1. Attention Is All You Need.pdf_chunk_30 (score: 90.0000)\n",
      "  2. Attention Is All You Need.pdf_chunk_28 (score: 85.0000)\n",
      "  3. Attention Is All You Need.pdf_chunk_9 (score: 70.0000)\n",
      "  4. BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_20 (score: 65.0000)\n",
      "\n",
      "Expected chunks in final results:\n",
      "Found: ['Attention Is All You Need.pdf_chunk_30'] | Missing: ['Attention Is All You Need.pdf_chunk_25']\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "RETRIEVAL METRICS (Golden Dataset Evaluation)\n",
      "============================================================\n",
      "Recall@4:    50.00%\n",
      "Precision@4: 25.00%\n",
      "F1@4:        33.33%\n",
      "Hit Rate:    100.00%\n",
      "MRR:         1.0000\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ROUTER: AFTER RETRIEVAL\n",
      "Quality: 80% (threshold: >=60%)\n",
      "Attempts: 1/2\n",
      "Decision: answer_generation (quality acceptable)\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Question: What is the time complexity of self-attention in the Transformer?\n",
      "Context size: 3637 chars\n",
      "Retrieval quality: 80%\n",
      "Generation attempt: 1/3\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER EVALUATION (Refusal + Groundedness + Quality)\n",
      "Generation attempt: 1\n",
      "Retrieval quality: 80%\n",
      "Refusal detection: ATTEMPTED - The assistant provided substantive information regarding the time complexity of self-attention in the Transformer, including specific complexities (O(n) and O(n/r)) and context about improvements for long sequences. Although it acknowledged limitations in the context, it still delivered useful information.\n",
      "Groundedness: 80%\n",
      "Quality: 85% (sufficient)\n",
      "Combined decision: SUFFICIENT\n",
      "============================================================\n",
      "\n",
      "\n",
      "Routing: END (answer sufficient)\n",
      "  F1@4: 33%\n",
      "\n",
      "Running Multi-Agent RAG...\n",
      "\n",
      "============================================================\n",
      "COMPLEXITY CLASSIFICATION\n",
      "Query: What is the time complexity of self-attention in the Transformer?\n",
      "Classification: COMPLEX\n",
      "Reasoning: The query involves understanding the time complexity of self-attention, which may require breaking down into sub-queries such as 'What is self-attention?', 'What factors influence its time complexity?', and 'How does it compare to other mechanisms?'. This indicates multiple distinct aspects.\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "QUERY DECOMPOSITION (ORCHESTRATOR)\n",
      "Original: What is the time complexity of self-attention in the Transformer?\n",
      "Sub-queries (2):\n",
      "  1. What is the time complexity of self-attention?\n",
      "  2. What is the Transformer architecture?\n",
      "Reasoning: Two aspects mentioned: 'self-attention' and 'Transformer'.\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ASSIGN WORKERS (Send API)\n",
      "Spawning 2 parallel retrieval workers\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "RETRIEVAL WORKER 0\n",
      "Sub-query: What is the time complexity of self-attention?\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "RETRIEVAL WORKER 1\n",
      "Sub-query: What is the Transformer architecture?\n",
      "============================================================\n",
      "  [Worker] Strategy: semantic (95%)\n",
      "  [Worker] Strategy: semantic (95%)\n",
      "\n",
      "============================================================\n",
      "STRATEGY-SPECIFIC QUERY OPTIMIZATION\n",
      "Strategy: semantic\n",
      "Original query: What is the time complexity of self-attention?\n",
      "Optimized query: What are the computational efficiency characteristics of self-attention mechanisms in machine learning?\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "STRATEGY-SPECIFIC QUERY OPTIMIZATION\n",
      "Strategy: semantic\n",
      "Original query: What is the Transformer architecture?\n",
      "Optimized query: Can you explain the fundamentals and key components of the Transformer architecture in machine learning?\n",
      "============================================================\n",
      "\n",
      "  [Worker] Retrieved 4 docs, quality: 85%, attempt: 1/2\n",
      "Worker 0 complete: 4 docs, quality: 85%\n",
      "  [Worker] Retrieved 4 docs, quality: 85%, attempt: 1/2\n",
      "Worker 1 complete: 4 docs, quality: 85%\n",
      "\n",
      "============================================================\n",
      "MERGE RESULTS (SYNTHESIZER)\n",
      "Merging results from 2 workers\n",
      "Deduplication: 8 total -> 8 unique -> 8 candidates\n",
      "Expected chunks: Found ['Attention Is All You Need.pdf_chunk_25', 'Attention Is All You Need.pdf_chunk_30'] | Missing []\n",
      "\n",
      "============================================================\n",
      "SET-WISE COVERAGE SELECTION (Multi-Agent Merge)\n",
      "Original question: What is the time complexity of self-attention in the Transformer?\n",
      "Aspects: 2\n",
      "  - What is the time complexity of self-attention?\n",
      "  - What is the Transformer architecture?\n",
      "Candidates: 8\n",
      "\n",
      "Chunk IDs before selection:\n",
      "  1. Attention Is All You Need.pdf_chunk_25\n",
      "  2. Attention Is All You Need.pdf_chunk_30\n",
      "  3. Attention Is All You Need.pdf_chunk_28\n",
      "  4. Attention Is All You Need.pdf_chunk_8\n",
      "  5. Attention Is All You Need.pdf_chunk_9\n",
      "  6. Attention Is All You Need.pdf_chunk_1\n",
      "  7. Attention Is All You Need.pdf_chunk_6\n",
      "  8. Attention Is All You Need.pdf_chunk_10\n",
      "\n",
      "Selection reasoning: The selected documents provide a comprehensive understanding of the time complexity of self-attention in the Transformer architecture. Document 0 explicitly states the time complexity as O(n^2 \u0000d) and discusses the computational aspects of self-attention. Document 1 elaborates on the efficiency of self-attention compared to recurrent layers, reinforcing the complexity details. Document 4 introduces the Transformer architecture and its reliance on self-attention, while Document 5 further explains the architecture's advantages and its performance in sequence transduction tasks. Together, these documents cover both the time complexity and the structural aspects of the Transformer.\n",
      "\n",
      "Selected document IDs: ['doc_0', 'doc_1', 'doc_4', 'doc_5']\n",
      "\n",
      "Final selection (top-4):\n",
      "  1. Attention Is All You Need.pdf_chunk_25\n",
      "  2. Attention Is All You Need.pdf_chunk_30\n",
      "  3. Attention Is All You Need.pdf_chunk_9\n",
      "  4. Attention Is All You Need.pdf_chunk_1\n",
      "============================================================\n",
      "\n",
      "LLM Scoring: 8 candidates -> 4 selected\n",
      "\n",
      "Expected chunks in final selection:\n",
      "Found: ['Attention Is All You Need.pdf_chunk_25', 'Attention Is All You Need.pdf_chunk_30'] | Missing: []\n",
      "Total unique docs: 8\n",
      "Multi-agent docs (in 2+ workers): 0\n",
      "Top-4 selected for generation\n",
      "Average quality: 85%\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Question: What is the time complexity of self-attention in the Transformer?\n",
      "Context size: 3666 chars\n",
      "Retrieval quality: 85%\n",
      "Generation attempt: 1/3\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER EVALUATION (Refusal + Groundedness + Quality)\n",
      "Generation attempt: 1\n",
      "Retrieval quality: 85%\n",
      "Refusal detection: ATTEMPTED - The answer provides substantive information about the time complexity of self-attention in the Transformer, specifically stating it is O(n^2 \u0000d) and explaining the reasoning behind this complexity. It also discusses the variation when self-attention is restricted to a neighborhood of size r, changing the complexity to O(r \u0000n \u0000d). Although it acknowledges that the context does not provide information on specific implementations, it still delivers useful details regarding the complexities.\n",
      "Groundedness: 67%\n",
      "Quality: 90% (sufficient)\n",
      "Combined decision: RETRY\n",
      "============================================================\n",
      "\n",
      "\n",
      "Routing: answer_generation (attempt 2/3)\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Question: What is the time complexity of self-attention in the Transformer?\n",
      "Context size: 3666 chars\n",
      "Retrieval quality: 85%\n",
      "Generation attempt: 2/3\n",
      "============================================================\n",
      "\n",
      "RETRY MODE:\n",
      "Feedback:\n",
      "HALLUCINATION DETECTED (67% grounded):\n",
      "Unsupported claims: The time complexity of self-attention in the Transformer is O(n^2 \u0000d), Restricting self-attention to a neighborhood of size r changes the complexity to O(r \u0000n \u0000d), The context does not provide information on specific implementations or variations of self-attention beyond these complexities.\n",
      "Fix: ONLY state facts explicitly in retrieved context. If information is missing, acknowledge the limitation rather than adding unsupported details.\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER EVALUATION (Refusal + Groundedness + Quality)\n",
      "Generation attempt: 2\n",
      "Retrieval quality: 85%\n",
      "Refusal detection: ATTEMPTED - The assistant provided substantive information regarding the time complexity of self-attention in the Transformer, including the complexities for both general and restricted cases. Although it acknowledged that the context does not provide information on specific implementations or variations, it still delivered factual content about the complexities.\n",
      "Groundedness: 62%\n",
      "Quality: 90% (sufficient)\n",
      "Combined decision: RETRY\n",
      "============================================================\n",
      "\n",
      "\n",
      "Routing: answer_generation (attempt 3/3)\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Question: What is the time complexity of self-attention in the Transformer?\n",
      "Context size: 3666 chars\n",
      "Retrieval quality: 85%\n",
      "Generation attempt: 3/3\n",
      "============================================================\n",
      "\n",
      "RETRY MODE:\n",
      "Feedback:\n",
      "HALLUCINATION DETECTED (62% grounded):\n",
      "Unsupported claims: The time complexity of self-attention in the Transformer is O(n^2 \u0000d), If self-attention is restricted to consider only a neighborhood of size r, the complexity changes to O(r \u0000n \u0000d), The context does not provide information on specific implementations or variations of self-attention beyond these complexities\n",
      "Fix: ONLY state facts explicitly in retrieved context. If information is missing, acknowledge the limitation rather than adding unsupported details.\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER EVALUATION (Refusal + Groundedness + Quality)\n",
      "Generation attempt: 3\n",
      "Retrieval quality: 85%\n",
      "Refusal detection: ATTEMPTED - The assistant provided a substantive answer regarding the time complexity of self-attention in the Transformer, including specific complexities for both general and restricted cases. Although it acknowledged that the context does not provide information about specific implementations or variations, it still included relevant details about the complexities, which constitutes a partial answer.\n",
      "Groundedness: 38%\n",
      "Quality: 90% (sufficient)\n",
      "Combined decision: RETRY\n",
      "============================================================\n",
      "\n",
      "  F1@4: 67%\n",
      "\n",
      "================================================================================\n",
      "Comparison complete!\n"
     ]
    }
   ],
   "source": [
    "# Run 4-Tier Comparison\n",
    "# Using a query from golden dataset so we can calculate F1@4\n",
    "\n",
    "from advanced_agentic_rag_langgraph.evaluation.retrieval_metrics import calculate_retrieval_metrics\n",
    "from advanced_agentic_rag_langgraph.validation import HHEMHallucinationDetector\n",
    "\n",
    "# Import modules to inject shared retriever\n",
    "import advanced_agentic_rag_langgraph.variants.basic_rag_graph as basic_module\n",
    "import advanced_agentic_rag_langgraph.variants.intermediate_rag_graph as intermediate_module\n",
    "import advanced_agentic_rag_langgraph.orchestration.nodes as advanced_module\n",
    "import advanced_agentic_rag_langgraph.variants.multi_agent_rag_graph as multi_agent_module\n",
    "\n",
    "# Inject pre-built retriever into all variants\n",
    "basic_module.adaptive_retriever = retriever\n",
    "intermediate_module.adaptive_retriever = retriever\n",
    "advanced_module.adaptive_retriever = retriever\n",
    "multi_agent_module.adaptive_retriever = retriever\n",
    "\n",
    "# Query from golden_set_standard_v2.json (transformer_time_complexity)\n",
    "test_query = \"What is the time complexity of self-attention in the Transformer?\"\n",
    "ground_truth_doc_ids = [\n",
    "      \"Attention Is All You Need.pdf_chunk_25\",\n",
    "      \"Attention Is All You Need.pdf_chunk_30\"\n",
    "]\n",
    "\n",
    "graphs = {\n",
    "    \"Basic\": basic_rag_graph,\n",
    "    \"Intermediate\": intermediate_rag_graph,\n",
    "    \"Advanced\": advanced_rag_graph,\n",
    "    \"Multi-Agent\": multi_agent_rag_graph,\n",
    "}\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Ground Truth Chunks: {len(ground_truth_doc_ids)} (from attention paper)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = {}\n",
    "for name, graph in graphs.items():\n",
    "    print(f\"\\nRunning {name} RAG...\")\n",
    "    \n",
    "    initial_state = {\n",
    "        \"user_question\": test_query,\n",
    "        \"baseline_query\": test_query,\n",
    "        \"messages\": [],\n",
    "        \"retrieved_docs\": [],\n",
    "        \"retrieval_attempts\": 0,\n",
    "        \"query_expansions\": [],\n",
    "        \"sub_agent_results\": [],\n",
    "        \"ground_truth_doc_ids\": ground_truth_doc_ids,\n",
    "    }\n",
    "    config = {\"configurable\": {\"thread_id\": f\"demo-{name.lower().replace('-', '_')}\"}}\n",
    "    \n",
    "    result = graph.invoke(initial_state, config=config)\n",
    "    results[name] = result\n",
    "    \n",
    "    # Calculate F1@4 for progress display\n",
    "    docs = result.get(\"unique_docs_list\", [])\n",
    "    if docs:\n",
    "        metrics = calculate_retrieval_metrics(docs, ground_truth_doc_ids, k_final)\n",
    "        f1 = metrics[\"f1_at_k\"]\n",
    "    else:\n",
    "        f1 = 0.0\n",
    "    print(f\"  F1@{k_final}: {f1:.0%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Comparison complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-8-results",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T08:01:59.492551Z",
     "iopub.status.busy": "2025-11-27T08:01:59.492551Z",
     "iopub.status.idle": "2025-11-27T08:01:59.502954Z",
     "shell.execute_reply": "2025-11-27T08:01:59.500944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "4-TIER ARCHITECTURE COMPARISON RESULTS\n",
      "================================================================================\n",
      "\n",
      "[BASIC]\n",
      "Strategy: semantic  |  Docs: 4  |  F1@4: 0%  |  Groundedness: 0%\n",
      "--------------------------------------------------------------------------------\n",
      "The time complexity of self-attention in the Transformer is \\(O(n^2 \\cdot d)\\), where \\(n\\) is the length of the input sequence and \\(d\\) is the dimensionality of the representations. This complexity arises because self-attention computes pairwise interactions between all elements in the sequence, leading to a quadratic relationship with respect to the sequence length.\n",
      "================================================================================\n",
      "\n",
      "[INTERMEDIATE]\n",
      "Strategy: semantic  |  Docs: 4  |  F1@4: 33%  |  Groundedness: 80%\n",
      "--------------------------------------------------------------------------------\n",
      "The time complexity of self-attention in the Transformer is constant with respect to the sequence length \\( n \\), as it connects all positions with a constant number of sequentially executed operations. This contrasts with recurrent layers, which require \\( O(n) \\) sequential operations. Therefore, self-attention layers are generally faster than recurrent layers, especially when the sequence length \\( n \\) is smaller than the representation dimensionality \\( d \\).\n",
      "================================================================================\n",
      "\n",
      "[ADVANCED]\n",
      "Strategy: semantic  |  Docs: 4  |  F1@4: 33%  |  Groundedness: 80%\n",
      "--------------------------------------------------------------------------------\n",
      "The time complexity of self-attention in the Transformer is O(n) for the number of sequential operations when considering all positions. However, it can be improved to O(n/r) by restricting self-attention to a neighborhood of size r in the input sequence, which is proposed for tasks involving very long sequences. This information is found in the document titled \"Attention Is All You Need\" (specifically in the first retrieved context).\n",
      "\n",
      "The context explains the computational complexity of self-attention layers compared to recurrent layers. However, it does not provide specific details on the time complexity in different scenarios beyond the mentioned improvements.\n",
      "================================================================================\n",
      "\n",
      "[MULTI-AGENT]\n",
      "Strategy: semantic  |  Docs: 4  |  F1@4: 67%  |  Groundedness: 38%\n",
      "--------------------------------------------------------------------------------\n",
      "The time complexity of self-attention in the Transformer is \\(O(n^2 \\cdot d)\\), where \\(n\\) is the sequence length and \\(d\\) is the representation dimensionality. This complexity arises because a self-attention layer connects all positions with a constant number of sequentially executed operations, which is \\(O(1)\\) for each position.\n",
      "\n",
      "Additionally, if self-attention is restricted to consider only a neighborhood of size \\(r\\), the complexity changes to \\(O(r \\cdot n \\cdot d)\\), and the maximum path length increases to \\(O(n/r)\\) (as noted in the context).\n",
      "\n",
      "The context does not provide information about specific implementations or variations of self-attention beyond these complexities.\n",
      "================================================================================\n",
      "\n",
      "Key Observations:\n",
      "- F1@4 measures retrieval quality against known ground truth chunks\n",
      "- Groundedness measures % of answer claims supported by retrieved context (via HHEM)\n",
      "- Multi-Agent shows best F1@4 due to query decomposition finding more relevant chunks\n"
     ]
    }
   ],
   "source": [
    "# Display Comparison Results\n",
    "# Calculate F1@4 and Groundedness independently (not from graph state)\n",
    "\n",
    "hhem_detector = HHEMHallucinationDetector()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"4-TIER ARCHITECTURE COMPARISON RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for name, result in results.items():\n",
    "    strategy = result.get(\"retrieval_strategy\", \"semantic\") or \"semantic\"\n",
    "    docs = result.get(\"unique_docs_list\", [])\n",
    "    answer = result.get(\"final_answer\", \"\")\n",
    "\n",
    "    # Calculate F1@4 using ground truth\n",
    "    if docs:\n",
    "        metrics = calculate_retrieval_metrics(docs, ground_truth_doc_ids, k_final)\n",
    "        f1_at_k = metrics[\"f1_at_k\"]\n",
    "    else:\n",
    "        f1_at_k = 0.0\n",
    "\n",
    "    # Calculate groundedness using HHEM\n",
    "    if docs and answer:\n",
    "        chunks = [doc.page_content for doc in docs[:k_final]]\n",
    "        groundedness_result = hhem_detector.verify_groundedness(answer, chunks)\n",
    "        groundedness = groundedness_result[\"groundedness_score\"]\n",
    "    else:\n",
    "        groundedness = 0.0\n",
    "\n",
    "    print(f\"\\n[{name.upper()}]\")\n",
    "    print(f\"Strategy: {strategy}  |  Docs: {len(docs)}  |  F1@4: {f1_at_k:.0%}  |  Groundedness: {groundedness:.0%}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(answer if answer else \"No answer\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- F1@4 measures retrieval quality against known ground truth chunks\")\n",
    "print(\"- Groundedness measures % of answer claims supported by retrieved context (via HHEM)\")\n",
    "print(\"- Multi-Agent shows best F1@4 due to query decomposition finding more relevant chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9-selfcorrect",
   "metadata": {},
   "source": [
    "## Deep Dive: Self-Correction Loops\n",
    "\n",
    "The Advanced RAG tier implements two self-correction mechanisms:\n",
    "\n",
    "### 1. Retrieval Correction (max 2 attempts)\n",
    "When `retrieval_quality_score < 0.6`:\n",
    "- **Path A (off_topic/wrong_domain)**: Switch strategy immediately (semantic  keyword)\n",
    "- **Path B (other issues)**: Inject suggested keywords into query for better term coverage\n",
    "\n",
    "### 2. Generation Retry (max 3 attempts)\n",
    "When answer fails quality evaluation:\n",
    "- Regenerate with combined feedback (quality issues + hallucination warnings)\n",
    "- Low temperature: 0.3\n",
    "\n",
    "### Why Single Correction Cycle?\n",
    "Research (CRAG, Self-RAG) shows diminishing returns after the first correction. The architecture accepts imperfect retrieval rather than looping indefinitely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10-trace",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T08:01:59.507949Z",
     "iopub.status.busy": "2025-11-27T08:01:59.507949Z",
     "iopub.status.idle": "2025-11-27T08:03:41.213280Z",
     "shell.execute_reply": "2025-11-27T08:03:41.211764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How is attention mechanism used differently in NLP vs vision models?\n",
      "================================================================================\n",
      "\n",
      "Running Advanced RAG with potential self-correction...\n",
      "\n",
      "\n",
      "============================================================\n",
      "STRATEGY SELECTION\n",
      "Query: How is attention mechanism used differently in NLP vs vision models?\n",
      "Selected: HYBRID\n",
      "Confidence: 90%\n",
      "Reasoning: The user is seeking a comparison between the use of attention mechanisms in NLP and vision models, which indicates a need for both conceptual understanding (how attention mechanisms work in each domain) and specific terms (NLP and vision models). This makes a hybrid search the best choice, as it allows for retrieving content that explains the differences while also ensuring that the specific terms 'NLP' and 'vision models' are accurately matched.\n",
      "Note: Query optimization will happen in query_expansion_node\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EXPANSION DECISION\n",
      "Query: How is attention mechanism used differently in NLP vs vision models?\n",
      "LLM decision: EXPAND query\n",
      "Reasoning: The query addresses a complex topic that involves comparing the use of attention mechanisms in two different fields (NLP and vision models). Expanding the query into variations can help capture different terminologies and phrasing that users might employ when seeking information on this topic, thus improving retrieval effectiveness.\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "QUERY EXPANDED (Strategy-Agnostic)\n",
      "Source query: How is attention mechanism used differently in NLP vs vision models?\n",
      "Expansions: ['What are the applications of attention mechanisms in NLP models and how do they function?', 'What are the applications of attention mechanisms in vision models and what is their operational structure?', 'In what ways do attention mechanisms differ in their implementation and effectiveness between NLP and vision models?']\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "STRATEGY-SPECIFIC QUERY OPTIMIZATION\n",
      "Strategy: hybrid\n",
      "Original query: How is attention mechanism used differently in NLP vs vision models?\n",
      "Optimized query: attention mechanism in NLP vs. vision models: differences in application and impact\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "RETRIEVAL EXECUTION START\n",
      "Using 4 query expansion(s)\n",
      "Expansions generated from: retrieval_query\n",
      "Retrieval strategy: hybrid\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "RRF MULTI-QUERY RETRIEVAL\n",
      "Query variants: 4\n",
      "Total retrievals: 52\n",
      "Unique docs after RRF: 31\n",
      "\n",
      "All 31 chunk IDs (RRF scores):\n",
      "  1. Attention Is All You Need.pdf_chunk_6 (0.0643)\n",
      "  2. Attention Is All You Need.pdf_chunk_32 (0.0602)\n",
      "  3. Attention Is All You Need.pdf_chunk_20 (0.0471)\n",
      "  4. Attention Is All You Need.pdf_chunk_21 (0.0419)\n",
      "  5. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_202 (0.0328)\n",
      "  6. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_20 (0.0313)\n",
      "  7. Attention Is All You Need.pdf_chunk_13 (0.0310)\n",
      "  8. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_19 (0.0306)\n",
      "  9. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_10 (0.0306)\n",
      "  10. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_100 (0.0303)\n",
      "  11. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_2 (0.0301)\n",
      "  12. Attention Is All You Need.pdf_chunk_8 (0.0297)\n",
      "  13. Attention Is All You Need.pdf_chunk_69 (0.0293)\n",
      "  14. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_1 (0.0290)\n",
      "  15. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_49 (0.0290)\n",
      "  16. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_8 (0.0159)\n",
      "  17. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_191 (0.0154)\n",
      "  18. Attention Is All You Need.pdf_chunk_15 (0.0152)\n",
      "  19. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_10 (0.0149)\n",
      "  20. Attention Is All You Need.pdf_chunk_63 (0.0149)\n",
      "  21. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_68 (0.0147)\n",
      "  22. Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_4 (0.0145)\n",
      "  23. Attention Is All You Need.pdf_chunk_61 (0.0145)\n",
      "  24. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_37 (0.0143)\n",
      "  25. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_48 (0.0143)\n",
      "  26. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_107 (0.0141)\n",
      "  27. Denoising Diffusion Probabilistic Models.pdf_chunk_91 (0.0141)\n",
      "  28. BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_29 (0.0137)\n",
      "  29. Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_27 (0.0137)\n",
      "  30. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_99 (0.0137)\n",
      "  31. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_120 (0.0137)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "TWO-STAGE RERANKING (After RRF)\n",
      "Input: 31 docs (from RRF top-40)\n",
      "\n",
      "Chunk IDs sent to reranking (top-40):\n",
      "  1. Attention Is All You Need.pdf_chunk_6\n",
      "  2. Attention Is All You Need.pdf_chunk_32\n",
      "  3. Attention Is All You Need.pdf_chunk_20\n",
      "  4. Attention Is All You Need.pdf_chunk_21\n",
      "  5. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_202\n",
      "  6. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_20\n",
      "  7. Attention Is All You Need.pdf_chunk_13\n",
      "  8. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_19\n",
      "  9. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_10\n",
      "  10. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_100\n",
      "  ... and 21 more\n",
      "\n",
      "============================================================\n",
      "RERANKING QUERY SOURCE\n",
      "Using: active_query (semantic, human-readable)\n",
      "Query: How is attention mechanism used differently in NLP vs vision models?\n",
      "Note: Reranking uses semantic query, NOT algorithm-optimized retrieval_query\n",
      "============================================================\n",
      "\n",
      "\n",
      "Output: 4 docs after two-stage reranking\n",
      "\n",
      "Final chunk IDs (after two-stage reranking):\n",
      "  1. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_1 (score: 90.0000)\n",
      "  2. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_2 (score: 85.0000)\n",
      "  3. Attention Is All You Need.pdf_chunk_6 (score: 80.0000)\n",
      "  4. Attention Is All You Need.pdf_chunk_8 (score: 75.0000)\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ROUTER: AFTER RETRIEVAL\n",
      "Quality: 50% (threshold: >=60%)\n",
      "Attempts: 1/2\n",
      "Issues: missing_key_info, partial_coverage\n",
      "Decision: rewrite_and_refine (semantic rewrite)\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "KEYWORD INJECTION\n",
      "Original query: How is attention mechanism used differently in NLP vs vision models?\n",
      "Retrieval quality: 50%\n",
      "Keywords to inject: ['NLP vs vision attention differences', 'attention mechanism applications', 'Transformer architecture in vision', 'self-attention in NLP', 'vision models attention usage']\n",
      "Issues detected: missing_key_info, partial_coverage\n",
      "============================================================\n",
      "\n",
      "Refined query: How do attention mechanism applications differ in NLP vs vision models, particularly in terms of self-attention in NLP and the use of Transformer architecture in vision?\n",
      "Note: Query expansions cleared - will regenerate for refined query\n",
      "\n",
      "State clearing (keyword injection):\n",
      "  query_expansions: [] (will regenerate)\n",
      "  retrieval_query: None (cleared to prevent stale optimization)\n",
      "  active_query: How do attention mechanism applications differ in NLP vs vision models, particularly in terms of self-attention in NLP and the use of Transformer architecture in vision? (with injected keywords)\n",
      "\n",
      "\n",
      "============================================================\n",
      "EXPANSION DECISION\n",
      "Query: How do attention mechanism applications differ in NLP vs vision models, particularly in terms of self-attention in NLP and the use of Transformer architecture in vision?\n",
      "LLM decision: EXPAND query\n",
      "Reasoning: The query is complex and involves multiple concepts (attention mechanisms in NLP and vision models, self-attention, and Transformer architecture). Expanding it into variations can help capture different terminologies and phrasing that users might employ when searching for this information, thus improving retrieval effectiveness.\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "QUERY EXPANDED (Strategy-Agnostic)\n",
      "Source query: How do attention mechanism applications differ in NLP vs vision models, particularly in terms of self-attention in NLP and the use of Transformer architecture in vision?\n",
      "Expansions: ['What is the role of self-attention in NLP models and how does it function?', 'What are the applications and characteristics of attention mechanisms in vision models?', 'How do self-attention in NLP and Transformer architecture in vision models compare in their functionality and impact?']\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "STRATEGY-SPECIFIC QUERY OPTIMIZATION\n",
      "Strategy: hybrid\n",
      "Original query: How do attention mechanism applications differ in NLP vs vision models, particularly in terms of self-attention in NLP and the use of Transformer architecture in vision?\n",
      "Optimized query: Comparative analysis of attention mechanism applications in NLP versus vision models: focusing on self-attention techniques in NLP and the implementation of Transformer architecture in vision systems.\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "RETRIEVAL EXECUTION START\n",
      "Using 4 query expansion(s)\n",
      "Expansions generated from: retrieval_query\n",
      "Retrieval strategy: hybrid\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "RRF MULTI-QUERY RETRIEVAL\n",
      "Query variants: 4\n",
      "Total retrievals: 53\n",
      "Unique docs after RRF: 31\n",
      "\n",
      "All 31 chunk IDs (RRF scores):\n",
      "  1. Attention Is All You Need.pdf_chunk_20 (0.0617)\n",
      "  2. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_49 (0.0604)\n",
      "  3. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_2 (0.0484)\n",
      "  4. Attention Is All You Need.pdf_chunk_6 (0.0484)\n",
      "  5. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_1 (0.0445)\n",
      "  6. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_112 (0.0417)\n",
      "  7. Attention Is All You Need.pdf_chunk_32 (0.0315)\n",
      "  8. Attention Is All You Need.pdf_chunk_9 (0.0308)\n",
      "  9. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_8 (0.0303)\n",
      "  10. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_48 (0.0302)\n",
      "  11. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_52 (0.0299)\n",
      "  12. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_7 (0.0299)\n",
      "  13. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_10 (0.0294)\n",
      "  14. Attention Is All You Need.pdf_chunk_69 (0.0286)\n",
      "  15. Attention Is All You Need.pdf_chunk_8 (0.0164)\n",
      "  16. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_37 (0.0159)\n",
      "  17. Attention Is All You Need.pdf_chunk_28 (0.0154)\n",
      "  18. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_75 (0.0149)\n",
      "  19. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_50 (0.0149)\n",
      "  20. Attention Is All You Need.pdf_chunk_13 (0.0147)\n",
      "  21. Attention Is All You Need.pdf_chunk_30 (0.0145)\n",
      "  22. Attention Is All You Need.pdf_chunk_17 (0.0143)\n",
      "  23. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_9 (0.0143)\n",
      "  24. BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_29 (0.0141)\n",
      "  25. Attention Is All You Need.pdf_chunk_21 (0.0141)\n",
      "  26. Attention Is All You Need.pdf_chunk_10 (0.0141)\n",
      "  27. Expanding Horizons in RAG - Exploring and Extending the Limits of RAPTOR.pdf_chunk_51 (0.0139)\n",
      "  28. Attention Is All You Need.pdf_chunk_16 (0.0139)\n",
      "  29. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_110 (0.0139)\n",
      "  30. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_185 (0.0137)\n",
      "  31. Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_65 (0.0135)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "TWO-STAGE RERANKING (After RRF)\n",
      "Input: 31 docs (from RRF top-40)\n",
      "\n",
      "Chunk IDs sent to reranking (top-40):\n",
      "  1. Attention Is All You Need.pdf_chunk_20\n",
      "  2. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_49\n",
      "  3. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_2\n",
      "  4. Attention Is All You Need.pdf_chunk_6\n",
      "  5. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_1\n",
      "  6. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_112\n",
      "  7. Attention Is All You Need.pdf_chunk_32\n",
      "  8. Attention Is All You Need.pdf_chunk_9\n",
      "  9. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_8\n",
      "  10. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_48\n",
      "  ... and 21 more\n",
      "\n",
      "============================================================\n",
      "RERANKING QUERY SOURCE\n",
      "Using: active_query (semantic, human-readable)\n",
      "Query: How do attention mechanism applications differ in NLP vs vision models, particularly in terms of self-attention in NLP and the use of Transformer architecture in vision?\n",
      "Note: Reranking uses semantic query, NOT algorithm-optimized retrieval_query\n",
      "============================================================\n",
      "\n",
      "\n",
      "Output: 4 docs after two-stage reranking\n",
      "\n",
      "Final chunk IDs (after two-stage reranking):\n",
      "  1. BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_29 (score: 90.0000)\n",
      "  2. Attention Is All You Need.pdf_chunk_20 (score: 85.0000)\n",
      "  3. Attention Is All You Need.pdf_chunk_6 (score: 80.0000)\n",
      "  4. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_1 (score: 75.0000)\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ROUTER: AFTER RETRIEVAL\n",
      "Quality: 60% (threshold: >=60%)\n",
      "Attempts: 2/2\n",
      "Issues: partial_coverage, missing_key_info\n",
      "Decision: answer_generation (quality acceptable)\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Question: How is attention mechanism used differently in NLP vs vision models?\n",
      "Context size: 2705 chars\n",
      "Retrieval quality: 60%\n",
      "Generation attempt: 1/3\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER EVALUATION (Refusal + Groundedness + Quality)\n",
      "Generation attempt: 1\n",
      "Retrieval quality: 60%\n",
      "Refusal detection: ATTEMPTED - The answer provides substantive information about how attention mechanisms are used in NLP and vision models, including references to specific architectures and documents. It explains the differences in application and context, even though it acknowledges limitations in the details provided. Therefore, it is a partial answer, not a full refusal.\n",
      "Groundedness: 82%\n",
      "Quality: 65% (insufficient)\n",
      "Issues: partial_answer, missing_details\n",
      "Combined decision: RETRY\n",
      "============================================================\n",
      "\n",
      "\n",
      "Routing: answer_generation (attempt 2/3)\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Question: How is attention mechanism used differently in NLP vs vision models?\n",
      "Context size: 2705 chars\n",
      "Retrieval quality: 60%\n",
      "Generation attempt: 2/3\n",
      "============================================================\n",
      "\n",
      "RETRY MODE:\n",
      "Feedback:\n",
      "QUALITY ISSUES:\n",
      "Problems: partial_answer, missing_details\n",
      "Fix: Ensure all question parts are answered completely; Add more depth and explanation where the context provides supporting information\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER EVALUATION (Refusal + Groundedness + Quality)\n",
      "Generation attempt: 2\n",
      "Retrieval quality: 60%\n",
      "Refusal detection: ATTEMPTED - The answer provides substantive information about how attention mechanisms are used in NLP and vision models, detailing the differences in their applications and referencing specific documents. Although it acknowledges that the context does not provide specific examples of tasks or performance metrics, it still offers valuable insights into the topic.\n",
      "Groundedness: 71%\n",
      "Quality: 65% (insufficient)\n",
      "Issues: partial_answer, missing_details\n",
      "Combined decision: RETRY\n",
      "============================================================\n",
      "\n",
      "\n",
      "Routing: answer_generation (attempt 3/3)\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Question: How is attention mechanism used differently in NLP vs vision models?\n",
      "Context size: 2705 chars\n",
      "Retrieval quality: 60%\n",
      "Generation attempt: 3/3\n",
      "============================================================\n",
      "\n",
      "RETRY MODE:\n",
      "Feedback:\n",
      "HALLUCINATION DETECTED (71% grounded):\n",
      "Unsupported claims: The attention mechanism is utilized differently in natural language processing (NLP) models compared to vision models., The Transformer architecture relies entirely on attention mechanisms in NLP., The Transformer architecture enables significant parallelization in NLP tasks., The document 'AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE' explains that the Transformer architecture is prevalent in NLP., NLP models fully leverage attention for sequence modeling and dependency management.\n",
      "Fix: ONLY state facts explicitly in retrieved context. If information is missing, acknowledge the limitation rather than adding unsupported details.\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER EVALUATION (Refusal + Groundedness + Quality)\n",
      "Generation attempt: 3\n",
      "Retrieval quality: 60%\n",
      "Refusal detection: ATTEMPTED - The assistant provided substantive information about how attention mechanisms are used in both NLP and vision models, detailing the differences in their applications and referencing specific documents. Although it acknowledged that the context did not provide specific examples of tasks or performance metrics, it still offered valuable insights into the topic.\n",
      "Groundedness: 81%\n",
      "Quality: 65% (insufficient)\n",
      "Issues: partial_answer, missing_details\n",
      "Combined decision: RETRY\n",
      "============================================================\n",
      "\n",
      "\n",
      "Routing: END (max attempts reached)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "EXECUTION TRACE\n",
      "----------------------------------------\n",
      "Retrieval Attempts: 2\n",
      "Strategy Used: hybrid\n",
      "Final Retrieval Quality: 60%\n",
      "Issues Detected: partial_coverage, missing_key_info\n",
      "\n",
      "Generation Attempts: 3\n",
      "Groundedness: 81%\n",
      "Answer Sufficient: False\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "----------------------------------------\n",
      "The attention mechanism is utilized differently in natural language processing (NLP) and vision models based on the retrieved documents.\n",
      "\n",
      "In NLP, the Transformer architecture employs attention mechanisms to model dependencies without regard to the distance between inputs and outputs. This is highlighted in the document \"Attention Is All You Need,\" which states that the Transformer relies entirely on attention to draw global dependencies, allowing for significant parallelization and achieving state-of-the-art results in tasks like translation. Additionally, the document on BERT explains that the self-attention mechanism enables BERT to model various downstream tasks, including those involving text pairs, through bidirectional cross attention.\n",
      "\n",
      "In contrast, in vision models, the application of attention is often in conjunction with convolutional neural networks (CNNs) or used to replace certain components of CNNs while maintaining their overall structure. The document \"AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\" indicates that while the Transformer architecture is standard in NLP, its use in computer vision has been more limited. The Vision Transformer (ViT) demonstrates that a pure Transformer can be applied directly to sequences of image patches for tasks like image classification, achieving excellent results compared to traditional CNNs while requiring fewer computational resources.\n",
      "\n",
      "In summary, NLP models utilize attention mechanisms to manage dependencies in sequences, while vision models often integrate attention with CNNs or apply it in a more restricted manner. However, the context does not provide specific examples of tasks or performance metrics in both fields, nor does it detail the effectiveness of attention mechanisms in each domain.\n"
     ]
    }
   ],
   "source": [
    "# Self-Correction Example\n",
    "# This query might trigger self-correction due to cross-domain scope\n",
    "\n",
    "correction_query = \"How is attention mechanism used differently in NLP vs vision models?\"\n",
    "\n",
    "print(f\"Query: {correction_query}\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nRunning Advanced RAG with potential self-correction...\\n\")\n",
    "\n",
    "initial_state = {\n",
    "    \"user_question\": correction_query,\n",
    "    \"baseline_query\": correction_query,\n",
    "    \"messages\": [],\n",
    "    \"retrieved_docs\": [],\n",
    "    \"retrieval_attempts\": 0,\n",
    "    \"query_expansions\": [],\n",
    "}\n",
    "config = {\"configurable\": {\"thread_id\": \"demo-selfcorrect\"}}\n",
    "\n",
    "result = advanced_rag_graph.invoke(initial_state, config=config)\n",
    "\n",
    "# Display self-correction trace\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"EXECUTION TRACE\")\n",
    "print(\"-\"*40)\n",
    "print(f\"Retrieval Attempts: {result.get('retrieval_attempts', 1)}\")\n",
    "print(f\"Strategy Used: {result.get('retrieval_strategy', 'semantic')}\")\n",
    "\n",
    "if result.get('strategy_changed'):\n",
    "    print(f\"Strategy Changed: Yes\")\n",
    "    print(f\"  Reason: {result.get('strategy_switch_reason', 'N/A')}\")\n",
    "\n",
    "quality = result.get('retrieval_quality_score', 0) or 0\n",
    "print(f\"Final Retrieval Quality: {quality:.0%}\")\n",
    "\n",
    "if result.get('retrieval_quality_issues'):\n",
    "    print(f\"Issues Detected: {', '.join(result['retrieval_quality_issues'])}\")\n",
    "\n",
    "if result.get('retrieval_improvement_suggestion'):\n",
    "    print(f\"Improvement Suggestion: {result['retrieval_improvement_suggestion']}\")\n",
    "\n",
    "print(f\"\\nGeneration Attempts: {result.get('generation_attempts', 1)}\")\n",
    "print(f\"Groundedness: {(result.get('groundedness_score', 0) or 0):.0%}\")\n",
    "print(f\"Answer Sufficient: {result.get('is_answer_sufficient', True)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"-\"*40)\n",
    "print(result.get('final_answer', 'No answer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11-metrics",
   "metadata": {},
   "source": [
    "## Evaluation Metrics Summary\n",
    "\n",
    "All tiers use **budget models** (GPT-4o-mini only) to isolate architectural improvements from model quality.\n",
    "\n",
    "### Standard Dataset (20 questions, k=4)\n",
    "\n",
    "| Tier | F1@4 | MRR | nDCG@4 | Groundedness |\n",
    "|------|------|-----|--------|--------------|\n",
    "| Basic | 17.3% | 0.254 | 0.236 | 48.6% |\n",
    "| Intermediate | 22.7% | 0.450 | 0.343 | 70.7% |\n",
    "| Advanced | 29.3% | 0.600 | 0.484 | 64.1% |\n",
    "| **Multi-Agent** | **31.7%** | **0.600** | **0.497** | **76.6%** |\n",
    "\n",
    "*Maximum achievable F1@4 is 64.6% (dataset avg: 2.1 relevant docs/question). Multi-Agent achieves 49% of ceiling.*\n",
    "\n",
    "### Hard Dataset (10 questions, k=6, multi-document)\n",
    "\n",
    "| Tier | F1@6 | MRR | nDCG@6 | Groundedness |\n",
    "|------|------|-----|--------|--------------|\n",
    "| Basic | 22.0% | 0.458 | 0.300 | 60.4% |\n",
    "| Intermediate | 25.6% | 0.408 | 0.293 | 62.5% |\n",
    "| Advanced | 32.5% | **0.750** | 0.460 | **88.9%** |\n",
    "| **Multi-Agent** | **38.7%** | 0.633 | **0.480** | 87.0% |\n",
    "\n",
    "*Maximum achievable F1@6 is 84.8% (dataset avg: 4.7 relevant docs/question). Multi-Agent achieves 46% of ceiling.*\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- **83% retrieval improvement** (F1@4: 17.3% -> 31.7%) with budget models only\n",
    "- Multi-Agent shows +76% F1 improvement over Basic on hard dataset\n",
    "- Query decomposition helps find relevant documents across multiple aspects\n",
    "- Architecture provides value independent of model quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13-conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Architecture > Model Size** - The graph structure provides value independent of model quality. Budget tier demonstrates the RAG intelligence; higher tiers add polish.\n",
    "\n",
    "2. **Distributed Decision-Making** - No central orchestrator. The StateGraph itself is the agent, with routing functions encoding planning logic.\n",
    "\n",
    "3. **Quality-Driven Flow** - Every routing point evaluates results and decides next action. Poor retrieval triggers correction; poor generation triggers retry.\n",
    "\n",
    "4. **Multi-Agent for Complexity** - Query decomposition with parallel workers significantly improves retrieval on complex, multi-faceted questions.\n",
    "\n",
    "### Source Code\n",
    "\n",
    "```\n",
    "src/advanced_agentic_rag_langgraph/\n",
    "    core/              # State, model tiers, retriever setup\n",
    "    evaluation/        # Metrics framework (F1, MRR, nDCG)\n",
    "    orchestration/     # Main graph, nodes, routing\n",
    "    retrieval/         # Strategy selection, reranking\n",
    "    validation/        # HHEM hallucination detection\n",
    "    variants/          # Basic, Intermediate, Advanced, Multi-Agent\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced-agentic-rag-langgraph (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
