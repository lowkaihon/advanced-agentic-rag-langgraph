{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-1-intro",
   "metadata": {},
   "source": [
    "# Advanced Agentic RAG with LangGraph\n",
    "\n",
    "**A portfolio project showcasing intelligent, adaptive retrieval pipelines**\n",
    "\n",
    "This system demonstrates production-grade RAG architecture patterns:\n",
    "\n",
    "- **Dynamic strategy selection** - Semantic, keyword, or hybrid retrieval based on query analysis\n",
    "- **Quality-driven self-correction** - Automatic query rewrites when retrieval quality is insufficient\n",
    "- **Multi-stage reranking** - CrossEncoder (top-10) + LLM-as-judge (top-4) for precision\n",
    "- **HHEM-based hallucination detection** - Claim decomposition with per-chunk HHEM-2.1-Open verification\n",
    "- **Multi-agent parallel retrieval** - Query decomposition with parallel workers for complex questions\n",
    "\n",
    "**Architecture**: 7-node StateGraph with distributed intelligence (no central orchestrator)  \n",
    "**Framework**: LangChain 1.0 & LangGraph 1.0  \n",
    "**Pattern**: Dynamic Planning and Execution Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2-setup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T07:55:37.058491Z",
     "iopub.status.busy": "2025-11-27T07:55:37.058491Z",
     "iopub.status.idle": "2025-11-27T07:56:34.893409Z",
     "shell.execute_reply": "2025-11-27T07:56:34.891880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using local HHEM backend (HHEM-2.1-Open)\n",
      "\n",
      "============================================================\n",
      "HHEM HALLUCINATION DETECTOR INITIALIZED\n",
      "Backend: HHEM-2.1-Open (local)\n",
      "Mode: Offline, no API keys required\n",
      "============================================================\n",
      "\n",
      "Using local HHEM backend (HHEM-2.1-Open)\n",
      "RAG variants loaded:\n",
      "  - basic_rag_graph: Simplest RAG (semantic search only)\n",
      "  - intermediate_rag_graph: Query expansion + hybrid + reranking\n",
      "  - advanced_rag_graph: Full agentic RAG with self-correction\n",
      "  - multi_agent_rag_graph: Parallel retrieval workers\n"
     ]
    }
   ],
   "source": [
    "# Setup & Imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# All RAG graph variants\n",
    "from advanced_agentic_rag_langgraph.variants import (\n",
    "    basic_rag_graph,\n",
    "    intermediate_rag_graph,\n",
    "    advanced_rag_graph,\n",
    "    multi_agent_rag_graph,\n",
    ")\n",
    "\n",
    "print(\"RAG variants loaded:\")\n",
    "print(\"  - basic_rag_graph: Simplest RAG (semantic search only)\")\n",
    "print(\"  - intermediate_rag_graph: Query expansion + hybrid + reranking\")\n",
    "print(\"  - advanced_rag_graph: Full agentic RAG with self-correction\")\n",
    "print(\"  - multi_agent_rag_graph: Parallel retrieval workers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3-diagram",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T07:56:34.897447Z",
     "iopub.status.busy": "2025-11-27T07:56:34.897447Z",
     "iopub.status.idle": "2025-11-27T07:56:36.525070Z",
     "shell.execute_reply": "2025-11-27T07:56:36.524050Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Architecture Diagram - LangGraph Mermaid Visualization\n",
    "from IPython.display import Image\n",
    "\n",
    "# Render the Advanced RAG graph structure\n",
    "Image(advanced_rag_graph.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4-routing",
   "metadata": {},
   "source": [
    "## Routing Logic (What the Diagram Doesn't Show)\n",
    "\n",
    "The graph visualization shows nodes and edges, but the **conditional routing logic** is where the \"intelligence\" lives:\n",
    "\n",
    "### `route_after_retrieval` - Quality-Based Branching\n",
    "```python\n",
    "if quality >= 0.6:\n",
    "    return \"answer_generation\"  # Good enough - proceed\n",
    "\n",
    "if attempts >= 2:\n",
    "    return \"answer_generation\"  # Max attempts - proceed anyway\n",
    "\n",
    "if (\"off_topic\" in issues or \"wrong_domain\" in issues) and (attempts == 1):\n",
    "    return \"query_expansion\"    # Early strategy switch\n",
    "else:\n",
    "    return \"rewrite_and_refine\" # Semantic rewrite\n",
    "```\n",
    "\n",
    "### `route_after_evaluation` - Answer Quality Gate\n",
    "```python\n",
    "if is_refusal:\n",
    "    return END  # LLM refused - terminal state\n",
    "\n",
    "if is_answer_sufficient:\n",
    "    return END\n",
    "\n",
    "if generation_attempts < 3:\n",
    "    return \"answer_generation\"  # Retry with feedback\n",
    "else:\n",
    "    return END  # Max attempts reached\n",
    "```\n",
    "\n",
    "### Key Design Principles\n",
    "- **Fix generation with generation** - Don't re-retrieve for generation problems\n",
    "- **Single correction cycle** - Research shows diminishing returns after first retry\n",
    "- **Adaptive thresholds** - Lower quality bar (50%) when retrieval is poor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5-retriever",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T07:56:36.529086Z",
     "iopub.status.busy": "2025-11-27T07:56:36.529086Z",
     "iopub.status.idle": "2025-11-27T07:57:38.702131Z",
     "shell.execute_reply": "2025-11-27T07:57:38.700599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOADING FROM MARKER JSON\n",
      "============================================================\n",
      "\n",
      "Found 10 JSON files in C:\\Users\\kaiho\\Downloads\\LangChain\\advanced-agentic-rag-langgraph\\evaluation\\corpus_chunks\\marker_json_v2\n",
      "Loaded 10 documents, 1295 chunks\n",
      "\n",
      "============================================================\n",
      "STEP 1: Profiling documents with LLM\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "DOCUMENT PROFILING\n",
      "============================================================\n",
      "Profiling 10 documents...\n",
      "\n",
      "Document 1 (AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.80\n",
      "  Reading Level: advanced\n",
      "  Domain: computer_vision, transformers, deep_learning\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "Document 2 (Attention Is All You Need.pdf):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.90\n",
      "  Reading Level: advanced\n",
      "  Domain: machine_learning, deep_learning, nlp\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "Document 3 (BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.90\n",
      "  Reading Level: advanced\n",
      "  Domain: nlp, transformers, deep_learning\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "Document 4 (Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.90\n",
      "  Reading Level: advanced\n",
      "  Domain: machine_learning, generative_models, diffusion_models\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "Document 5 (Denoising Diffusion Probabilistic Models.pdf):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.90\n",
      "  Reading Level: advanced\n",
      "  Domain: machine_learning, deep_learning, computer_vision\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "Document 6 (Expanding Horizons in RAG - Exploring and Extending the Limits of RAPTOR.pdf):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.80\n",
      "  Reading Level: advanced\n",
      "  Domain: nlp, retrieval_augmentation, machine_learning\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "Document 7 (Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.80\n",
      "  Reading Level: advanced\n",
      "  Domain: computer_vision, deep_learning, generative_models\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "Document 8 (Improved Training of Wasserstein GANs.pdf):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.90\n",
      "  Reading Level: advanced\n",
      "  Domain: machine_learning, deep_learning, generative_adversarial_networks\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "Document 9 (Learning Transferable Visual Models From Natural Language Supervision.pdf):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.80\n",
      "  Reading Level: advanced\n",
      "  Domain: computer_vision, machine_learning, natural_language_processing\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "Document 10 (U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.80\n",
      "  Reading Level: advanced\n",
      "  Domain: deep_learning, computer_vision, biomedical_image_segmentation\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "============================================================\n",
      "CORPUS STATISTICS\n",
      "============================================================\n",
      "Total Documents: 10\n",
      "Average Technical Density: 0.85\n",
      "\n",
      "Document Types:\n",
      "  - conference_paper: 10 (100.0%)\n",
      "\n",
      "Domain Distribution:\n",
      "  - computer_vision: 5\n",
      "  - transformers: 2\n",
      "  - deep_learning: 7\n",
      "  - machine_learning: 6\n",
      "  - nlp: 3\n",
      "  - generative_models: 2\n",
      "  - diffusion_models: 1\n",
      "  - retrieval_augmentation: 1\n",
      "  - generative_adversarial_networks: 1\n",
      "  - natural_language_processing: 1\n",
      "  - biomedical_image_segmentation: 1\n",
      "\n",
      "Percentage with Code: 30.0%\n",
      "Percentage with Math: 90.0%\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "STEP 2: Enriching pre-existing chunks with profile metadata\n",
      "============================================================\n",
      "\n",
      "Enriched 1295 chunks from 10 documents\n",
      "\n",
      "============================================================\n",
      "CORPUS STATISTICS\n",
      "============================================================\n",
      "Total documents: 10\n",
      "Total chunks: 1295\n",
      "Avg technical density: 0.85\n",
      "Document types: {'conference_paper': 10}\n",
      "Has code: 30%\n",
      "Has math: 90%\n",
      "============================================================\n",
      "\n",
      "\n",
      "Retriever initialized with 10 research papers (1295 Marker chunks)\n",
      "Papers include: Transformers, BERT, ViT, DDPM, CLIP, U-Net, WGAN-GP, Consistency Models\n"
     ]
    }
   ],
   "source": [
    "# Initialize Retriever (one-time setup)\n",
    "# This loads pre-processed Marker JSON chunks (1295 chunks from 10 papers)\n",
    "from advanced_agentic_rag_langgraph.core import setup_retriever\n",
    "\n",
    "k_final=4\n",
    "retriever = setup_retriever(from_marker_json=True, k_final=k_final)\n",
    "print(\"\\nRetriever initialized with 10 research papers (1295 Marker chunks)\")\n",
    "print(\"Papers include: Transformers, BERT, ViT, DDPM, CLIP, U-Net, WGAN-GP, Consistency Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6-tiers",
   "metadata": {},
   "source": [
    "## 4-Tier Architecture Comparison\n",
    "\n",
    "Each tier adds capabilities while using the **same budget model tier** (GPT-4o-mini) to isolate architectural improvements from model quality:\n",
    "\n",
    "| Tier | Features | Key Additions |\n",
    "|------|----------|---------------|\n",
    "| **Basic** | 1 | Semantic search only, direct LLM generation |\n",
    "| **Intermediate** | 5 | + Query expansion, hybrid retrieval, CrossEncoder reranking, RRF fusion |\n",
    "| **Advanced** | 17 | + Strategy selection, two-stage reranking, HHEM detection, quality gates, self-correction loops |\n",
    "| **Multi-Agent** | 20 | + Query decomposition, parallel retrieval workers, cross-agent LLM relevance scoring |\n",
    "\n",
    "### Feature Progression\n",
    "- **Basic**: Pure semantic similarity - works well for simple, direct questions\n",
    "- **Intermediate**: Query variations improve recall, reranking improves precision\n",
    "- **Advanced**: Adapts strategy based on query type, retries on poor results\n",
    "- **Multi-Agent**: Decomposes complex questions, retrieves in parallel, merges results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38ccd786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the time complexity of self-attention in the Transformer?\n",
      "Ground Truth Chunks: 2 (from attention paper)\n",
      "================================================================================\n",
      "\n",
      "Running Basic RAG...\n",
      "\n",
      "============================================================\n",
      "BASIC RETRIEVAL\n",
      "Strategy: semantic only (vector similarity)\n",
      "Top-K: 4 chunks (no reranking)\n",
      "Retrieved: 4 documents\n",
      "\n",
      "All 4 chunk IDs (rank order):\n",
      "  1. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_2\n",
      "  2. Attention Is All You Need.pdf_chunk_6\n",
      "  3. Attention Is All You Need.pdf_chunk_9\n",
      "  4. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_112\n",
      "\n",
      "Expected chunks: ['Attention Is All You Need.pdf_chunk_25', 'Attention Is All You Need.pdf_chunk_30']\n",
      "Found: [] | Missing: ['Attention Is All You Need.pdf_chunk_25', 'Attention Is All You Need.pdf_chunk_30']\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Answer length: 298 chars\n",
      "Context docs: 4\n",
      "============================================================\n",
      "\n",
      "  F1@4: 0%\n",
      "\n",
      "Running Intermediate RAG...\n",
      "\n",
      "============================================================\n",
      "QUERY EXPANSION\n",
      "Original: What is the time complexity of self-attention in the Transformer?\n",
      "Generated 4 variations\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "HYBRID RETRIEVAL WITH RRF FUSION\n",
      "Strategy: hybrid (always)\n",
      "Query variants: 4\n",
      "Total retrievals: 49\n",
      "Unique docs after RRF: 25\n",
      "\n",
      "All 25 chunk IDs (RRF scores):\n",
      "  1. Attention Is All You Need.pdf_chunk_6 (0.0648)\n",
      "  2. Attention Is All You Need.pdf_chunk_9 (0.0632)\n",
      "  3. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_2 (0.0628)\n",
      "  4. Attention Is All You Need.pdf_chunk_10 (0.0463)\n",
      "  5. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_7 (0.0462)\n",
      "  6. Attention Is All You Need.pdf_chunk_20 (0.0451)\n",
      "  7. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_112 (0.0444)\n",
      "  8. Attention Is All You Need.pdf_chunk_28 (0.0444)\n",
      "  9. Attention Is All You Need.pdf_chunk_55 (0.0301)\n",
      "  10. Attention Is All You Need.pdf_chunk_30 (0.0299)\n",
      "  11. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_8 (0.0290)\n",
      "  12. Attention Is All You Need.pdf_chunk_31 (0.0282)\n",
      "  13. Attention Is All You Need.pdf_chunk_21 (0.0280)\n",
      "  14. Attention Is All You Need.pdf_chunk_43 (0.0161)\n",
      "  15. Attention Is All You Need.pdf_chunk_1 (0.0154)\n",
      "  16. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_14 (0.0152)\n",
      "  17. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_49 (0.0149)\n",
      "  18. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_26 (0.0147)\n",
      "  19. Attention Is All You Need.pdf_chunk_8 (0.0145)\n",
      "  20. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_17 (0.0145)\n",
      "  21. Attention Is All You Need.pdf_chunk_38 (0.0143)\n",
      "  22. BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_20 (0.0141)\n",
      "  23. BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_19 (0.0139)\n",
      "  24. BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_25 (0.0139)\n",
      "  25. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_19 (0.0137)\n",
      "\n",
      "Expected chunks: ['Attention Is All You Need.pdf_chunk_25', 'Attention Is All You Need.pdf_chunk_30']\n",
      "Found: ['Attention Is All You Need.pdf_chunk_30'] | Missing: ['Attention Is All You Need.pdf_chunk_25']\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "CROSSENCODER RERANKING\n",
      "Input: 25 documents\n",
      "\n",
      "Chunk IDs sent to reranking (top-15):\n",
      "  1. Attention Is All You Need.pdf_chunk_6\n",
      "  2. Attention Is All You Need.pdf_chunk_9\n",
      "  3. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_2\n",
      "  4. Attention Is All You Need.pdf_chunk_10\n",
      "  5. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_7\n",
      "  6. Attention Is All You Need.pdf_chunk_20\n",
      "  7. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_112\n",
      "  8. Attention Is All You Need.pdf_chunk_28\n",
      "  9. Attention Is All You Need.pdf_chunk_55\n",
      "  10. Attention Is All You Need.pdf_chunk_30\n",
      "  ... and 5 more\n",
      "\n",
      "Expected chunks in reranking input:\n",
      "Found: ['Attention Is All You Need.pdf_chunk_30'] | Missing: ['Attention Is All You Need.pdf_chunk_25']\n",
      "\n",
      "Output: 4 documents after CrossEncoder reranking\n",
      "\n",
      "Final chunk IDs (after CrossEncoder reranking):\n",
      "  1. Attention Is All You Need.pdf_chunk_30 (score: 1.6689)\n",
      "  2. Attention Is All You Need.pdf_chunk_9 (score: 1.4595)\n",
      "  3. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_112 (score: 1.0619)\n",
      "  4. Attention Is All You Need.pdf_chunk_28 (score: -0.1438)\n",
      "\n",
      "Expected chunks in final results:\n",
      "Found: ['Attention Is All You Need.pdf_chunk_30'] | Missing: ['Attention Is All You Need.pdf_chunk_25']\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Answer length: 618 chars\n",
      "Context docs: 4\n",
      "============================================================\n",
      "\n",
      "  F1@4: 33%\n",
      "\n",
      "Running Advanced RAG...\n",
      "\n",
      "============================================================\n",
      "STRATEGY SELECTION\n",
      "Query: What is the time complexity of self-attention in the Transformer?\n",
      "Selected: SEMANTIC\n",
      "Confidence: 90%\n",
      "Reasoning: The user is seeking an explanation of the time complexity of self-attention in the Transformer, which indicates a desire for conceptual understanding. This aligns with the semantic search strengths, as it can provide detailed explanations and handle paraphrasing. The query does not focus on exact matching of terms but rather on understanding a concept, making semantic the optimal choice. Additionally, the corpus is technical and relevant to the query, further supporting the use of semantic search.\n",
      "Note: Query optimization will happen in query_expansion_node\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EXPANSION DECISION\n",
      "Query: What is the time complexity of self-attention in the Transformer?\n",
      "LLM decision: EXPAND query\n",
      "Reasoning: The query involves a complex concept (self-attention in Transformers) that could benefit from variations in phrasing and terminology. Users may use different terms or ask about related aspects, such as 'time complexity of attention mechanisms' or 'efficiency of Transformer models', which could enhance retrieval of relevant information.\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "QUERY EXPANDED (Strategy-Agnostic)\n",
      "Source query: What is the time complexity of self-attention in the Transformer?\n",
      "Expansions: ['What is the time complexity associated with self-attention mechanisms in the Transformer model?', 'How does the time complexity of the Transformer compare to other neural network architectures?', 'What factors influence the time complexity of self-attention in the Transformer, and how can it be optimized?']\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "STRATEGY-SPECIFIC QUERY OPTIMIZATION\n",
      "Strategy: semantic\n",
      "Original query: What is the time complexity of self-attention in the Transformer?\n",
      "Optimized query: What are the computational efficiency characteristics of self-attention mechanisms in the Transformer architecture?\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "RETRIEVAL EXECUTION START\n",
      "Using 4 query expansion(s)\n",
      "Expansions generated from: retrieval_query\n",
      "Retrieval strategy: semantic\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "RRF MULTI-QUERY RETRIEVAL\n",
      "Query variants: 4\n",
      "Total retrievals: 60\n",
      "Unique docs after RRF: 26\n",
      "\n",
      "All 26 chunk IDs (RRF scores):\n",
      "  1. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_2 (0.0643)\n",
      "  2. Attention Is All You Need.pdf_chunk_6 (0.0641)\n",
      "  3. Attention Is All You Need.pdf_chunk_9 (0.0635)\n",
      "  4. Attention Is All You Need.pdf_chunk_1 (0.0593)\n",
      "  5. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_112 (0.0592)\n",
      "  6. Attention Is All You Need.pdf_chunk_10 (0.0575)\n",
      "  7. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_7 (0.0464)\n",
      "  8. Attention Is All You Need.pdf_chunk_55 (0.0455)\n",
      "  9. Attention Is All You Need.pdf_chunk_28 (0.0437)\n",
      "  10. Attention Is All You Need.pdf_chunk_20 (0.0437)\n",
      "  11. Attention Is All You Need.pdf_chunk_30 (0.0436)\n",
      "  12. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_8 (0.0432)\n",
      "  13. BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_20 (0.0408)\n",
      "  14. Attention Is All You Need.pdf_chunk_56 (0.0274)\n",
      "  15. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_49 (0.0274)\n",
      "  16. Attention Is All You Need.pdf_chunk_7 (0.0161)\n",
      "  17. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_97 (0.0149)\n",
      "  18. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_14 (0.0147)\n",
      "  19. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_26 (0.0145)\n",
      "  20. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_380 (0.0143)\n",
      "  21. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_17 (0.0141)\n",
      "  22. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_45 (0.0139)\n",
      "  23. Denoising Diffusion Probabilistic Models.pdf_chunk_32 (0.0137)\n",
      "  24. Attention Is All You Need.pdf_chunk_29 (0.0137)\n",
      "  25. Attention Is All You Need.pdf_chunk_8 (0.0133)\n",
      "  26. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_1 (0.0133)\n",
      "\n",
      "Expected chunks: ['Attention Is All You Need.pdf_chunk_25', 'Attention Is All You Need.pdf_chunk_30']\n",
      "Found: ['Attention Is All You Need.pdf_chunk_30'] | Missing: ['Attention Is All You Need.pdf_chunk_25']\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "TWO-STAGE RERANKING (After RRF)\n",
      "Input: 26 docs (from RRF top-40)\n",
      "\n",
      "Chunk IDs sent to reranking (top-40):\n",
      "  1. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_2\n",
      "  2. Attention Is All You Need.pdf_chunk_6\n",
      "  3. Attention Is All You Need.pdf_chunk_9\n",
      "  4. Attention Is All You Need.pdf_chunk_1\n",
      "  5. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_112\n",
      "  6. Attention Is All You Need.pdf_chunk_10\n",
      "  7. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_7\n",
      "  8. Attention Is All You Need.pdf_chunk_55\n",
      "  9. Attention Is All You Need.pdf_chunk_28\n",
      "  10. Attention Is All You Need.pdf_chunk_20\n",
      "  ... and 16 more\n",
      "\n",
      "Expected chunks in reranking input:\n",
      "Found: ['Attention Is All You Need.pdf_chunk_30'] | Missing: ['Attention Is All You Need.pdf_chunk_25']\n",
      "\n",
      "============================================================\n",
      "RERANKING QUERY SOURCE\n",
      "Using: active_query (semantic, human-readable)\n",
      "Query: What is the time complexity of self-attention in the Transformer?\n",
      "Note: Reranking uses semantic query, NOT algorithm-optimized retrieval_query\n",
      "============================================================\n",
      "\n",
      "\n",
      "Output: 4 docs after two-stage reranking\n",
      "\n",
      "Final chunk IDs (after two-stage reranking):\n",
      "  1. Attention Is All You Need.pdf_chunk_30 (score: 95.0000)\n",
      "  2. Attention Is All You Need.pdf_chunk_28 (score: 85.0000)\n",
      "  3. Attention Is All You Need.pdf_chunk_9 (score: 70.0000)\n",
      "  4. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_17 (score: 60.0000)\n",
      "\n",
      "Expected chunks in final results:\n",
      "Found: ['Attention Is All You Need.pdf_chunk_30'] | Missing: ['Attention Is All You Need.pdf_chunk_25']\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "RETRIEVAL METRICS (Golden Dataset Evaluation)\n",
      "============================================================\n",
      "Recall@4:    50.00%\n",
      "Precision@4: 25.00%\n",
      "F1@4:        33.33%\n",
      "Hit Rate:    100.00%\n",
      "MRR:         1.0000\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ROUTER: AFTER RETRIEVAL\n",
      "Quality: 80% (threshold: >=60%)\n",
      "Attempts: 1/2\n",
      "Decision: answer_generation (quality acceptable)\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Question: What is the time complexity of self-attention in the Transformer?\n",
      "Context size: 3655 chars\n",
      "Retrieval quality: 80%\n",
      "Generation attempt: 1/3\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER EVALUATION (Refusal + Groundedness + Quality)\n",
      "Generation attempt: 1\n",
      "Retrieval quality: 80%\n",
      "Refusal detection: ATTEMPTED - The answer provides substantive information about the time complexity of self-attention in the Transformer, specifically stating it is O(n) for each layer and explaining the implications of sequence length and representation dimensionality. Although it acknowledges that the context does not provide specific examples or numerical values, it still delivers relevant details about the topic.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groundedness: 71% (HHEM-2.1-Open (local))\n",
      "Hallucination detected - skipping quality check (efficiency optimization)\n",
      "============================================================\n",
      "\n",
      "\n",
      "Routing: answer_generation (attempt 2/3)\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Question: What is the time complexity of self-attention in the Transformer?\n",
      "Context size: 3655 chars\n",
      "Retrieval quality: 80%\n",
      "Generation attempt: 2/3\n",
      "============================================================\n",
      "\n",
      "RETRY MODE:\n",
      "Feedback:\n",
      "HALLUCINATION DETECTED (71% grounded):\n",
      "Unsupported claims: The time complexity of self-attention in the Transformer is O(n) for each layer, The document 'Attention Is All You Need' discusses self-attention\n",
      "\n",
      "Fix: ONLY state facts explicitly in retrieved context. If information is missing, acknowledge the limitation rather than adding unsupported details.\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER EVALUATION (Refusal + Groundedness + Quality)\n",
      "Generation attempt: 2\n",
      "Retrieval quality: 80%\n",
      "Refusal detection: ATTEMPTED - The answer provides substantive information about the time complexity of self-attention in the Transformer, discussing its relationship with recurrent layers and mentioning the potential for improving computational performance for long sequences. Although it acknowledges that a specific time complexity value is not provided, it still includes relevant details from the context.\n",
      "Groundedness: 88% (HHEM-2.1-Open (local))\n",
      "Quality: 70% (insufficient)\n",
      "Issues: partial_answer, missing_details\n",
      "Combined decision: RETRY\n",
      "============================================================\n",
      "\n",
      "\n",
      "Routing: answer_generation (attempt 3/3)\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Question: What is the time complexity of self-attention in the Transformer?\n",
      "Context size: 3655 chars\n",
      "Retrieval quality: 80%\n",
      "Generation attempt: 3/3\n",
      "============================================================\n",
      "\n",
      "RETRY MODE:\n",
      "Feedback:\n",
      "QUALITY ISSUES:\n",
      "Problems: partial_answer, missing_details\n",
      "Fix: Ensure all question parts are answered completely; Add more depth and explanation where the context provides supporting information\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER EVALUATION (Refusal + Groundedness + Quality)\n",
      "Generation attempt: 3\n",
      "Retrieval quality: 80%\n",
      "Refusal detection: ATTEMPTED - The answer provides substantive information about the time complexity of self-attention in the Transformer, including comparisons to recurrent layers and adjustments for long sequences. It acknowledges limitations in the context but still delivers relevant details.\n",
      "Groundedness: 100% (HHEM-2.1-Open (local))\n",
      "Quality: 70% (insufficient)\n",
      "Issues: partial_answer, missing_details\n",
      "Combined decision: RETRY\n",
      "============================================================\n",
      "\n",
      "\n",
      "Routing: END (max attempts reached)\n",
      "  F1@4: 33%\n",
      "\n",
      "Running Multi-Agent RAG...\n",
      "\n",
      "============================================================\n",
      "COMPLEXITY CLASSIFICATION\n",
      "Query: What is the time complexity of self-attention in the Transformer?\n",
      "Classification: COMPLEX\n",
      "Reasoning: The query involves understanding the time complexity of self-attention, which may require breaking down into sub-queries about the definition of self-attention, the components involved in calculating its time complexity, and possibly comparisons with other mechanisms.\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "QUERY DECOMPOSITION (ORCHESTRATOR)\n",
      "Original: What is the time complexity of self-attention in the Transformer?\n",
      "Sub-queries (3):\n",
      "  1. What is the time complexity of self-attention?\n",
      "  2. What is self-attention in the Transformer?\n",
      "  3. What is the Transformer architecture?\n",
      "Reasoning: Three aspects (time complexity, self-attention, Transformer) = 3 sub-queries.\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ASSIGN WORKERS (Send API)\n",
      "Spawning 3 parallel retrieval workers\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "RETRIEVAL WORKER 0\n",
      "Sub-query: What is the time complexity of self-attention?\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "RETRIEVAL WORKER 1\n",
      "Sub-query: What is self-attention in the Transformer?\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "RETRIEVAL WORKER 2\n",
      "Sub-query: What is the Transformer architecture?\n",
      "============================================================\n",
      "  [Worker] Strategy: semantic (95%)\n",
      "  [Worker] Strategy: semantic (95%)\n",
      "  [Worker] Strategy: semantic (95%)\n",
      "\n",
      "============================================================\n",
      "STRATEGY-SPECIFIC QUERY OPTIMIZATION\n",
      "Strategy: semantic\n",
      "Original query: What is the Transformer architecture?\n",
      "Optimized query: Can you explain the fundamentals and applications of the Transformer architecture in machine learning?\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "STRATEGY-SPECIFIC QUERY OPTIMIZATION\n",
      "Strategy: semantic\n",
      "Original query: What is the time complexity of self-attention?\n",
      "Optimized query: What are the computational efficiency and performance considerations of self-attention mechanisms?\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "STRATEGY-SPECIFIC QUERY OPTIMIZATION\n",
      "Strategy: semantic\n",
      "Original query: What is self-attention in the Transformer?\n",
      "Optimized query: How does self-attention function within the context of the Transformer architecture?\n",
      "============================================================\n",
      "\n",
      "  [Worker] Retrieved 4 docs, quality: 80%, attempt: 1/2\n",
      "Worker 1 complete: 4 docs, quality: 80%\n",
      "  [Worker] Retrieved 4 docs, quality: 85%, attempt: 1/2\n",
      "Worker 2 complete: 4 docs, quality: 85%\n",
      "  [Worker] Retrieved 4 docs, quality: 85%, attempt: 1/2\n",
      "Worker 0 complete: 4 docs, quality: 85%\n",
      "\n",
      "============================================================\n",
      "MERGE RESULTS (SYNTHESIZER)\n",
      "Merging results from 3 workers\n",
      "Deduplication: 12 total -> 9 unique -> 9 candidates\n",
      "Expected chunks: Found ['Attention Is All You Need.pdf_chunk_25', 'Attention Is All You Need.pdf_chunk_30'] | Missing []\n",
      "\n",
      "============================================================\n",
      "SET-WISE COVERAGE SELECTION (Multi-Agent Merge)\n",
      "Original question: What is the time complexity of self-attention in the Transformer?\n",
      "Aspects: 3\n",
      "  - What is the time complexity of self-attention?\n",
      "  - What is self-attention in the Transformer?\n",
      "  - What is the Transformer architecture?\n",
      "Candidates: 9\n",
      "\n",
      "Chunk IDs before selection:\n",
      "  1. Attention Is All You Need.pdf_chunk_25\n",
      "  2. Attention Is All You Need.pdf_chunk_30\n",
      "  3. Attention Is All You Need.pdf_chunk_28\n",
      "  4. Attention Is All You Need.pdf_chunk_8\n",
      "  5. Attention Is All You Need.pdf_chunk_9\n",
      "  6. Attention Is All You Need.pdf_chunk_10\n",
      "  7. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_17\n",
      "  8. Attention Is All You Need.pdf_chunk_1\n",
      "  9. Attention Is All You Need.pdf_chunk_6\n",
      "\n",
      "Selection reasoning: The selected documents provide a comprehensive overview of self-attention in the Transformer architecture. Document 0 explicitly states the time complexity of self-attention as O(n^2 \u00183 d), while Document 1 elaborates on the computational advantages of self-attention compared to recurrent layers. Document 4 discusses the Transformer architecture's reliance on self-attention, and Document 7 provides context on the overall architecture and performance benefits of the Transformer model. Together, these documents cover the time complexity, the concept of self-attention, and the Transformer architecture.\n",
      "\n",
      "Selected document IDs: ['doc_0', 'doc_1', 'doc_4', 'doc_7']\n",
      "\n",
      "Final selection (top-4):\n",
      "  1. Attention Is All You Need.pdf_chunk_25\n",
      "  2. Attention Is All You Need.pdf_chunk_30\n",
      "  3. Attention Is All You Need.pdf_chunk_9\n",
      "  4. Attention Is All You Need.pdf_chunk_1\n",
      "============================================================\n",
      "\n",
      "LLM Scoring: 9 candidates -> 4 selected\n",
      "\n",
      "Expected chunks in final selection:\n",
      "Found: ['Attention Is All You Need.pdf_chunk_25', 'Attention Is All You Need.pdf_chunk_30'] | Missing: []\n",
      "Total unique docs: 9\n",
      "Multi-agent docs (in 2+ workers): 3\n",
      "Top-4 selected for generation\n",
      "Average quality: 83%\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Question: What is the time complexity of self-attention in the Transformer?\n",
      "Context size: 3666 chars\n",
      "Retrieval quality: 83%\n",
      "Generation attempt: 1/3\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER EVALUATION (Refusal + Groundedness + Quality)\n",
      "Generation attempt: 1\n",
      "Retrieval quality: 83%\n",
      "Refusal detection: ATTEMPTED - The answer provides substantive information about the time complexity of self-attention in the Transformer, including the complexities for both the general case and the restricted case. It explains the factors involved in the complexity and does not merely acknowledge insufficiency.\n",
      "Groundedness: 33%\n",
      "Quality: 75% (insufficient)\n",
      "Issues: partial_answer, missing_details\n",
      "Combined decision: RETRY\n",
      "============================================================\n",
      "\n",
      "\n",
      "Routing: answer_generation (attempt 2/3)\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Question: What is the time complexity of self-attention in the Transformer?\n",
      "Context size: 3666 chars\n",
      "Retrieval quality: 83%\n",
      "Generation attempt: 2/3\n",
      "============================================================\n",
      "\n",
      "RETRY MODE:\n",
      "Feedback:\n",
      "HALLUCINATION DETECTED (33% grounded):\n",
      "Unsupported claims: The time complexity of self-attention in the Transformer is O(n^2 \u0000b7 d), In the time complexity formula, n is the sequence length, In the time complexity formula, d is the representation dimensionality, When self-attention is restricted to consider only a neighborhood of size r, the complexity changes to O(r \u0000b7 n \u0000b7 d)\n",
      "Fix: ONLY state facts explicitly in retrieved context. If information is missing, acknowledge the limitation rather than adding unsupported details.\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER EVALUATION (Refusal + Groundedness + Quality)\n",
      "Generation attempt: 2\n",
      "Retrieval quality: 83%\n",
      "Refusal detection: ATTEMPTED - The answer provides substantive information about the time complexity of self-attention in the Transformer, including the complexities for both the general case and the restricted case. Although it acknowledges that the context does not provide specific examples of practical applications, it still delivers relevant details regarding the time complexity.\n",
      "Groundedness: 60%\n",
      "Quality: 75% (insufficient)\n",
      "Issues: partial_answer, missing_details\n",
      "Combined decision: RETRY\n",
      "============================================================\n",
      "\n",
      "\n",
      "Routing: answer_generation (attempt 3/3)\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Question: What is the time complexity of self-attention in the Transformer?\n",
      "Context size: 3666 chars\n",
      "Retrieval quality: 83%\n",
      "Generation attempt: 3/3\n",
      "============================================================\n",
      "\n",
      "RETRY MODE:\n",
      "Feedback:\n",
      "HALLUCINATION DETECTED (60% grounded):\n",
      "Unsupported claims: The time complexity of self-attention in the Transformer is O(n^2 \u0000b7 d), If self-attention is restricted to consider only a neighborhood of size r, the complexity changes to O(r \u0000b7 n \u0000b7 d)\n",
      "Fix: ONLY state facts explicitly in retrieved context. If information is missing, acknowledge the limitation rather than adding unsupported details.\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER EVALUATION (Refusal + Groundedness + Quality)\n",
      "Generation attempt: 3\n",
      "Retrieval quality: 83%\n",
      "Refusal detection: ATTEMPTED - The answer provides substantive information about the time complexity of self-attention in the Transformer, including specific complexities for both the general case and a restricted case. Although it acknowledges that the context does not provide examples of practical applications, it still delivers relevant details regarding the time complexity.\n",
      "Groundedness: 67%\n",
      "Quality: 75% (insufficient)\n",
      "Issues: partial_answer, missing_details\n",
      "Combined decision: RETRY\n",
      "============================================================\n",
      "\n",
      "  F1@4: 67%\n",
      "\n",
      "================================================================================\n",
      "Comparison complete!\n"
     ]
    }
   ],
   "source": [
    "# Run 4-Tier Comparison\n",
    "# Using a query from golden dataset so we can calculate F1@4\n",
    "\n",
    "from advanced_agentic_rag_langgraph.evaluation.retrieval_metrics import calculate_retrieval_metrics\n",
    "from advanced_agentic_rag_langgraph.validation import HHEMHallucinationDetector\n",
    "\n",
    "# Import modules to inject shared retriever\n",
    "import advanced_agentic_rag_langgraph.variants.basic_rag_graph as basic_module\n",
    "import advanced_agentic_rag_langgraph.variants.intermediate_rag_graph as intermediate_module\n",
    "import advanced_agentic_rag_langgraph.orchestration.nodes as advanced_module\n",
    "import advanced_agentic_rag_langgraph.variants.multi_agent_rag_graph as multi_agent_module\n",
    "\n",
    "# Inject pre-built retriever into all variants\n",
    "basic_module.adaptive_retriever = retriever\n",
    "intermediate_module.adaptive_retriever = retriever\n",
    "advanced_module.adaptive_retriever = retriever\n",
    "multi_agent_module.adaptive_retriever = retriever\n",
    "\n",
    "# Query from golden_set_standard_v2.json (transformer_time_complexity)\n",
    "test_query = \"What is the time complexity of self-attention in the Transformer?\"\n",
    "ground_truth_doc_ids = [\n",
    "      \"Attention Is All You Need.pdf_chunk_25\",\n",
    "      \"Attention Is All You Need.pdf_chunk_30\"\n",
    "]\n",
    "\n",
    "graphs = {\n",
    "    \"Basic\": basic_rag_graph,\n",
    "    \"Intermediate\": intermediate_rag_graph,\n",
    "    \"Advanced\": advanced_rag_graph,\n",
    "    \"Multi-Agent\": multi_agent_rag_graph,\n",
    "}\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Ground Truth Chunks: {len(ground_truth_doc_ids)} (from attention paper)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = {}\n",
    "for name, graph in graphs.items():\n",
    "    print(f\"\\nRunning {name} RAG...\")\n",
    "    \n",
    "    initial_state = {\n",
    "        \"user_question\": test_query,\n",
    "        \"baseline_query\": test_query,\n",
    "        \"messages\": [],\n",
    "        \"retrieved_docs\": [],\n",
    "        \"retrieval_attempts\": 0,\n",
    "        \"query_expansions\": [],\n",
    "        \"sub_agent_results\": [],\n",
    "        \"ground_truth_doc_ids\": ground_truth_doc_ids,\n",
    "    }\n",
    "    config = {\"configurable\": {\"thread_id\": f\"demo-{name.lower().replace('-', '_')}\"}}\n",
    "    \n",
    "    result = graph.invoke(initial_state, config=config)\n",
    "    results[name] = result\n",
    "    \n",
    "    # Calculate F1@4 for progress display\n",
    "    docs = result.get(\"unique_docs_list\", [])\n",
    "    if docs:\n",
    "        metrics = calculate_retrieval_metrics(docs, ground_truth_doc_ids, k_final)\n",
    "        f1 = metrics[\"f1_at_k\"]\n",
    "    else:\n",
    "        f1 = 0.0\n",
    "    print(f\"  F1@{k_final}: {f1:.0%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Comparison complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-8-results",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T08:01:59.492551Z",
     "iopub.status.busy": "2025-11-27T08:01:59.492551Z",
     "iopub.status.idle": "2025-11-27T08:01:59.502954Z",
     "shell.execute_reply": "2025-11-27T08:01:59.500944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using local HHEM backend (HHEM-2.1-Open)\n",
      "================================================================================\n",
      "4-TIER ARCHITECTURE COMPARISON RESULTS\n",
      "================================================================================\n",
      "\n",
      "[BASIC]\n",
      "Strategy: semantic  |  Docs: 4  |  F1@4: 0%  |  Groundedness: 0%\n",
      "--------------------------------------------------------------------------------\n",
      "The time complexity of self-attention in the Transformer is \\(O(n^2 \\cdot d)\\), where \\(n\\) is the sequence length and \\(d\\) is the dimensionality of the input representations. This quadratic complexity arises from the need to compute attention scores for each pair of input tokens in the sequence.\n",
      "================================================================================\n",
      "\n",
      "[INTERMEDIATE]\n",
      "Strategy: semantic  |  Docs: 4  |  F1@4: 33%  |  Groundedness: 67%\n",
      "--------------------------------------------------------------------------------\n",
      "The time complexity of self-attention in the Transformer is constant with respect to the sequence length \\( n \\) for each position, making it faster than recurrent layers, especially when the sequence length \\( n \\) is smaller than the representation dimensionality \\( d \\). Specifically, for very long sequences, self-attention can be restricted to a neighborhood of size \\( r \\), which increases the maximum path length to \\( O(n/r) \\). However, the basic self-attention mechanism itself operates with a constant number of sequential operations, unlike recurrent layers that require \\( O(n) \\) sequential operations.\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ADVANCED]\n",
      "Strategy: semantic  |  Docs: 4  |  F1@4: 33%  |  Groundedness: 100%\n",
      "--------------------------------------------------------------------------------\n",
      "The time complexity of self-attention in the Transformer is characterized by the fact that a self-attention layer connects all positions with a constant number of sequentially executed operations. In contrast, a recurrent layer requires O(n) sequential operations. Self-attention layers are generally faster than recurrent layers when the sequence length \\( n \\) is smaller than the representation dimensionality \\( d \\), which is often the case in state-of-the-art models used for tasks like machine translation.\n",
      "\n",
      "To enhance computational performance for very long sequences, self-attention can be restricted to a neighborhood of size \\( r \\) around each output position. This adjustment increases the maximum path length to O(n/r).\n",
      "\n",
      "The context provides a clear understanding of the computational complexity of self-attention compared to recurrent layers and mentions the possibility of optimizing performance for long sequences. However, it does not specify a precise time complexity value for self-attention itself.\n",
      "================================================================================\n",
      "\n",
      "[MULTI-AGENT]\n",
      "Strategy: semantic  |  Docs: 4  |  F1@4: 67%  |  Groundedness: 60%\n",
      "--------------------------------------------------------------------------------\n",
      "The time complexity of self-attention in the Transformer is \\(O(n^2 \\cdot d)\\), where \\(n\\) is the sequence length and \\(d\\) is the representation dimensionality. Additionally, if self-attention is restricted to consider only a neighborhood of size \\(r\\), the complexity changes to \\(O(r \\cdot n \\cdot d)\\), and the maximum path length increases to \\(O(n/r)\\) (as noted in the retrieved documents).\n",
      "\n",
      "The context explains the time complexity of self-attention and its variations. However, it does not provide specific examples of how this complexity impacts performance in practical applications beyond the general description.\n",
      "================================================================================\n",
      "\n",
      "Key Observations:\n",
      "- F1@4 measures retrieval quality against known ground truth chunks\n",
      "- Groundedness measures % of answer claims supported by retrieved context (via HHEM)\n",
      "- Multi-Agent shows best F1@4 due to query decomposition finding more relevant chunks\n"
     ]
    }
   ],
   "source": [
    "# Display Comparison Results\n",
    "# Calculate F1@4 and Groundedness independently (not from graph state)\n",
    "\n",
    "hhem_detector = HHEMHallucinationDetector()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"4-TIER ARCHITECTURE COMPARISON RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for name, result in results.items():\n",
    "    strategy = result.get(\"retrieval_strategy\", \"semantic\") or \"semantic\"\n",
    "    docs = result.get(\"unique_docs_list\", [])\n",
    "    answer = result.get(\"final_answer\", \"\")\n",
    "\n",
    "    # Calculate F1@4 using ground truth\n",
    "    if docs:\n",
    "        metrics = calculate_retrieval_metrics(docs, ground_truth_doc_ids, k_final)\n",
    "        f1_at_k = metrics[\"f1_at_k\"]\n",
    "    else:\n",
    "        f1_at_k = 0.0\n",
    "\n",
    "    # Calculate groundedness using HHEM\n",
    "    if docs and answer:\n",
    "        chunks = [doc.page_content for doc in docs[:k_final]]\n",
    "        groundedness_result = hhem_detector.verify_groundedness(answer, chunks)\n",
    "        groundedness = groundedness_result[\"groundedness_score\"]\n",
    "    else:\n",
    "        groundedness = 0.0\n",
    "\n",
    "    print(f\"\\n[{name.upper()}]\")\n",
    "    print(f\"Strategy: {strategy}  |  Docs: {len(docs)}  |  F1@4: {f1_at_k:.0%}  |  Groundedness: {groundedness:.0%}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(answer if answer else \"No answer\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- F1@4 measures retrieval quality against known ground truth chunks\")\n",
    "print(\"- Groundedness measures % of answer claims supported by retrieved context (via HHEM)\")\n",
    "print(\"- Multi-Agent shows best F1@4 due to query decomposition finding more relevant chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "how20k2e5hs",
   "metadata": {},
   "source": [
    "## Production Deployment - Azure Container Apps API\n",
    "\n",
    "The same RAG pipeline is deployed as a production-ready REST API on Azure Container Apps.\n",
    "\n",
    "**Live Endpoint:** https://aca-agenticrag-api.jollyglacier-33123547.eastus.azurecontainerapps.io\n",
    "\n",
    "**Key Features:**\n",
    "- Serverless autoscaling (1-3 replicas)\n",
    "- Docker image with pre-downloaded AI models\n",
    "- Dual HHEM backend (local + Vectara API)\n",
    "- Cost-optimized: 0.5 CPU, 1Gi memory (~$21/month)\n",
    "- CI/CD with GitHub Actions\n",
    "\n",
    "Let's call the deployed API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dxlz5f0206d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The attention mechanism is a process that maps a query and a set of key-value pairs to an output, where all componentsquery, keys, values, and outputare represented as vectors. The output is computed as a weighted sum of the values, with the weights determined by a compatibility function that evaluates the relationship between the query and each corresponding key.\n",
      "\n",
      "One specific type of attention mechanism is self-attention (or intra-attention), which relates different positions within a single sequence to compute a representation of that sequence. This mechanism has been effectively applied in various tasks, including reading comprehension, abstractive summarization, and textual entailment.\n",
      "\n",
      "The Transformer model, which is based solely on attention mechanisms, eliminates the need for recurrence and convolutions. This architecture has demonstrated advantages in terms of parallelization and training efficiency, achieving superior performance in tasks such as machine translation.\n",
      "\n",
      "However, the context does not provide detailed mathematical formulations or specific examples of how the compatibility function operates or how attention is implemented in practice. Additionally, it does not elaborate on the different types of attention mechanisms beyond self-attention.\n",
      "\n",
      "Confidence: 75%\n",
      "Sources: Attention Is All You Need.pdf\n"
     ]
    }
   ],
   "source": [
    "# Simple API Call Example\n",
    "import requests\n",
    "\n",
    "url = \"https://aca-agenticrag-api.jollyglacier-33123547.eastus.azurecontainerapps.io/v1/query\"\n",
    "payload = {\n",
    "    \"question\": \"What is attention mechanism?\",\n",
    "    \"thread_id\": None\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "result = response.json()\n",
    "\n",
    "print(f\"Answer: {result['answer']}\\n\")\n",
    "print(f\"Confidence: {result['confidence_score']:.0%}\")\n",
    "print(f\"Sources: {', '.join(result['sources'][:3])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9-selfcorrect",
   "metadata": {},
   "source": [
    "## Deep Dive: Self-Correction Loops\n",
    "\n",
    "The Advanced RAG tier implements two self-correction mechanisms:\n",
    "\n",
    "### 1. Retrieval Correction (max 2 attempts)\n",
    "When `retrieval_quality_score < 0.6`:\n",
    "- **Path A (off_topic/wrong_domain)**: Switch strategy immediately (semantic  keyword)\n",
    "- **Path B (other issues)**: Inject suggested keywords into query for better term coverage\n",
    "\n",
    "### 2. Generation Retry (max 3 attempts)\n",
    "When answer fails quality evaluation:\n",
    "- Regenerate with combined feedback (quality issues + hallucination warnings)\n",
    "- Low temperature: 0.3\n",
    "\n",
    "### Why Single Correction Cycle?\n",
    "Research (CRAG, Self-RAG) shows diminishing returns after the first correction. The architecture accepts imperfect retrieval rather than looping indefinitely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-10-trace",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T08:01:59.507949Z",
     "iopub.status.busy": "2025-11-27T08:01:59.507949Z",
     "iopub.status.idle": "2025-11-27T08:03:41.213280Z",
     "shell.execute_reply": "2025-11-27T08:03:41.211764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How is attention mechanism used differently in NLP vs vision models?\n",
      "================================================================================\n",
      "\n",
      "Running Advanced RAG with potential self-correction...\n",
      "\n",
      "\n",
      "============================================================\n",
      "STRATEGY SELECTION\n",
      "Query: How is attention mechanism used differently in NLP vs vision models?\n",
      "Selected: HYBRID\n",
      "Confidence: 90%\n",
      "Reasoning: The user is seeking a comparison between the use of attention mechanisms in NLP and vision models, which indicates a need for both conceptual understanding (how attention mechanisms work in each domain) and specific terms (NLP and vision models). This makes a hybrid search the best choice, as it can provide both the necessary explanations and precise information about the two different applications of attention mechanisms.\n",
      "Note: Query optimization will happen in query_expansion_node\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EXPANSION DECISION\n",
      "Query: How is attention mechanism used differently in NLP vs vision models?\n",
      "LLM decision: EXPAND query\n",
      "Reasoning: The query addresses a complex topic that involves comparing the use of attention mechanisms in two different fields (NLP and vision models). Expanding the query into variations can help capture different terminologies and phrasing that users might employ, enhancing retrieval effectiveness. Additionally, users may have varying levels of familiarity with the concepts, making synonyms and related terms beneficial for comprehensive understanding.\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "QUERY EXPANDED (Strategy-Agnostic)\n",
      "Source query: How is attention mechanism used differently in NLP vs vision models?\n",
      "Expansions: ['What are the applications and functions of attention mechanisms in NLP models?', 'What are the applications and functions of attention mechanisms in vision models?', 'How do attention mechanisms in NLP models compare to those in vision models in terms of architecture and effectiveness?']\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "STRATEGY-SPECIFIC QUERY OPTIMIZATION\n",
      "Strategy: hybrid\n",
      "Original query: How is attention mechanism used differently in NLP vs vision models?\n",
      "Optimized query: attention mechanism: differences in application and effectiveness in NLP models versus vision models\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "RETRIEVAL EXECUTION START\n",
      "Using 4 query expansion(s)\n",
      "Expansions generated from: retrieval_query\n",
      "Retrieval strategy: hybrid\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "RRF MULTI-QUERY RETRIEVAL\n",
      "Query variants: 4\n",
      "Total retrievals: 53\n",
      "Unique docs after RRF: 33\n",
      "\n",
      "All 33 chunk IDs (RRF scores):\n",
      "  1. Attention Is All You Need.pdf_chunk_6 (0.0645)\n",
      "  2. Attention Is All You Need.pdf_chunk_20 (0.0473)\n",
      "  3. Attention Is All You Need.pdf_chunk_32 (0.0457)\n",
      "  4. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_10 (0.0441)\n",
      "  5. Attention Is All You Need.pdf_chunk_21 (0.0419)\n",
      "  6. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_202 (0.0325)\n",
      "  7. Attention Is All You Need.pdf_chunk_13 (0.0313)\n",
      "  8. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_2 (0.0310)\n",
      "  9. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_100 (0.0304)\n",
      "  10. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_20 (0.0301)\n",
      "  11. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_49 (0.0290)\n",
      "  12. Attention Is All You Need.pdf_chunk_15 (0.0282)\n",
      "  13. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_204 (0.0276)\n",
      "  14. Attention Is All You Need.pdf_chunk_16 (0.0274)\n",
      "  15. Attention Is All You Need.pdf_chunk_8 (0.0156)\n",
      "  16. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_48 (0.0156)\n",
      "  17. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_3 (0.0156)\n",
      "  18. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_191 (0.0154)\n",
      "  19. Attention Is All You Need.pdf_chunk_69 (0.0154)\n",
      "  20. Attention Is All You Need.pdf_chunk_1 (0.0154)\n",
      "  21. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_10 (0.0152)\n",
      "  22. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_8 (0.0152)\n",
      "  23. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_89 (0.0149)\n",
      "  24. Attention Is All You Need.pdf_chunk_63 (0.0149)\n",
      "  25. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_19 (0.0147)\n",
      "  26. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_68 (0.0147)\n",
      "  27. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_37 (0.0145)\n",
      "  28. Attention Is All You Need.pdf_chunk_17 (0.0145)\n",
      "  29. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_27 (0.0143)\n",
      "  30. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_65 (0.0141)\n",
      "  31. BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_38 (0.0139)\n",
      "  32. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_6 (0.0137)\n",
      "  33. Expanding Horizons in RAG - Exploring and Extending the Limits of RAPTOR.pdf_chunk_12 (0.0135)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "TWO-STAGE RERANKING (After RRF)\n",
      "Input: 33 docs (from RRF top-40)\n",
      "\n",
      "Chunk IDs sent to reranking (top-40):\n",
      "  1. Attention Is All You Need.pdf_chunk_6\n",
      "  2. Attention Is All You Need.pdf_chunk_20\n",
      "  3. Attention Is All You Need.pdf_chunk_32\n",
      "  4. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_10\n",
      "  5. Attention Is All You Need.pdf_chunk_21\n",
      "  6. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_202\n",
      "  7. Attention Is All You Need.pdf_chunk_13\n",
      "  8. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_2\n",
      "  9. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_100\n",
      "  10. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_20\n",
      "  ... and 23 more\n",
      "\n",
      "============================================================\n",
      "RERANKING QUERY SOURCE\n",
      "Using: active_query (semantic, human-readable)\n",
      "Query: How is attention mechanism used differently in NLP vs vision models?\n",
      "Note: Reranking uses semantic query, NOT algorithm-optimized retrieval_query\n",
      "============================================================\n",
      "\n",
      "\n",
      "Output: 4 docs after two-stage reranking\n",
      "\n",
      "Final chunk IDs (after two-stage reranking):\n",
      "  1. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_8 (score: 85.0000)\n",
      "  2. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_3 (score: 80.0000)\n",
      "  3. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_2 (score: 75.0000)\n",
      "  4. Attention Is All You Need.pdf_chunk_8 (score: 70.0000)\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ROUTER: AFTER RETRIEVAL\n",
      "Quality: 50% (threshold: >=60%)\n",
      "Attempts: 1/2\n",
      "Issues: missing_key_info\n",
      "Decision: rewrite_and_refine (semantic rewrite)\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "KEYWORD INJECTION\n",
      "Original query: How is attention mechanism used differently in NLP vs vision models?\n",
      "Retrieval quality: 50%\n",
      "Keywords to inject: ['NLP vs vision attention differences', 'self-attention applications', 'transformers in vision', 'attention mechanism comparison', 'convolutional networks vs transformers']\n",
      "Issues detected: missing_key_info\n",
      "============================================================\n",
      "\n",
      "Refined query: How do NLP vs vision attention differences manifest in the use of self-attention applications, particularly in comparing transformers in vision with convolutional networks?\n",
      "Note: Query expansions cleared - will regenerate for refined query\n",
      "\n",
      "State clearing (keyword injection):\n",
      "  query_expansions: [] (will regenerate)\n",
      "  retrieval_query: None (cleared to prevent stale optimization)\n",
      "  active_query: How do NLP vs vision attention differences manifest in the use of self-attention applications, particularly in comparing transformers in vision with convolutional networks? (with injected keywords)\n",
      "\n",
      "\n",
      "============================================================\n",
      "EXPANSION DECISION\n",
      "Query: How do NLP vs vision attention differences manifest in the use of self-attention applications, particularly in comparing transformers in vision with convolutional networks?\n",
      "LLM decision: EXPAND query\n",
      "Reasoning: The query is complex and involves multiple concepts (NLP, vision, attention mechanisms, self-attention, transformers, convolutional networks) that could be phrased in various ways. Expanding the query into multiple variations would help capture different terminologies and perspectives that users might employ when searching for information on this topic.\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "QUERY EXPANDED (Strategy-Agnostic)\n",
      "Source query: How do NLP vs vision attention differences manifest in the use of self-attention applications, particularly in comparing transformers in vision with convolutional networks?\n",
      "Expansions: ['What are the core principles and applications of self-attention in NLP transformers?', 'What are the core principles and applications of self-attention in vision convolutional networks?', 'How do self-attention mechanisms in NLP transformers compare to those in vision convolutional networks in terms of functionality and performance?']\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "STRATEGY-SPECIFIC QUERY OPTIMIZATION\n",
      "Strategy: hybrid\n",
      "Original query: How do NLP vs vision attention differences manifest in the use of self-attention applications, particularly in comparing transformers in vision with convolutional networks?\n",
      "Optimized query: Comparing NLP and vision attention: how differences in self-attention applications manifest in transformers for vision versus convolutional networks, including key mechanisms and performance implications.\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "RETRIEVAL EXECUTION START\n",
      "Using 4 query expansion(s)\n",
      "Expansions generated from: retrieval_query\n",
      "Retrieval strategy: hybrid\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "RRF MULTI-QUERY RETRIEVAL\n",
      "Query variants: 4\n",
      "Total retrievals: 53\n",
      "Unique docs after RRF: 27\n",
      "\n",
      "All 27 chunk IDs (RRF scores):\n",
      "  1. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_7 (0.0643)\n",
      "  2. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_49 (0.0600)\n",
      "  3. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_2 (0.0487)\n",
      "  4. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_10 (0.0469)\n",
      "  5. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_1 (0.0459)\n",
      "  6. Attention Is All You Need.pdf_chunk_6 (0.0458)\n",
      "  7. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_50 (0.0450)\n",
      "  8. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_8 (0.0446)\n",
      "  9. Attention Is All You Need.pdf_chunk_28 (0.0431)\n",
      "  10. Attention Is All You Need.pdf_chunk_20 (0.0306)\n",
      "  11. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_52 (0.0306)\n",
      "  12. Attention Is All You Need.pdf_chunk_9 (0.0297)\n",
      "  13. Attention Is All You Need.pdf_chunk_8 (0.0292)\n",
      "  14. Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_21 (0.0280)\n",
      "  15. Attention Is All You Need.pdf_chunk_21 (0.0276)\n",
      "  16. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_48 (0.0156)\n",
      "  17. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_3 (0.0154)\n",
      "  18. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_58 (0.0152)\n",
      "  19. Attention Is All You Need.pdf_chunk_32 (0.0149)\n",
      "  20. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_109 (0.0143)\n",
      "  21. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_344 (0.0141)\n",
      "  22. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_176 (0.0139)\n",
      "  23. BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_29 (0.0139)\n",
      "  24. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_110 (0.0139)\n",
      "  25. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_117 (0.0137)\n",
      "  26. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_190 (0.0137)\n",
      "  27. Attention Is All You Need.pdf_chunk_31 (0.0137)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "TWO-STAGE RERANKING (After RRF)\n",
      "Input: 27 docs (from RRF top-40)\n",
      "\n",
      "Chunk IDs sent to reranking (top-40):\n",
      "  1. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_7\n",
      "  2. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_49\n",
      "  3. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_2\n",
      "  4. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_10\n",
      "  5. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_1\n",
      "  6. Attention Is All You Need.pdf_chunk_6\n",
      "  7. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_50\n",
      "  8. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_8\n",
      "  9. Attention Is All You Need.pdf_chunk_28\n",
      "  10. Attention Is All You Need.pdf_chunk_20\n",
      "  ... and 17 more\n",
      "\n",
      "============================================================\n",
      "RERANKING QUERY SOURCE\n",
      "Using: active_query (semantic, human-readable)\n",
      "Query: How do NLP vs vision attention differences manifest in the use of self-attention applications, particularly in comparing transformers in vision with convolutional networks?\n",
      "Note: Reranking uses semantic query, NOT algorithm-optimized retrieval_query\n",
      "============================================================\n",
      "\n",
      "\n",
      "Output: 4 docs after two-stage reranking\n",
      "\n",
      "Final chunk IDs (after two-stage reranking):\n",
      "  1. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_1 (score: 90.0000)\n",
      "  2. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_52 (score: 85.0000)\n",
      "  3. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_2 (score: 80.0000)\n",
      "  4. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_3 (score: 75.0000)\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ROUTER: AFTER RETRIEVAL\n",
      "Quality: 70% (threshold: >=60%)\n",
      "Attempts: 2/2\n",
      "Issues: partial_coverage, missing_key_info\n",
      "Decision: answer_generation (quality acceptable)\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Question: How is attention mechanism used differently in NLP vs vision models?\n",
      "Context size: 3422 chars\n",
      "Retrieval quality: 70%\n",
      "Generation attempt: 1/3\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER EVALUATION (Refusal + Groundedness + Quality)\n",
      "Generation attempt: 1\n",
      "Retrieval quality: 70%\n",
      "Refusal detection: ATTEMPTED - The answer provides substantive information about how the attention mechanism is used in NLP and vision models, including details about Transformers in NLP and the integration of self-attention in vision models. It also discusses the Vision Transformer (ViT) and mentions the limitations of current models. Although it acknowledges that the context does not provide detailed examples or specific performance metrics, it still contains useful information.\n",
      "Groundedness: 87% (HHEM-2.1-Open (local))\n",
      "Quality: 65% (insufficient)\n",
      "Issues: partial_answer, missing_details\n",
      "Combined decision: RETRY\n",
      "============================================================\n",
      "\n",
      "\n",
      "Routing: answer_generation (attempt 2/3)\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Question: How is attention mechanism used differently in NLP vs vision models?\n",
      "Context size: 3422 chars\n",
      "Retrieval quality: 70%\n",
      "Generation attempt: 2/3\n",
      "============================================================\n",
      "\n",
      "RETRY MODE:\n",
      "Feedback:\n",
      "QUALITY ISSUES:\n",
      "Problems: partial_answer, missing_details\n",
      "Fix: Ensure all question parts are answered completely; Add more depth and explanation where the context provides supporting information\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER EVALUATION (Refusal + Groundedness + Quality)\n",
      "Generation attempt: 2\n",
      "Retrieval quality: 70%\n",
      "Refusal detection: ATTEMPTED - The answer provides substantive information about how the attention mechanism is used in NLP and vision models, detailing the differences in architecture and implementation. It discusses the use of Transformers in NLP and the role of CNNs in vision, along with the introduction of the Vision Transformer (ViT). Although it acknowledges limitations regarding specific examples and performance metrics, it still contains factual content, thus it is not a full refusal.\n",
      "Groundedness: 100% (HHEM-2.1-Open (local))\n",
      "Quality: 65% (insufficient)\n",
      "Issues: partial_answer, missing_details\n",
      "Combined decision: RETRY\n",
      "============================================================\n",
      "\n",
      "\n",
      "Routing: answer_generation (attempt 3/3)\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Question: How is attention mechanism used differently in NLP vs vision models?\n",
      "Context size: 3422 chars\n",
      "Retrieval quality: 70%\n",
      "Generation attempt: 3/3\n",
      "============================================================\n",
      "\n",
      "RETRY MODE:\n",
      "Feedback:\n",
      "QUALITY ISSUES:\n",
      "Problems: partial_answer, missing_details\n",
      "Fix: Ensure all question parts are answered completely; Add more depth and explanation where the context provides supporting information\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER EVALUATION (Refusal + Groundedness + Quality)\n",
      "Generation attempt: 3\n",
      "Retrieval quality: 70%\n",
      "Refusal detection: ATTEMPTED - The answer provides substantive information about how the attention mechanism is used in NLP and vision models, detailing the differences in architectural frameworks and the nature of the data. It explains the use of Transformers in NLP and mentions the Vision Transformer (ViT) in the context of computer vision. Although it acknowledges that the context does not provide specific examples or performance metrics, it still offers valuable insights into the topic.\n",
      "Groundedness: 86% (HHEM-2.1-Open (local))\n",
      "Quality: 65% (insufficient)\n",
      "Issues: partial_answer, missing_details\n",
      "Combined decision: RETRY\n",
      "============================================================\n",
      "\n",
      "\n",
      "Routing: END (max attempts reached)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "EXECUTION TRACE\n",
      "----------------------------------------\n",
      "Retrieval Attempts: 2\n",
      "Strategy Used: hybrid\n",
      "Final Retrieval Quality: 70%\n",
      "Issues Detected: partial_coverage, missing_key_info\n",
      "\n",
      "Generation Attempts: 3\n",
      "Groundedness: 86%\n",
      "Answer Sufficient: False\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "----------------------------------------\n",
      "The attention mechanism is used differently in natural language processing (NLP) and vision models primarily due to the architectural frameworks and the nature of the data being processed.\n",
      "\n",
      "In NLP, the Transformer architecture is the standard model, which employs self-attention directly on sequences of text. This allows for pre-training on large text corpora followed by fine-tuning on specific tasks. The computational efficiency and scalability of Transformers enable the training of very large models, with over 100 billion parameters, without performance saturation (as noted in the document \"AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\").\n",
      "\n",
      "In contrast, computer vision has traditionally relied on convolutional neural networks (CNNs). While there have been attempts to integrate self-attention into CNN architectures, many models still maintain a reliance on convolutions. Some approaches have tried to replace convolutions entirely with self-attention, but these models have struggled to scale effectively on modern hardware. Consequently, classic architectures like ResNet remain the state of the art in large-scale image recognition.\n",
      "\n",
      "The Vision Transformer (ViT) represents a significant shift by applying a pure Transformer directly to sequences of image patches, without the need for CNNs. This model has demonstrated competitive performance in image classification tasks compared to state-of-the-art convolutional networks while requiring fewer computational resources to train.\n",
      "\n",
      "However, the context does not provide specific examples of the operational implementation differences between the attention mechanisms in NLP and vision models, nor does it include performance metrics comparing the two domains directly.\n"
     ]
    }
   ],
   "source": [
    "# Self-Correction Example\n",
    "# This query might trigger self-correction due to cross-domain scope\n",
    "\n",
    "correction_query = \"How is attention mechanism used differently in NLP vs vision models?\"\n",
    "\n",
    "print(f\"Query: {correction_query}\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nRunning Advanced RAG with potential self-correction...\\n\")\n",
    "\n",
    "initial_state = {\n",
    "    \"user_question\": correction_query,\n",
    "    \"baseline_query\": correction_query,\n",
    "    \"messages\": [],\n",
    "    \"retrieved_docs\": [],\n",
    "    \"retrieval_attempts\": 0,\n",
    "    \"query_expansions\": [],\n",
    "}\n",
    "config = {\"configurable\": {\"thread_id\": \"demo-selfcorrect\"}}\n",
    "\n",
    "result = advanced_rag_graph.invoke(initial_state, config=config)\n",
    "\n",
    "# Display self-correction trace\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"EXECUTION TRACE\")\n",
    "print(\"-\"*40)\n",
    "print(f\"Retrieval Attempts: {result.get('retrieval_attempts', 1)}\")\n",
    "print(f\"Strategy Used: {result.get('retrieval_strategy', 'semantic')}\")\n",
    "\n",
    "if result.get('strategy_changed'):\n",
    "    print(f\"Strategy Changed: Yes\")\n",
    "    print(f\"  Reason: {result.get('strategy_switch_reason', 'N/A')}\")\n",
    "\n",
    "quality = result.get('retrieval_quality_score', 0) or 0\n",
    "print(f\"Final Retrieval Quality: {quality:.0%}\")\n",
    "\n",
    "if result.get('retrieval_quality_issues'):\n",
    "    print(f\"Issues Detected: {', '.join(result['retrieval_quality_issues'])}\")\n",
    "\n",
    "if result.get('retrieval_improvement_suggestion'):\n",
    "    print(f\"Improvement Suggestion: {result['retrieval_improvement_suggestion']}\")\n",
    "\n",
    "print(f\"\\nGeneration Attempts: {result.get('generation_attempts', 1)}\")\n",
    "print(f\"Groundedness: {(result.get('groundedness_score', 0) or 0):.0%}\")\n",
    "print(f\"Answer Sufficient: {result.get('is_answer_sufficient', True)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"-\"*40)\n",
    "print(result.get('final_answer', 'No answer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11-metrics",
   "metadata": {},
   "source": [
    "## Evaluation Metrics Summary\n",
    "\n",
    "All tiers use **budget models** (GPT-4o-mini only) to isolate architectural improvements from model quality.\n",
    "\n",
    "### Standard Dataset (20 questions, k=4)\n",
    "\n",
    "| Tier | F1@4 | MRR | nDCG@4 | Groundedness |\n",
    "|------|------|-----|--------|--------------|\n",
    "| Basic | 17.3% | 0.254 | 0.236 | 48.6% |\n",
    "| Intermediate | 22.7% | 0.450 | 0.343 | 70.7% |\n",
    "| Advanced | 29.3% | 0.600 | 0.484 | 64.1% |\n",
    "| **Multi-Agent** | **31.7%** | **0.600** | **0.497** | **76.6%** |\n",
    "\n",
    "*Maximum achievable F1@4 is 64.6% (dataset avg: 2.1 relevant docs/question). Multi-Agent achieves 49% of ceiling.*\n",
    "\n",
    "### Hard Dataset (10 questions, k=6, multi-document)\n",
    "\n",
    "| Tier | F1@6 | MRR | nDCG@6 | Groundedness |\n",
    "|------|------|-----|--------|--------------|\n",
    "| Basic | 22.0% | 0.458 | 0.300 | 60.4% |\n",
    "| Intermediate | 25.6% | 0.408 | 0.293 | 62.5% |\n",
    "| Advanced | 32.5% | **0.750** | 0.460 | **88.9%** |\n",
    "| **Multi-Agent** | **38.7%** | 0.633 | **0.480** | 87.0% |\n",
    "\n",
    "*Maximum achievable F1@6 is 84.8% (dataset avg: 4.7 relevant docs/question). Multi-Agent achieves 46% of ceiling.*\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- **83% retrieval improvement** (F1@4: 17.3% -> 31.7%) with budget models only\n",
    "- Multi-Agent shows +76% F1 improvement over Basic on hard dataset\n",
    "- Query decomposition helps find relevant documents across multiple aspects\n",
    "- Architecture provides value independent of model quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13-conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Architecture > Model Size** - The graph structure provides value independent of model quality. Budget tier demonstrates the RAG intelligence; higher tiers add polish.\n",
    "\n",
    "2. **Distributed Decision-Making** - No central orchestrator. The StateGraph itself is the agent, with routing functions encoding planning logic.\n",
    "\n",
    "3. **Quality-Driven Flow** - Every routing point evaluates results and decides next action. Poor retrieval triggers correction; poor generation triggers retry.\n",
    "\n",
    "4. **Multi-Agent for Complexity** - Query decomposition with parallel workers significantly improves retrieval on complex, multi-faceted questions.\n",
    "\n",
    "### Source Code\n",
    "\n",
    "```\n",
    "src/advanced_agentic_rag_langgraph/\n",
    "    core/              # State, model tiers, retriever setup\n",
    "    evaluation/        # Metrics framework (F1, MRR, nDCG)\n",
    "    orchestration/     # Main graph, nodes, routing\n",
    "    retrieval/         # Strategy selection, reranking\n",
    "    validation/        # HHEM hallucination detection\n",
    "    variants/          # Basic, Intermediate, Advanced, Multi-Agent\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced-agentic-rag-langgraph (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
