{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-1-intro",
   "metadata": {},
   "source": [
    "# Advanced Agentic RAG with LangGraph\n",
    "\n",
    "**A portfolio project showcasing intelligent, adaptive retrieval pipelines**\n",
    "\n",
    "This system demonstrates production-grade RAG architecture patterns:\n",
    "\n",
    "- **Dynamic strategy selection** - Semantic, keyword, or hybrid retrieval based on query analysis\n",
    "- **Quality-driven self-correction** - Automatic query rewrites when retrieval quality is insufficient\n",
    "- **Multi-stage reranking** - CrossEncoder (top-10) + LLM-as-judge (top-4) for precision\n",
    "- **HHEM-based hallucination detection** - Claim decomposition with per-chunk HHEM-2.1-Open verification\n",
    "- **Multi-agent parallel retrieval** - Query decomposition with parallel workers for complex questions\n",
    "\n",
    "**Architecture**: 7-node StateGraph with distributed intelligence (no central orchestrator)  \n",
    "**Framework**: LangChain 1.0 & LangGraph 1.0  \n",
    "**Pattern**: Dynamic Planning and Execution Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2-setup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T07:55:37.058491Z",
     "iopub.status.busy": "2025-11-27T07:55:37.058491Z",
     "iopub.status.idle": "2025-11-27T07:56:34.893409Z",
     "shell.execute_reply": "2025-11-27T07:56:34.891880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG variants loaded:\n",
      "  - basic_rag_graph: Simplest RAG (semantic search only)\n",
      "  - intermediate_rag_graph: Query expansion + hybrid + reranking\n",
      "  - advanced_rag_graph: Full agentic RAG with self-correction\n",
      "  - multi_agent_rag_graph: Parallel retrieval workers\n"
     ]
    }
   ],
   "source": [
    "# Setup & Imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# All RAG graph variants\n",
    "from advanced_agentic_rag_langgraph.variants import (\n",
    "    basic_rag_graph,\n",
    "    intermediate_rag_graph,\n",
    "    advanced_rag_graph,\n",
    "    multi_agent_rag_graph,\n",
    ")\n",
    "\n",
    "print(\"RAG variants loaded:\")\n",
    "print(\"  - basic_rag_graph: Simplest RAG (semantic search only)\")\n",
    "print(\"  - intermediate_rag_graph: Query expansion + hybrid + reranking\")\n",
    "print(\"  - advanced_rag_graph: Full agentic RAG with self-correction\")\n",
    "print(\"  - multi_agent_rag_graph: Parallel retrieval workers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3-diagram",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T07:56:34.897447Z",
     "iopub.status.busy": "2025-11-27T07:56:34.897447Z",
     "iopub.status.idle": "2025-11-27T07:56:36.525070Z",
     "shell.execute_reply": "2025-11-27T07:56:36.524050Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Architecture Diagram - LangGraph Mermaid Visualization\n",
    "from IPython.display import Image\n",
    "\n",
    "# Render the Advanced RAG graph structure\n",
    "Image(advanced_rag_graph.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4-routing",
   "metadata": {},
   "source": [
    "## Routing Logic (What the Diagram Doesn't Show)\n",
    "\n",
    "The graph visualization shows nodes and edges, but the **conditional routing logic** is where the \"intelligence\" lives:\n",
    "\n",
    "### `route_after_retrieval` - Quality-Based Branching\n",
    "```python\n",
    "if quality >= 0.6:\n",
    "    return \"answer_generation\"  # Good enough - proceed\n",
    "\n",
    "if attempts >= 2:\n",
    "    return \"answer_generation\"  # Max attempts - proceed anyway\n",
    "\n",
    "if (\"off_topic\" in issues or \"wrong_domain\" in issues) and (attempts == 1):\n",
    "    return \"query_expansion\"    # Early strategy switch\n",
    "else:\n",
    "    return \"rewrite_and_refine\" # Semantic rewrite\n",
    "```\n",
    "\n",
    "### `route_after_evaluation` - Answer Quality Gate\n",
    "```python\n",
    "if is_refusal:\n",
    "    return END  # LLM refused - terminal state\n",
    "\n",
    "if is_answer_sufficient:\n",
    "    return END\n",
    "\n",
    "if generation_attempts < 3:\n",
    "    return \"answer_generation\"  # Retry with feedback\n",
    "else:\n",
    "    return END  # Max attempts reached\n",
    "```\n",
    "\n",
    "### Key Design Principles\n",
    "- **Fix generation with generation** - Don't re-retrieve for generation problems\n",
    "- **Single correction cycle** - Research shows diminishing returns after first retry\n",
    "- **Adaptive thresholds** - Lower quality bar (50%) when retrieval is poor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5-retriever",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T07:56:36.529086Z",
     "iopub.status.busy": "2025-11-27T07:56:36.529086Z",
     "iopub.status.idle": "2025-11-27T07:57:38.702131Z",
     "shell.execute_reply": "2025-11-27T07:57:38.700599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOADING ALL PDFS FROM docs/\n",
      "============================================================\n",
      "\n",
      "Found 10 PDF file(s):\n",
      "  1. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf (3.6 MB)\n",
      "  2. Attention Is All You Need.pdf (2.1 MB)\n",
      "  3. BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf (0.7 MB)\n",
      "  4. Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf (30.1 MB)\n",
      "  5. Denoising Diffusion Probabilistic Models.pdf (9.8 MB)\n",
      "  6. Expanding Horizons in RAG - Exploring and Extending the Limits of RAPTOR.pdf (0.6 MB)\n",
      "  7. Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf (40.9 MB)\n",
      "  8. Improved Training of Wasserstein GANs.pdf (5.9 MB)\n",
      "  9. Learning Transferable Visual Models From Natural Language Supervision.pdf (6.5 MB)\n",
      "  10. U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf (1.6 MB)\n",
      "\n",
      "============================================================\n",
      "STEP 1: Loading full documents (no chunking)\n",
      "============================================================\n",
      "\n",
      "\n",
      "Loading 10 PDFs as full documents (no chunking)...\n",
      "Loading full document: AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf\n",
      "  Pages: 22, Characters: 67,154\n",
      "Loading full document: Attention Is All You Need.pdf\n",
      "  Pages: 15, Characters: 39,511\n",
      "Loading full document: BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf\n",
      "  Pages: 16, Characters: 64,131\n",
      "Loading full document: Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf\n",
      "  Pages: 42, Characters: 97,294\n",
      "Loading full document: Denoising Diffusion Probabilistic Models.pdf\n",
      "  Pages: 25, Characters: 54,041\n",
      "Loading full document: Expanding Horizons in RAG - Exploring and Extending the Limits of RAPTOR.pdf\n",
      "  Pages: 11, Characters: 37,149\n",
      "Loading full document: Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf\n",
      "  Pages: 27, Characters: 56,402\n",
      "Loading full document: Improved Training of Wasserstein GANs.pdf\n",
      "  Pages: 20, Characters: 41,584\n",
      "Loading full document: Learning Transferable Visual Models From Natural Language Supervision.pdf\n",
      "  Pages: 48, Characters: 224,303\n",
      "Loading full document: U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf\n",
      "  Pages: 8, Characters: 19,623\n",
      "\n",
      "Loaded 10 full documents, 701,192 total characters\n",
      "\n",
      "============================================================\n",
      "STEP 2: Profiling documents with LLM\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "DOCUMENT PROFILING\n",
      "============================================================\n",
      "Profiling 10 documents...\n",
      "\n",
      "Document 1 (doc_0):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.80\n",
      "  Reading Level: advanced\n",
      "  Domain: computer_vision, transformers, deep_learning\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "Document 2 (doc_1):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.90\n",
      "  Reading Level: advanced\n",
      "  Domain: machine_learning, deep_learning, nlp\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "Document 3 (doc_2):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.90\n",
      "  Reading Level: advanced\n",
      "  Domain: nlp, transformers, deep_learning\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "Document 4 (doc_3):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.90\n",
      "  Reading Level: advanced\n",
      "  Domain: machine_learning, generative_models, diffusion_models\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "Document 5 (doc_4):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.90\n",
      "  Reading Level: advanced\n",
      "  Domain: machine_learning, deep_learning, computer_vision\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "Document 6 (doc_5):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.80\n",
      "  Reading Level: advanced\n",
      "  Domain: nlp, retrieval_augmented_generation, machine_learning\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "Document 7 (doc_6):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.80\n",
      "  Reading Level: advanced\n",
      "  Domain: computer_vision, machine_learning, generative_models\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "Document 8 (doc_7):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.90\n",
      "  Reading Level: advanced\n",
      "  Domain: machine_learning, deep_learning, generative_adversarial_networks\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "Document 9 (doc_8):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.90\n",
      "  Reading Level: advanced\n",
      "  Domain: computer_vision, natural_language_processing, machine_learning\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "Document 10 (doc_9):\n",
      "  Type: conference_paper\n",
      "  Technical Density: 0.80\n",
      "  Reading Level: advanced\n",
      "  Domain: computer_vision, deep_learning, biomedical_image_segmentation\n",
      "  Best Strategy: hybrid\n",
      "\n",
      "============================================================\n",
      "CORPUS STATISTICS\n",
      "============================================================\n",
      "Total Documents: 10\n",
      "Average Technical Density: 0.86\n",
      "\n",
      "Document Types:\n",
      "  - conference_paper: 10 (100.0%)\n",
      "\n",
      "Domain Distribution:\n",
      "  - computer_vision: 5\n",
      "  - transformers: 2\n",
      "  - deep_learning: 6\n",
      "  - machine_learning: 7\n",
      "  - nlp: 3\n",
      "  - generative_models: 2\n",
      "  - diffusion_models: 1\n",
      "  - retrieval_augmented_generation: 1\n",
      "  - generative_adversarial_networks: 1\n",
      "  - natural_language_processing: 1\n",
      "  - biomedical_image_segmentation: 1\n",
      "\n",
      "Percentage with Code: 50.0%\n",
      "Percentage with Math: 60.0%\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "STEP 3: Chunking documents\n",
      "============================================================\n",
      "\n",
      "Created 924 chunks from 10 documents\n",
      "\n",
      "============================================================\n",
      "CORPUS STATISTICS\n",
      "============================================================\n",
      "Total documents: 10\n",
      "Total chunks: 924\n",
      "Avg technical density: 0.86\n",
      "Document types: {'conference_paper': 10}\n",
      "Has code: 50%\n",
      "Has math: 60%\n",
      "============================================================\n",
      "\n",
      "\n",
      "Retriever initialized with 10 research papers (924 chunks)\n",
      "Papers include: Transformers, BERT, ViT, DDPM, CLIP, U-Net, WGAN-GP, Consistency Models\n"
     ]
    }
   ],
   "source": [
    "# Initialize Retriever (one-time setup)\n",
    "# This loads 10 research papers and creates the vector store\n",
    "from advanced_agentic_rag_langgraph.core import setup_retriever\n",
    "\n",
    "k_final=4\n",
    "retriever = setup_retriever(k_final=k_final)\n",
    "print(\"\\nRetriever initialized with 10 research papers (924 chunks)\")\n",
    "print(\"Papers include: Transformers, BERT, ViT, DDPM, CLIP, U-Net, WGAN-GP, Consistency Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6-tiers",
   "metadata": {},
   "source": [
    "## 4-Tier Architecture Comparison\n",
    "\n",
    "Each tier adds capabilities while using the **same budget model tier** (GPT-4o-mini) to isolate architectural improvements from model quality:\n",
    "\n",
    "| Tier | Features | Key Additions |\n",
    "|------|----------|---------------|\n",
    "| **Basic** | 1 | Semantic search only, direct LLM generation |\n",
    "| **Intermediate** | 5 | + Query expansion, hybrid retrieval, CrossEncoder reranking, RRF fusion |\n",
    "| **Advanced** | 17 | + Strategy selection, two-stage reranking, HHEM detection, quality gates, self-correction loops |\n",
    "| **Multi-Agent** | 20 | + Query decomposition, parallel retrieval workers, cross-agent LLM relevance scoring |\n",
    "\n",
    "### Feature Progression\n",
    "- **Basic**: Pure semantic similarity - works well for simple, direct questions\n",
    "- **Intermediate**: Query variations improve recall, reranking improves precision\n",
    "- **Advanced**: Adapts strategy based on query type, retries on poor results\n",
    "- **Multi-Agent**: Decomposes complex questions, retrieves in parallel, merges results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ccd786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Describe the complete training procedure for the Transformer, including optimization, regularization, and learning rate schedule.\n",
      "Ground Truth Chunks: 4 (from attention paper)\n",
      "================================================================================\n",
      "\n",
      "Running Basic RAG...\n",
      "\n",
      "============================================================\n",
      "BASIC RETRIEVAL\n",
      "Strategy: semantic only (vector similarity)\n",
      "Top-K: 4 chunks (no reranking)\n",
      "Retrieved: 4 documents\n",
      "\n",
      "All 4 chunk IDs (rank order):\n",
      "  1. Attention Is All You Need.pdf_chunk_6\n",
      "  2. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_31\n",
      "  3. Attention Is All You Need.pdf_chunk_10\n",
      "  4. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_69\n",
      "\n",
      "Expected chunks: ['Attention Is All You Need.pdf_chunk_28', 'Attention Is All You Need.pdf_chunk_29', 'Attention Is All You Need.pdf_chunk_30', 'Attention Is All You Need.pdf_chunk_31']\n",
      "Found: [] | Missing: ['Attention Is All You Need.pdf_chunk_28', 'Attention Is All You Need.pdf_chunk_29', 'Attention Is All You Need.pdf_chunk_30', 'Attention Is All You Need.pdf_chunk_31']\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Answer length: 1603 chars\n",
      "Context docs: 4\n",
      "============================================================\n",
      "\n",
      "  F1@4: 0%\n",
      "\n",
      "Running Intermediate RAG...\n",
      "\n",
      "============================================================\n",
      "QUERY EXPANSION\n",
      "Original: Describe the complete training procedure for the Transformer, including optimization, regularization, and learning rate schedule.\n",
      "Generated 4 variations\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "HYBRID RETRIEVAL WITH RRF FUSION\n",
      "Strategy: hybrid (always)\n",
      "Query variants: 4\n",
      "Total retrievals: 55\n",
      "Unique docs after RRF: 31\n",
      "\n",
      "All 31 chunk IDs (RRF scores):\n",
      "  1. Attention Is All You Need.pdf_chunk_6 (0.0648)\n",
      "  2. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_3 (0.0635)\n",
      "  3. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_31 (0.0623)\n",
      "  4. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_69 (0.0618)\n",
      "  5. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_5 (0.0607)\n",
      "  6. Attention Is All You Need.pdf_chunk_5 (0.0455)\n",
      "  7. Attention Is All You Need.pdf_chunk_1 (0.0429)\n",
      "  8. Attention Is All You Need.pdf_chunk_10 (0.0320)\n",
      "  9. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_71 (0.0292)\n",
      "  10. Attention Is All You Need.pdf_chunk_8 (0.0288)\n",
      "  11. Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_110 (0.0280)\n",
      "  12. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_58 (0.0278)\n",
      "  13. Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_54 (0.0156)\n",
      "  14. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_12 (0.0152)\n",
      "  15. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_28 (0.0152)\n",
      "  16. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_20 (0.0149)\n",
      "  17. Attention Is All You Need.pdf_chunk_28 (0.0147)\n",
      "  18. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_32 (0.0147)\n",
      "  19. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_42 (0.0147)\n",
      "  20. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_31 (0.0145)\n",
      "  21. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_38 (0.0145)\n",
      "  22. BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_73 (0.0141)\n",
      "  23. Expanding Horizons in RAG - Exploring and Extending the Limits of RAPTOR.pdf_chunk_6 (0.0141)\n",
      "  24. Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_36 (0.0139)\n",
      "  25. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_70 (0.0139)\n",
      "  26. Expanding Horizons in RAG - Exploring and Extending the Limits of RAPTOR.pdf_chunk_5 (0.0139)\n",
      "  27. BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_69 (0.0137)\n",
      "  28. Expanding Horizons in RAG - Exploring and Extending the Limits of RAPTOR.pdf_chunk_7 (0.0137)\n",
      "  29. Denoising Diffusion Probabilistic Models.pdf_chunk_61 (0.0135)\n",
      "  30. Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_33 (0.0135)\n",
      "  31. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_121 (0.0135)\n",
      "\n",
      "Expected chunks: ['Attention Is All You Need.pdf_chunk_28', 'Attention Is All You Need.pdf_chunk_29', 'Attention Is All You Need.pdf_chunk_30', 'Attention Is All You Need.pdf_chunk_31']\n",
      "Found: ['Attention Is All You Need.pdf_chunk_28'] | Missing: ['Attention Is All You Need.pdf_chunk_29', 'Attention Is All You Need.pdf_chunk_30', 'Attention Is All You Need.pdf_chunk_31']\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "CROSSENCODER RERANKING\n",
      "Input: 31 documents\n",
      "\n",
      "Chunk IDs sent to reranking (top-15):\n",
      "  1. Attention Is All You Need.pdf_chunk_6\n",
      "  2. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_3\n",
      "  3. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_31\n",
      "  4. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_69\n",
      "  5. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_5\n",
      "  6. Attention Is All You Need.pdf_chunk_5\n",
      "  7. Attention Is All You Need.pdf_chunk_1\n",
      "  8. Attention Is All You Need.pdf_chunk_10\n",
      "  9. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_71\n",
      "  10. Attention Is All You Need.pdf_chunk_8\n",
      "  ... and 5 more\n",
      "\n",
      "Expected chunks in reranking input:\n",
      "Found: [] | Missing: ['Attention Is All You Need.pdf_chunk_28', 'Attention Is All You Need.pdf_chunk_29', 'Attention Is All You Need.pdf_chunk_30', 'Attention Is All You Need.pdf_chunk_31']\n",
      "\n",
      "Output: 4 documents after CrossEncoder reranking\n",
      "\n",
      "Final chunk IDs (after CrossEncoder reranking):\n",
      "  1. Attention Is All You Need.pdf_chunk_1 (score: -0.6854)\n",
      "  2. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_28 (score: -1.6279)\n",
      "  3. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_12 (score: -2.4223)\n",
      "  4. Attention Is All You Need.pdf_chunk_6 (score: -2.7427)\n",
      "\n",
      "Expected chunks in final results:\n",
      "Found: [] | Missing: ['Attention Is All You Need.pdf_chunk_28', 'Attention Is All You Need.pdf_chunk_29', 'Attention Is All You Need.pdf_chunk_30', 'Attention Is All You Need.pdf_chunk_31']\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Answer length: 2194 chars\n",
      "Context docs: 4\n",
      "============================================================\n",
      "\n",
      "  F1@4: 0%\n",
      "\n",
      "Running Advanced RAG...\n",
      "\n",
      "============================================================\n",
      "STRATEGY SELECTION\n",
      "Query: Describe the complete training procedure for the Transformer, including optimization, regularization, and learning rate schedule.\n",
      "Selected: SEMANTIC\n",
      "Confidence: 90%\n",
      "Reasoning: The user is seeking a detailed explanation of the training procedure for the Transformer model, which indicates a desire for conceptual understanding and comprehensive information. This aligns with the semantic search strategy, as it can provide explanations and descriptions that cover the various aspects of training, including optimization, regularization, and learning rate schedules. Exact matching is less critical here since the user is looking for a broad understanding rather than specific terms or identifiers. Additionally, the corpus is technical and focused on deep learning and transformers, making semantic search the most suitable choice for retrieving relevant content.\n",
      "Note: Query optimization will happen in query_expansion_node\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "STRATEGY-SPECIFIC QUERY OPTIMIZATION\n",
      "Strategy: semantic\n",
      "Original query: Describe the complete training procedure for the Transformer, including optimization, regularization, and learning rate schedule.\n",
      "Optimized query: Explain the comprehensive training process for the Transformer model, detailing aspects of optimization strategies, methods for regularization, and the frameworks for learning rate scheduling.\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EXPANSION DECISION\n",
      "Query: Explain the comprehensive training process for the Transformer model, detailing aspects of optimization strategies, methods for regularization, and the frameworks for learning rate scheduling.\n",
      "LLM decision: EXPAND query\n",
      "Reasoning: The query is complex and covers multiple aspects of the Transformer model's training process, including optimization strategies, regularization methods, and learning rate scheduling. Expanding it into variations can help capture different terminologies and phrasing that users might employ when searching for this information, thus improving retrieval effectiveness.\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "QUERY EXPANDED\n",
      "Optimized query: Explain the comprehensive training process for the Transformer model, detailing aspects of optimization strategies, methods for regularization, and the frameworks for learning rate scheduling.\n",
      "Expansions: ['What are the specific optimization strategies, regularization methods, and learning rate scheduling frameworks used in the training process of the Transformer model?', \"What are the practical applications of the Transformer model's training process, particularly regarding optimization strategies and regularization techniques?\", 'What are the foundational concepts behind the training process of the Transformer model, including its optimization strategies, regularization methods, and learning rate scheduling principles?']\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "RETRIEVAL EXECUTION START\n",
      "Using 4 query expansion(s)\n",
      "Expansions generated from: retrieval_query\n",
      "Retrieval strategy: semantic\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "RRF MULTI-QUERY RETRIEVAL\n",
      "Query variants: 4\n",
      "Total retrievals: 60\n",
      "Unique docs after RRF: 27\n",
      "\n",
      "All 27 chunk IDs (RRF scores):\n",
      "  1. Attention Is All You Need.pdf_chunk_6 (0.0645)\n",
      "  2. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_3 (0.0636)\n",
      "  3. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_69 (0.0613)\n",
      "  4. Attention Is All You Need.pdf_chunk_5 (0.0612)\n",
      "  5. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_5 (0.0604)\n",
      "  6. Attention Is All You Need.pdf_chunk_1 (0.0541)\n",
      "  7. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_31 (0.0474)\n",
      "  8. Attention Is All You Need.pdf_chunk_10 (0.0459)\n",
      "  9. Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_54 (0.0453)\n",
      "  10. Attention Is All You Need.pdf_chunk_28 (0.0439)\n",
      "  11. Attention Is All You Need.pdf_chunk_2 (0.0436)\n",
      "  12. Attention Is All You Need.pdf_chunk_8 (0.0429)\n",
      "  13. Attention Is All You Need.pdf_chunk_17 (0.0285)\n",
      "  14. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_30 (0.0276)\n",
      "  15. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_71 (0.0276)\n",
      "  16. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_20 (0.0154)\n",
      "  17. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_32 (0.0145)\n",
      "  18. Attention Is All You Need.pdf_chunk_35 (0.0143)\n",
      "  19. Attention Is All You Need.pdf_chunk_32 (0.0141)\n",
      "  20. BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_42 (0.0141)\n",
      "  21. Attention Is All You Need.pdf_chunk_39 (0.0141)\n",
      "  22. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_20 (0.0139)\n",
      "  23. Attention Is All You Need.pdf_chunk_37 (0.0139)\n",
      "  24. BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_12 (0.0137)\n",
      "  25. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_28 (0.0135)\n",
      "  26. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_31 (0.0133)\n",
      "  27. Denoising Diffusion Probabilistic Models.pdf_chunk_59 (0.0133)\n",
      "\n",
      "Expected chunks: ['Attention Is All You Need.pdf_chunk_28', 'Attention Is All You Need.pdf_chunk_29', 'Attention Is All You Need.pdf_chunk_30', 'Attention Is All You Need.pdf_chunk_31']\n",
      "Found: ['Attention Is All You Need.pdf_chunk_28'] | Missing: ['Attention Is All You Need.pdf_chunk_29', 'Attention Is All You Need.pdf_chunk_30', 'Attention Is All You Need.pdf_chunk_31']\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "TWO-STAGE RERANKING (After RRF)\n",
      "Input: 27 docs (from RRF top-40)\n",
      "\n",
      "Chunk IDs sent to reranking (top-40):\n",
      "  1. Attention Is All You Need.pdf_chunk_6\n",
      "  2. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_3\n",
      "  3. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_69\n",
      "  4. Attention Is All You Need.pdf_chunk_5\n",
      "  5. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_5\n",
      "  6. Attention Is All You Need.pdf_chunk_1\n",
      "  7. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_31\n",
      "  8. Attention Is All You Need.pdf_chunk_10\n",
      "  9. Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_54\n",
      "  10. Attention Is All You Need.pdf_chunk_28\n",
      "  ... and 17 more\n",
      "\n",
      "Expected chunks in reranking input:\n",
      "Found: ['Attention Is All You Need.pdf_chunk_28'] | Missing: ['Attention Is All You Need.pdf_chunk_29', 'Attention Is All You Need.pdf_chunk_30', 'Attention Is All You Need.pdf_chunk_31']\n",
      "\n",
      "============================================================\n",
      "RERANKING QUERY SOURCE\n",
      "Using: active_query (semantic, human-readable)\n",
      "Query: Describe the complete training procedure for the Transformer, including optimization, regularization, and learning rate schedule.\n",
      "Note: Reranking uses semantic query, NOT algorithm-optimized retrieval_query\n",
      "============================================================\n",
      "\n",
      "\n",
      "Output: 4 docs after two-stage reranking\n",
      "\n",
      "Final chunk IDs (after two-stage reranking):\n",
      "  1. Attention Is All You Need.pdf_chunk_28 (score: 95.0000)\n",
      "  2. Attention Is All You Need.pdf_chunk_1 (score: 85.0000)\n",
      "  3. Attention Is All You Need.pdf_chunk_35 (score: 80.0000)\n",
      "  4. Attention Is All You Need.pdf_chunk_6 (score: 75.0000)\n",
      "\n",
      "Expected chunks in final results:\n",
      "Found: ['Attention Is All You Need.pdf_chunk_28'] | Missing: ['Attention Is All You Need.pdf_chunk_29', 'Attention Is All You Need.pdf_chunk_30', 'Attention Is All You Need.pdf_chunk_31']\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "RETRIEVAL METRICS (Golden Dataset Evaluation)\n",
      "============================================================\n",
      "Recall@4:    25.00%\n",
      "Precision@4: 25.00%\n",
      "F1@4:        25.00%\n",
      "Hit Rate:    100.00%\n",
      "MRR:         1.0000\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ROUTER: AFTER RETRIEVAL\n",
      "Quality: 75% (threshold: >=60%)\n",
      "Attempts: 1/2\n",
      "Issues: partial_coverage, missing_key_info\n",
      "Decision: answer_generation (quality acceptable)\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Question: Describe the complete training procedure for the Transformer, including optimization, regularization, and learning rate schedule.\n",
      "Context size: 3839 chars\n",
      "Retrieval quality: 75%\n",
      "Generation attempt: 1/3\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER EVALUATION (Refusal + Groundedness + Quality)\n",
      "Generation attempt: 1\n",
      "Retrieval quality: 75%\n",
      "Refusal detection: ATTEMPTED - The answer provides substantive information about the training procedure for the Transformer, including details on optimization (Adam optimizer and learning rate schedule) and mentions regularization and training duration. Although it acknowledges gaps in the specifics of regularization, it still contains useful information.\n",
      "Groundedness: 82%\n",
      "Quality: 65% (insufficient)\n",
      "Issues: partial_answer, missing_details\n",
      "Combined decision: RETRY\n",
      "============================================================\n",
      "\n",
      "\n",
      "Routing: answer_generation (attempt 2/3)\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Question: Describe the complete training procedure for the Transformer, including optimization, regularization, and learning rate schedule.\n",
      "Context size: 3839 chars\n",
      "Retrieval quality: 75%\n",
      "Generation attempt: 2/3\n",
      "============================================================\n",
      "\n",
      "RETRY MODE:\n",
      "Feedback:\n",
      "QUALITY ISSUES:\n",
      "Problems: partial_answer, missing_details\n",
      "Fix: Ensure all question parts are answered completely; Add more depth and explanation where the context provides supporting information\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER EVALUATION (Refusal + Groundedness + Quality)\n",
      "Generation attempt: 2\n",
      "Retrieval quality: 75%\n",
      "Refusal detection: ATTEMPTED - The assistant provided substantive information about the training procedure for the Transformer, including details on optimization (Adam optimizer and learning rate formula), regularization (acknowledged but not specified), and training duration. Although it noted a gap regarding the specific types of regularization, it still included relevant facts about the training process.\n",
      "Groundedness: 64%\n",
      "Quality: 70% (insufficient)\n",
      "Issues: missing_details, partial_answer\n",
      "Combined decision: RETRY\n",
      "============================================================\n",
      "\n",
      "\n",
      "Routing: answer_generation (attempt 3/3)\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Question: Describe the complete training procedure for the Transformer, including optimization, regularization, and learning rate schedule.\n",
      "Context size: 3839 chars\n",
      "Retrieval quality: 75%\n",
      "Generation attempt: 3/3\n",
      "============================================================\n",
      "\n",
      "RETRY MODE:\n",
      "Feedback:\n",
      "HALLUCINATION DETECTED (64% grounded):\n",
      "Unsupported claims: The training procedure for the Transformer model includes optimization, regularization, and a learning rate schedule., The learning rate increases linearly for the first 4,000 steps., The learning rate decreases proportionally to the inverse square root of the step number after the first 4,000 steps., Three types of regularization were employed during training to help prevent overfitting.\n",
      "Fix: ONLY state facts explicitly in retrieved context. If information is missing, acknowledge the limitation rather than adding unsupported details.\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER EVALUATION (Refusal + Groundedness + Quality)\n",
      "Generation attempt: 3\n",
      "Retrieval quality: 75%\n",
      "Refusal detection: ATTEMPTED - The assistant provided substantive information about the optimization process, learning rate schedule, and training duration for the Transformer model. Although it acknowledged a gap regarding the specific types of regularization, it still included relevant details from the context, which qualifies as a partial answer rather than a full refusal.\n",
      "Groundedness: 64%\n",
      "Quality: 65% (insufficient)\n",
      "Issues: missing_details, partial_answer\n",
      "Combined decision: RETRY\n",
      "============================================================\n",
      "\n",
      "\n",
      "Routing: END (max attempts reached)\n",
      "  F1@4: 25%\n",
      "\n",
      "Running Multi-Agent RAG...\n",
      "\n",
      "============================================================\n",
      "COMPLEXITY CLASSIFICATION\n",
      "Query: Describe the complete training procedure for the Transformer, including optimization, regularization, and learning rate schedule.\n",
      "Classification: COMPLEX\n",
      "Reasoning: The query involves multiple distinct aspects: the training procedure, optimization, regularization, and learning rate schedule, which would benefit from focused sub-queries for clarity and depth.\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "QUERY DECOMPOSITION (ORCHESTRATOR)\n",
      "Original: Describe the complete training procedure for the Transformer, including optimization, regularization, and learning rate schedule.\n",
      "Sub-queries (4):\n",
      "  1. What is the complete training procedure for the Transformer?\n",
      "  2. What optimization techniques are used in the training of the Transformer?\n",
      "  3. What regularization methods are applied during the training of the Transformer?\n",
      "  4. What learning rate schedule is implemented in the training of the Transformer?\n",
      "Reasoning: Four aspects (complete training procedure, optimization, regularization, learning rate schedule) = 4 sub-queries.\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ASSIGN WORKERS (Send API)\n",
      "Spawning 4 parallel retrieval workers\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "RETRIEVAL WORKER 0\n",
      "Sub-query: What is the complete training procedure for the Transformer?\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "RETRIEVAL WORKER 1\n",
      "Sub-query: What optimization techniques are used in the training of the Transformer?\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "RETRIEVAL WORKER 2\n",
      "Sub-query: What regularization methods are applied during the training of the Transformer?\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "RETRIEVAL WORKER 3\n",
      "Sub-query: What learning rate schedule is implemented in the training of the Transformer?\n",
      "============================================================\n",
      "  [Worker] Strategy: semantic (90%)\n",
      "  [Worker] Strategy: semantic (90%)\n",
      "  [Worker] Strategy: semantic (90%)\n",
      "  [Worker] Strategy: semantic (95%)\n",
      "\n",
      "============================================================\n",
      "STRATEGY-SPECIFIC QUERY OPTIMIZATION\n",
      "Strategy: semantic\n",
      "Original query: What learning rate schedule is implemented in the training of the Transformer?\n",
      "Optimized query: What are the strategies and principles behind the learning rate schedule used during the training of the Transformer model?\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "STRATEGY-SPECIFIC QUERY OPTIMIZATION\n",
      "Strategy: semantic\n",
      "Original query: What optimization techniques are used in the training of the Transformer?\n",
      "Optimized query: What strategies and methodologies enhance the training process of the Transformer model?\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "============================================================\n",
      "STRATEGY-SPECIFIC QUERY OPTIMIZATION\n",
      "Strategy: semantic\n",
      "Original query: What is the complete training procedure for the Transformer?\n",
      "Optimized query: What are the comprehensive steps and methodologies involved in training the Transformer model?\n",
      "============================================================\n",
      "\n",
      "\n",
      "STRATEGY-SPECIFIC QUERY OPTIMIZATION\n",
      "Strategy: semantic\n",
      "Original query: What regularization methods are applied during the training of the Transformer?\n",
      "Optimized query: What are the various regularization techniques utilized in the training process of the Transformer model?\n",
      "============================================================\n",
      "\n",
      "  [Worker] Retrieved 4 docs, quality: 60%, attempt: 1/2\n",
      "Worker 1 complete: 4 docs, quality: 60%\n",
      "  [Worker] Retrieved 4 docs, quality: 50%, attempt: 1/2\n",
      "  [Worker] Retrieved 4 docs, quality: 60%, attempt: 1/2\n",
      "Worker 0 complete: 4 docs, quality: 60%\n",
      "  [Worker] Retrieved 4 docs, quality: 50%, attempt: 1/2\n",
      "  [Worker] Keywords injected: What learning rate schedule is implemented in the ... -> What learning rate schedule, including learning ra...\n",
      "  [Worker] Keywords injected: What regularization methods are applied during the... -> What regularization methods, such as dropout, laye...\n",
      "\n",
      "============================================================\n",
      "STRATEGY-SPECIFIC QUERY OPTIMIZATION\n",
      "Strategy: semantic\n",
      "Original query: What learning rate schedule, including learning rate decay and warmup steps, is implemented in the training hyperparameters and optimizer settings of the Transformer?\n",
      "Optimized query: What are the various learning rate schedules, such as learning rate decay and warmup steps, utilized in the hyperparameters and optimizer settings during the training of the Transformer?\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "STRATEGY-SPECIFIC QUERY OPTIMIZATION\n",
      "Strategy: semantic\n",
      "Original query: What regularization methods, such as dropout, layer normalization, data augmentation, early stopping, and weight regularization, are commonly applied during the training of the Transformer?\n",
      "Optimized query: What are the various techniques for regularization, including dropout, layer normalization, data augmentation, early stopping, and weight regularization, that are typically utilized to enhance the training process of the Transformer model?\n",
      "============================================================\n",
      "\n",
      "  [Worker] Retrieved 4 docs, quality: 60%, attempt: 2/2\n",
      "Worker 2 complete: 4 docs, quality: 60%\n",
      "  [Worker] Retrieved 4 docs, quality: 85%, attempt: 2/2\n",
      "Worker 3 complete: 4 docs, quality: 85%\n",
      "\n",
      "============================================================\n",
      "MERGE RESULTS (SYNTHESIZER)\n",
      "Merging results from 4 workers\n",
      "Deduplication: 16 total -> 11 unique -> 11 candidates\n",
      "Expected chunks: Found ['Attention Is All You Need.pdf_chunk_28', 'Attention Is All You Need.pdf_chunk_30'] | Missing ['Attention Is All You Need.pdf_chunk_29', 'Attention Is All You Need.pdf_chunk_31']\n",
      "\n",
      "============================================================\n",
      "LLM RELEVANCE SCORING (Multi-Agent Merge)\n",
      "Original question: Describe the complete training procedure for the Transformer, including optimization, regularization, and learning rate schedule.\n",
      "Candidates: 11\n",
      "\n",
      "Chunk IDs before scoring:\n",
      "  1. Attention Is All You Need.pdf_chunk_1\n",
      "  2. Attention Is All You Need.pdf_chunk_6\n",
      "  3. Attention Is All You Need.pdf_chunk_30\n",
      "  4. Attention Is All You Need.pdf_chunk_35\n",
      "  5. Attention Is All You Need.pdf_chunk_5\n",
      "  6. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_32\n",
      "  7. Attention Is All You Need.pdf_chunk_10\n",
      "  8. Attention Is All You Need.pdf_chunk_28\n",
      "  9. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_22\n",
      "  10. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_55\n",
      "  11. BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_69\n",
      "\n",
      "LLM Scores (all 11 candidates):\n",
      "  1. Attention Is All You Need.pdf_chunk_1 (score: 40.0)\n",
      "  2. Attention Is All You Need.pdf_chunk_6 (score: 40.0)\n",
      "  3. Attention Is All You Need.pdf_chunk_30 (score: 75.0)\n",
      "  4. Attention Is All You Need.pdf_chunk_35 (score: 60.0)\n",
      "  5. Attention Is All You Need.pdf_chunk_5 (score: 20.0)\n",
      "  6. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_32 (score: 20.0)\n",
      "  7. Attention Is All You Need.pdf_chunk_10 (score: 90.0)\n",
      "  8. Attention Is All You Need.pdf_chunk_28 (score: 85.0)\n",
      "  9. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_22 (score: 20.0)\n",
      "  10. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_55 (score: 20.0)\n",
      "  11. BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_69 (score: 60.0)\n",
      "\n",
      "Final selection (top-4):\n",
      "  1. Attention Is All You Need.pdf_chunk_10 (score: 90.0)\n",
      "  2. Attention Is All You Need.pdf_chunk_28 (score: 85.0)\n",
      "  3. Attention Is All You Need.pdf_chunk_30 (score: 75.0)\n",
      "  4. Attention Is All You Need.pdf_chunk_35 (score: 60.0)\n",
      "============================================================\n",
      "\n",
      "LLM Scoring: 11 candidates -> 4 selected\n",
      "\n",
      "Expected chunks in final selection:\n",
      "Found: ['Attention Is All You Need.pdf_chunk_28', 'Attention Is All You Need.pdf_chunk_30'] | Missing: ['Attention Is All You Need.pdf_chunk_29', 'Attention Is All You Need.pdf_chunk_31']\n",
      "Total unique docs: 11\n",
      "Multi-agent docs (in 2+ workers): 4\n",
      "Top-4 selected for generation\n",
      "Average quality: 78%\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Question: Describe the complete training procedure for the Transformer, including optimization, regularization, and learning rate schedule.\n",
      "Context size: 3905 chars\n",
      "Retrieval quality: 78%\n",
      "Generation attempt: 1/3\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER EVALUATION (Refusal + Groundedness + Quality)\n",
      "Generation attempt: 1\n",
      "Retrieval quality: 78%\n",
      "Refusal detection: ATTEMPTED - The answer provides substantive information about the training procedure for the Transformer, including details on optimization (Adam optimizer and learning rate schedule) and regularization techniques (dropout and label smoothing). Although it acknowledges that the context does not include specific data used for training or the exact architecture, it still offers valuable insights into the training process.\n",
      "Groundedness: 80%\n",
      "Quality: 75% (insufficient)\n",
      "Issues: partial_answer, missing_details\n",
      "Combined decision: RETRY\n",
      "============================================================\n",
      "\n",
      "\n",
      "Routing: answer_generation (attempt 2/3)\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Question: Describe the complete training procedure for the Transformer, including optimization, regularization, and learning rate schedule.\n",
      "Context size: 3905 chars\n",
      "Retrieval quality: 78%\n",
      "Generation attempt: 2/3\n",
      "============================================================\n",
      "\n",
      "RETRY MODE:\n",
      "Feedback:\n",
      "QUALITY ISSUES:\n",
      "Problems: partial_answer, missing_details\n",
      "Fix: Ensure all question parts are answered completely; Add more depth and explanation where the context provides supporting information\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER EVALUATION (Refusal + Groundedness + Quality)\n",
      "Generation attempt: 2\n",
      "Retrieval quality: 78%\n",
      "Refusal detection: ATTEMPTED - The assistant provided a detailed description of the training procedure for the Transformer, including specifics about optimization (Adam optimizer and learning rate schedule), regularization techniques (dropout and label smoothing), and the training environment. Although it acknowledged that the context did not include certain details, it still provided substantial information about the training process.\n",
      "Groundedness: 92%\n",
      "Quality: 75% (insufficient)\n",
      "Issues: missing_details, partial_answer\n",
      "Combined decision: RETRY\n",
      "============================================================\n",
      "\n",
      "\n",
      "Routing: answer_generation (attempt 3/3)\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Question: Describe the complete training procedure for the Transformer, including optimization, regularization, and learning rate schedule.\n",
      "Context size: 3905 chars\n",
      "Retrieval quality: 78%\n",
      "Generation attempt: 3/3\n",
      "============================================================\n",
      "\n",
      "RETRY MODE:\n",
      "Feedback:\n",
      "QUALITY ISSUES:\n",
      "Problems: missing_details, partial_answer\n",
      "Fix: Add more depth and explanation where the context provides supporting information; Ensure all question parts are answered completely\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER EVALUATION (Refusal + Groundedness + Quality)\n",
      "Generation attempt: 3\n",
      "Retrieval quality: 78%\n",
      "Refusal detection: ATTEMPTED - The answer provides substantive information about the training procedure for the Transformer, including details on optimization (Adam optimizer and learning rate schedule), regularization techniques (dropout and label smoothing), and the training environment. Although it acknowledges that some specific details are not covered, it still offers a comprehensive overview of the training process.\n",
      "Groundedness: 100%\n",
      "Quality: 75% (insufficient)\n",
      "Issues: missing_details, partial_answer\n",
      "Combined decision: RETRY\n",
      "============================================================\n",
      "\n",
      "  F1@4: 50%\n",
      "\n",
      "================================================================================\n",
      "Comparison complete!\n"
     ]
    }
   ],
   "source": [
    "# Run 4-Tier Comparison\n",
    "# Using a query from golden dataset so we can calculate F1@4\n",
    "\n",
    "from advanced_agentic_rag_langgraph.evaluation.retrieval_metrics import calculate_retrieval_metrics\n",
    "from advanced_agentic_rag_langgraph.validation import HHEMHallucinationDetector\n",
    "\n",
    "# Import modules to inject shared retriever\n",
    "import advanced_agentic_rag_langgraph.variants.basic_rag_graph as basic_module\n",
    "import advanced_agentic_rag_langgraph.variants.intermediate_rag_graph as intermediate_module\n",
    "import advanced_agentic_rag_langgraph.orchestration.nodes as advanced_module\n",
    "import advanced_agentic_rag_langgraph.variants.multi_agent_rag_graph as multi_agent_module\n",
    "\n",
    "# Inject pre-built retriever into all variants\n",
    "basic_module.adaptive_retriever = retriever\n",
    "intermediate_module.adaptive_retriever = retriever\n",
    "advanced_module.adaptive_retriever = retriever\n",
    "multi_agent_module.adaptive_retriever = retriever\n",
    "\n",
    "# Query from golden_set_standard.json (transformer_training_procedure)\n",
    "test_query = \"Describe the complete training procedure for the Transformer, including optimization, regularization, and learning rate schedule.\"\n",
    "ground_truth_doc_ids = [\n",
    "      \"Attention Is All You Need.pdf_chunk_28\",\n",
    "      \"Attention Is All You Need.pdf_chunk_29\",\n",
    "      \"Attention Is All You Need.pdf_chunk_30\",\n",
    "      \"Attention Is All You Need.pdf_chunk_31\"\n",
    "]\n",
    "\n",
    "graphs = {\n",
    "    \"Basic\": basic_rag_graph,\n",
    "    \"Intermediate\": intermediate_rag_graph,\n",
    "    \"Advanced\": advanced_rag_graph,\n",
    "    \"Multi-Agent\": multi_agent_rag_graph,\n",
    "}\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Ground Truth Chunks: {len(ground_truth_doc_ids)} (from attention paper)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = {}\n",
    "for name, graph in graphs.items():\n",
    "    print(f\"\\nRunning {name} RAG...\")\n",
    "    \n",
    "    initial_state = {\n",
    "        \"user_question\": test_query,\n",
    "        \"baseline_query\": test_query,\n",
    "        \"messages\": [],\n",
    "        \"retrieved_docs\": [],\n",
    "        \"retrieval_attempts\": 0,\n",
    "        \"query_expansions\": [],\n",
    "        \"sub_agent_results\": [],\n",
    "        \"ground_truth_doc_ids\": ground_truth_doc_ids,\n",
    "    }\n",
    "    config = {\"configurable\": {\"thread_id\": f\"demo-{name.lower().replace('-', '_')}\"}}\n",
    "    \n",
    "    result = graph.invoke(initial_state, config=config)\n",
    "    results[name] = result\n",
    "    \n",
    "    # Calculate F1@4 for progress display\n",
    "    docs = result.get(\"unique_docs_list\", [])\n",
    "    if docs:\n",
    "        metrics = calculate_retrieval_metrics(docs, ground_truth_doc_ids, k_final)\n",
    "        f1 = metrics[\"f1_at_k\"]\n",
    "    else:\n",
    "        f1 = 0.0\n",
    "    print(f\"  F1@{k_final}: {f1:.0%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Comparison complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-8-results",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T08:01:59.492551Z",
     "iopub.status.busy": "2025-11-27T08:01:59.492551Z",
     "iopub.status.idle": "2025-11-27T08:01:59.502954Z",
     "shell.execute_reply": "2025-11-27T08:01:59.500944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "4-TIER ARCHITECTURE COMPARISON RESULTS\n",
      "================================================================================\n",
      "\n",
      "[BASIC]\n",
      "Strategy: semantic  |  Docs: 4  |  F1@4: 0%  |  Groundedness: 64%\n",
      "--------------------------------------------------------------------------------\n",
      "The complete training procedure for the Transformer involves several key components, including optimization, regularization, and learning rate scheduling. Heres a detailed description:\n",
      "\n",
      "1. **Optimizer**: The Adam optimizer is used for training the Transformer model. Adam is known for its efficiency and effectiveness in handling sparse gradients and is well-suited for a variety of training tasks.\n",
      "\n",
      "2. **Weight Regularization**: The model employs decoupled weight decay regularization. This technique is applied to all weights in the model that are not gains or biases, helping to prevent overfitting by penalizing large weights.\n",
      "\n",
      "3. **Learning Rate Schedule**: The learning rate is adjusted using a cosine schedule. This means that the learning rate starts at a certain value and gradually decreases following a cosine curve, allowing for a smoother transition as training progresses.\n",
      "\n",
      "4. **Training Duration**: The models are trained for a total of 32 epochs, which allows sufficient time for the optimizer to adjust the weights effectively.\n",
      "\n",
      "5. **Initial Hyperparameters**: The initial hyperparameters for training are set using a combination of grid searches, random search, and manual tuning based on the baseline ResNet-50 model when trained for 1 epoch. These hyperparameters are then adapted heuristically for larger models in subsequent training sessions.\n",
      "\n",
      "This comprehensive approach to training the Transformer model ensures that it effectively learns from the data while maintaining robustness against overfitting and optimizing performance through careful adjustment of the learning rate.\n",
      "================================================================================\n",
      "\n",
      "[INTERMEDIATE]\n",
      "Strategy: semantic  |  Docs: 4  |  F1@4: 0%  |  Groundedness: 18%\n",
      "--------------------------------------------------------------------------------\n",
      "The training procedure for the Transformer involves several key components, including optimization, regularization, and the learning rate schedule. Here is a detailed description:\n",
      "\n",
      "1. **Optimization**: The Transformer model is trained using an optimization algorithm, typically Adam, which adapts the learning rate for each parameter based on first and second moments of the gradients. This allows for efficient training and convergence.\n",
      "\n",
      "2. **Regularization**: To prevent overfitting and improve generalization, several regularization techniques are employed:\n",
      "   - **Weight Decay**: A regularization term is added to the loss function to penalize large weights, encouraging simpler models.\n",
      "   - **Dropout**: Randomly dropping units during training helps to prevent co-adaptation of neurons, enhancing the model's robustness.\n",
      "   - **Label Smoothing**: This technique softens the target labels, which helps the model to avoid being overly confident in its predictions, thereby improving generalization.\n",
      "\n",
      "3. **Learning Rate Schedule**: The learning rate is crucial for the training process and is often adjusted dynamically:\n",
      "   - A common approach is to use a warm-up strategy, where the learning rate starts small and gradually increases to a specified value over a certain number of training steps. After reaching the peak learning rate, it is then decayed, often following a linear or cosine schedule.\n",
      "   - This method helps stabilize the training process, especially in the early stages, allowing the model to learn more effectively.\n",
      "\n",
      "4. **Training Time**: The Transformer can achieve state-of-the-art results relatively quickly, with the ability to reach competitive translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "\n",
      "5. **Batch Size and Parallelization**: The architecture allows for significant parallelization, enabling the model to utilize large batch sizes effectively, which contributes to faster training times.\n",
      "\n",
      "Overall, the training procedure for the Transformer combines advanced optimization techniques, regularization methods, and a carefully managed learning rate schedule to achieve high performance in tasks such as machine translation.\n",
      "================================================================================\n",
      "\n",
      "[ADVANCED]\n",
      "Strategy: semantic  |  Docs: 4  |  F1@4: 25%  |  Groundedness: 85%\n",
      "--------------------------------------------------------------------------------\n",
      "The training procedure for the Transformer model includes the following components:\n",
      "\n",
      "1. **Optimization**: The Adam optimizer is used with the hyperparameters 1 = 0.9, 2 = 0.98, and  = 109. The learning rate is varied over the course of training according to the formula:\n",
      "   \\[\n",
      "   lrate = d^{-0.5} \\cdot \\min(step\\_num^{-0.5}, step\\_num \\cdot warmup\\_steps^{-1.5})\n",
      "   \\]\n",
      "   This indicates that the learning rate increases linearly for the first 4,000 steps (warmup_steps) and decreases thereafter proportionally to the inverse square root of the step number.\n",
      "\n",
      "2. **Regularization**: Three types of regularization are employed during training, although the specific types are not detailed in the retrieved context.\n",
      "\n",
      "3. **Training Duration**: The base models are trained for a total of 100,000 steps, which takes approximately 12 hours. Larger models are trained for 300,000 steps, taking about 3.5 days. The training is conducted on a machine with 8 NVIDIA P100 GPUs.\n",
      "\n",
      "The context provides detailed information about the optimization process and training duration. However, it does not specify the exact types of regularization used.\n",
      "================================================================================\n",
      "\n",
      "[MULTI-AGENT]\n",
      "Strategy: semantic  |  Docs: 4  |  F1@4: 50%  |  Groundedness: 100%\n",
      "--------------------------------------------------------------------------------\n",
      "The training procedure for the Transformer includes several key components: optimization, regularization, and a learning rate schedule.\n",
      "\n",
      "1. **Optimization**: The Adam optimizer is employed with parameters 1 = 0.9, 2 = 0.98, and  = 109. The learning rate is adjusted throughout the training process according to the formula:\n",
      "   \\[\n",
      "   lrate = d^{-0.5}_{model} \\cdot \\min(step\\_num^{-0.5}, step\\_num \\cdot warmup\\_steps^{-1.5})\n",
      "   \\]\n",
      "   This means that the learning rate increases linearly for the first `warmup_steps` (set to 4000), and then decreases proportionally to the inverse square root of the step number thereafter. This strategy helps stabilize the training process.\n",
      "\n",
      "2. **Regularization**: Three types of regularization techniques are utilized during training:\n",
      "   - **Dropout**: A dropout rate of Pdrop = 0.1 is applied to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks, which helps prevent overfitting.\n",
      "   - **Label Smoothing**: A label smoothing value of ls = 0.1 is applied. Although this may negatively impact perplexity, it improves accuracy and BLEU scores by encouraging the model to be less confident in its predictions.\n",
      "\n",
      "3. **Training Environment**: The models are trained on a machine with 8 NVIDIA P100 GPUs. The base models are trained for 100,000 steps (approximately 12 hours), while the larger models are trained for 300,000 steps (around 3.5 days).\n",
      "\n",
      "The context provides a comprehensive overview of the training procedure, including the optimizer and its parameters, the learning rate schedule, and the types of regularization used. However, it does not detail the specific data used for training or the exact architecture of the model beyond the general description of the encoder and decoder stacks.\n",
      "================================================================================\n",
      "\n",
      "Key Observations:\n",
      "- F1@4 measures retrieval quality against known ground truth chunks\n",
      "- Groundedness measures % of answer claims supported by retrieved context (via HHEM)\n",
      "- Multi-Agent shows best F1@4 due to query decomposition finding more relevant chunks\n"
     ]
    }
   ],
   "source": [
    "# Display Comparison Results\n",
    "# Calculate F1@4 and Groundedness independently (not from graph state)\n",
    "\n",
    "hhem_detector = HHEMHallucinationDetector()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"4-TIER ARCHITECTURE COMPARISON RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for name, result in results.items():\n",
    "    strategy = result.get(\"retrieval_strategy\", \"semantic\") or \"semantic\"\n",
    "    docs = result.get(\"unique_docs_list\", [])\n",
    "    answer = result.get(\"final_answer\", \"\")\n",
    "\n",
    "    # Calculate F1@4 using ground truth\n",
    "    if docs:\n",
    "        metrics = calculate_retrieval_metrics(docs, ground_truth_doc_ids, k_final)\n",
    "        f1_at_k = metrics[\"f1_at_k\"]\n",
    "    else:\n",
    "        f1_at_k = 0.0\n",
    "\n",
    "    # Calculate groundedness using HHEM\n",
    "    if docs and answer:\n",
    "        chunks = [doc.page_content for doc in docs[:k_final]]\n",
    "        groundedness_result = hhem_detector.verify_groundedness(answer, chunks)\n",
    "        groundedness = groundedness_result[\"groundedness_score\"]\n",
    "    else:\n",
    "        groundedness = 0.0\n",
    "\n",
    "    print(f\"\\n[{name.upper()}]\")\n",
    "    print(f\"Strategy: {strategy}  |  Docs: {len(docs)}  |  F1@4: {f1_at_k:.0%}  |  Groundedness: {groundedness:.0%}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(answer if answer else \"No answer\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- F1@4 measures retrieval quality against known ground truth chunks\")\n",
    "print(\"- Groundedness measures % of answer claims supported by retrieved context (via HHEM)\")\n",
    "print(\"- Multi-Agent shows best F1@4 due to query decomposition finding more relevant chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9-selfcorrect",
   "metadata": {},
   "source": [
    "## Deep Dive: Self-Correction Loops\n",
    "\n",
    "The Advanced RAG tier implements two self-correction mechanisms:\n",
    "\n",
    "### 1. Retrieval Correction (max 2 attempts)\n",
    "When `retrieval_quality_score < 0.6`:\n",
    "- **Path A (off_topic/wrong_domain)**: Switch strategy immediately (semantic  keyword)\n",
    "- **Path B (other issues)**: Inject suggested keywords into query for better term coverage\n",
    "\n",
    "### 2. Generation Retry (max 3 attempts)\n",
    "When answer fails quality evaluation:\n",
    "- Regenerate with combined feedback (quality issues + hallucination warnings)\n",
    "- Low temperature: 0.3\n",
    "\n",
    "### Why Single Correction Cycle?\n",
    "Research (CRAG, Self-RAG) shows diminishing returns after the first correction. The architecture accepts imperfect retrieval rather than looping indefinitely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10-trace",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T08:01:59.507949Z",
     "iopub.status.busy": "2025-11-27T08:01:59.507949Z",
     "iopub.status.idle": "2025-11-27T08:03:41.213280Z",
     "shell.execute_reply": "2025-11-27T08:03:41.211764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How is attention mechanism used differently in NLP vs vision models?\n",
      "================================================================================\n",
      "\n",
      "Running Advanced RAG with potential self-correction...\n",
      "\n",
      "\n",
      "============================================================\n",
      "STRATEGY SELECTION\n",
      "Query: How is attention mechanism used differently in NLP vs vision models?\n",
      "Selected: HYBRID\n",
      "Confidence: 90%\n",
      "Reasoning: The user is seeking a comparison between the use of attention mechanisms in NLP and vision models, which indicates a need for both conceptual understanding (how attention mechanisms function in each domain) and specific terms (NLP and vision models). This makes a hybrid search the best choice, as it allows for retrieving documents that explain the differences while also ensuring that the exact terms 'NLP' and 'vision models' are matched accurately. The corpus is technical and focused on computer vision, which supports the need for precise retrieval of relevant technical content.\n",
      "Note: Query optimization will happen in query_expansion_node\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "STRATEGY-SPECIFIC QUERY OPTIMIZATION\n",
      "Strategy: hybrid\n",
      "Original query: How is attention mechanism used differently in NLP vs vision models?\n",
      "Optimized query: attention mechanism in NLP and vision models: differences in application and impact\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EXPANSION DECISION\n",
      "Query: attention mechanism in NLP and vision models: differences in application and impact\n",
      "LLM decision: EXPAND query\n",
      "Reasoning: The query addresses a complex topic that involves multiple domains (NLP and vision models) and could benefit from variations in terminology and phrasing. Users may refer to the 'attention mechanism' using different terms or may have different focuses within the applications and impacts, making expansion useful for capturing a broader range of relevant information.\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "QUERY EXPANDED\n",
      "Optimized query: attention mechanism in NLP and vision models: differences in application and impact\n",
      "Expansions: ['What are the technical implementations and mechanisms of attention mechanisms in NLP and vision models?', 'What are some practical applications and use cases that highlight the differences and impacts of attention mechanisms in NLP versus vision models?', 'What are the fundamental concepts and principles that differentiate the application and impact of attention mechanisms in NLP and vision models?']\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "RETRIEVAL EXECUTION START\n",
      "Using 4 query expansion(s)\n",
      "Expansions generated from: retrieval_query\n",
      "Retrieval strategy: hybrid\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "RRF MULTI-QUERY RETRIEVAL\n",
      "Query variants: 4\n",
      "Total retrievals: 54\n",
      "Unique docs after RRF: 27\n",
      "\n",
      "All 27 chunk IDs (RRF scores):\n",
      "  1. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_169 (0.0656)\n",
      "  2. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_35 (0.0609)\n",
      "  3. Attention Is All You Need.pdf_chunk_5 (0.0558)\n",
      "  4. Attention Is All You Need.pdf_chunk_49 (0.0472)\n",
      "  5. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_79 (0.0471)\n",
      "  6. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_6 (0.0464)\n",
      "  7. Attention Is All You Need.pdf_chunk_43 (0.0460)\n",
      "  8. Attention Is All You Need.pdf_chunk_17 (0.0458)\n",
      "  9. Attention Is All You Need.pdf_chunk_14 (0.0415)\n",
      "  10. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_160 (0.0413)\n",
      "  11. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_158 (0.0311)\n",
      "  12. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_14 (0.0301)\n",
      "  13. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_38 (0.0299)\n",
      "  14. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_51 (0.0297)\n",
      "  15. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_149 (0.0156)\n",
      "  16. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_170 (0.0149)\n",
      "  17. Attention Is All You Need.pdf_chunk_44 (0.0149)\n",
      "  18. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_48 (0.0147)\n",
      "  19. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_159 (0.0147)\n",
      "  20. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_176 (0.0145)\n",
      "  21. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_224 (0.0143)\n",
      "  22. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_1 (0.0143)\n",
      "  23. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_164 (0.0143)\n",
      "  24. Attention Is All You Need.pdf_chunk_7 (0.0139)\n",
      "  25. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_42 (0.0139)\n",
      "  26. Attention Is All You Need.pdf_chunk_50 (0.0137)\n",
      "  27. Attention Is All You Need.pdf_chunk_39 (0.0137)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "TWO-STAGE RERANKING (After RRF)\n",
      "Input: 27 docs (from RRF top-40)\n",
      "\n",
      "Chunk IDs sent to reranking (top-40):\n",
      "  1. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_169\n",
      "  2. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_35\n",
      "  3. Attention Is All You Need.pdf_chunk_5\n",
      "  4. Attention Is All You Need.pdf_chunk_49\n",
      "  5. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_79\n",
      "  6. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_6\n",
      "  7. Attention Is All You Need.pdf_chunk_43\n",
      "  8. Attention Is All You Need.pdf_chunk_17\n",
      "  9. Attention Is All You Need.pdf_chunk_14\n",
      "  10. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_160\n",
      "  ... and 17 more\n",
      "\n",
      "============================================================\n",
      "RERANKING QUERY SOURCE\n",
      "Using: active_query (semantic, human-readable)\n",
      "Query: How is attention mechanism used differently in NLP vs vision models?\n",
      "Note: Reranking uses semantic query, NOT algorithm-optimized retrieval_query\n",
      "============================================================\n",
      "\n",
      "\n",
      "Output: 4 docs after two-stage reranking\n",
      "\n",
      "Final chunk IDs (after two-stage reranking):\n",
      "  1. Attention Is All You Need.pdf_chunk_17 (score: 85.0000)\n",
      "  2. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_38 (score: 80.0000)\n",
      "  3. Attention Is All You Need.pdf_chunk_5 (score: 75.0000)\n",
      "  4. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_1 (score: 75.0000)\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ROUTER: AFTER RETRIEVAL\n",
      "Quality: 55% (threshold: >=60%)\n",
      "Attempts: 1/2\n",
      "Issues: missing_key_info, partial_coverage\n",
      "Decision: rewrite_and_refine (semantic rewrite)\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "KEYWORD INJECTION\n",
      "Original query: How is attention mechanism used differently in NLP vs vision models?\n",
      "Retrieval quality: 55%\n",
      "Keywords to inject: ['NLP vs vision attention differences', 'attention mechanism applications', 'Transformer architecture in vision', 'self-attention in images', 'attention in language models']\n",
      "Issues detected: missing_key_info, partial_coverage\n",
      "============================================================\n",
      "\n",
      "Refined query: How do the attention mechanism applications differ between NLP vs vision models, particularly in the context of self-attention in images and the Transformer architecture in vision versus its role in language models?\n",
      "Note: Query expansions cleared - will regenerate for refined query\n",
      "\n",
      "State clearing (keyword injection):\n",
      "  query_expansions: [] (will regenerate)\n",
      "  retrieval_query: None (cleared to prevent stale optimization)\n",
      "  active_query: How do the attention mechanism applications differ between NLP vs vision models, particularly in the context of self-attention in images and the Transformer architecture in vision versus its role in language models? (with injected keywords)\n",
      "\n",
      "\n",
      "============================================================\n",
      "STRATEGY-SPECIFIC QUERY OPTIMIZATION\n",
      "Strategy: hybrid\n",
      "Original query: How do the attention mechanism applications differ between NLP vs vision models, particularly in the context of self-attention in images and the Transformer architecture in vision versus its role in language models?\n",
      "Optimized query: Comparative analysis of attention mechanism applications: differences in self-attention roles in NLP vs vision models, focusing on the Transformer architecture's impact on language models versus its application in image processing.\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "EXPANSION DECISION\n",
      "Query: Comparative analysis of attention mechanism applications: differences in self-attention roles in NLP vs vision models, focusing on the Transformer architecture's impact on language models versus its application in image processing.\n",
      "LLM decision: EXPAND query\n",
      "Reasoning: The query is complex and covers multiple domains (NLP and vision models) with specific terminology (self-attention, Transformer architecture). Expanding it into variations can help capture different phrasing and terminology that users might employ, enhancing retrieval effectiveness.\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "QUERY EXPANDED\n",
      "Optimized query: Comparative analysis of attention mechanism applications: differences in self-attention roles in NLP vs vision models, focusing on the Transformer architecture's impact on language models versus its application in image processing.\n",
      "Expansions: ['What are the technical implementations and mechanisms of self-attention in Transformer architectures for NLP compared to vision models?', 'In what practical applications do self-attention mechanisms in Transformer architectures differ between language models and image processing tasks?', 'What are the fundamental concepts and principles underlying the roles of self-attention in NLP versus vision models within the Transformer framework?']\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "RETRIEVAL EXECUTION START\n",
      "Using 4 query expansion(s)\n",
      "Expansions generated from: retrieval_query\n",
      "Retrieval strategy: hybrid\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "RRF MULTI-QUERY RETRIEVAL\n",
      "Query variants: 4\n",
      "Total retrievals: 49\n",
      "Unique docs after RRF: 20\n",
      "\n",
      "All 20 chunk IDs (RRF scores):\n",
      "  1. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_38 (0.0643)\n",
      "  2. Attention Is All You Need.pdf_chunk_17 (0.0643)\n",
      "  3. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_5 (0.0636)\n",
      "  4. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_1 (0.0628)\n",
      "  5. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_41 (0.0616)\n",
      "  6. Attention Is All You Need.pdf_chunk_8 (0.0606)\n",
      "  7. Attention Is All You Need.pdf_chunk_7 (0.0452)\n",
      "  8. Attention Is All You Need.pdf_chunk_2 (0.0441)\n",
      "  9. Attention Is All You Need.pdf_chunk_1 (0.0437)\n",
      "  10. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_35 (0.0294)\n",
      "  11. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_6 (0.0292)\n",
      "  12. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_83 (0.0288)\n",
      "  13. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_82 (0.0282)\n",
      "  14. Attention Is All You Need.pdf_chunk_5 (0.0276)\n",
      "  15. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_7 (0.0143)\n",
      "  16. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_11 (0.0141)\n",
      "  17. Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_20 (0.0141)\n",
      "  18. Learning Transferable Visual Models From Natural Language Supervision.pdf_chunk_170 (0.0139)\n",
      "  19. Expanding Horizons in RAG - Exploring and Extending the Limits of RAPTOR.pdf_chunk_5 (0.0139)\n",
      "  20. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_0 (0.0139)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "TWO-STAGE RERANKING (After RRF)\n",
      "Input: 20 docs (from RRF top-40)\n",
      "\n",
      "Chunk IDs sent to reranking (top-40):\n",
      "  1. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_38\n",
      "  2. Attention Is All You Need.pdf_chunk_17\n",
      "  3. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_5\n",
      "  4. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_1\n",
      "  5. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_41\n",
      "  6. Attention Is All You Need.pdf_chunk_8\n",
      "  7. Attention Is All You Need.pdf_chunk_7\n",
      "  8. Attention Is All You Need.pdf_chunk_2\n",
      "  9. Attention Is All You Need.pdf_chunk_1\n",
      "  10. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_35\n",
      "  ... and 10 more\n",
      "\n",
      "============================================================\n",
      "RERANKING QUERY SOURCE\n",
      "Using: active_query (semantic, human-readable)\n",
      "Query: How do the attention mechanism applications differ between NLP vs vision models, particularly in the context of self-attention in images and the Transformer architecture in vision versus its role in language models?\n",
      "Note: Reranking uses semantic query, NOT algorithm-optimized retrieval_query\n",
      "============================================================\n",
      "\n",
      "\n",
      "Output: 4 docs after two-stage reranking\n",
      "\n",
      "Final chunk IDs (after two-stage reranking):\n",
      "  1. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_41 (score: 90.0000)\n",
      "  2. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_1 (score: 85.0000)\n",
      "  3. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_38 (score: 80.0000)\n",
      "  4. AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_5 (score: 75.0000)\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ROUTER: AFTER RETRIEVAL\n",
      "Quality: 70% (threshold: >=60%)\n",
      "Attempts: 2/2\n",
      "Issues: partial_coverage, missing_key_info\n",
      "Decision: answer_generation (quality acceptable)\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Question: How is attention mechanism used differently in NLP vs vision models?\n",
      "Context size: 4083 chars\n",
      "Retrieval quality: 70%\n",
      "Generation attempt: 1/3\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER EVALUATION (Refusal + Groundedness + Quality)\n",
      "Generation attempt: 1\n",
      "Retrieval quality: 70%\n",
      "Refusal detection: ATTEMPTED - The answer provides substantive information about how the attention mechanism is used in NLP and vision models, including specific details about Transformers, the treatment of input data, and differences in attention characteristics. Although it acknowledges that the context does not provide detailed comparisons or mathematical formulations, it still offers valuable insights into the application of attention in both domains.\n",
      "Groundedness: 85%\n",
      "Quality: 65% (insufficient)\n",
      "Issues: partial_answer, missing_details\n",
      "Combined decision: RETRY\n",
      "============================================================\n",
      "\n",
      "\n",
      "Routing: answer_generation (attempt 2/3)\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Question: How is attention mechanism used differently in NLP vs vision models?\n",
      "Context size: 4083 chars\n",
      "Retrieval quality: 70%\n",
      "Generation attempt: 2/3\n",
      "============================================================\n",
      "\n",
      "RETRY MODE:\n",
      "Feedback:\n",
      "QUALITY ISSUES:\n",
      "Problems: partial_answer, missing_details\n",
      "Fix: Ensure all question parts are answered completely; Add more depth and explanation where the context provides supporting information\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER EVALUATION (Refusal + Groundedness + Quality)\n",
      "Generation attempt: 2\n",
      "Retrieval quality: 70%\n",
      "Refusal detection: ATTEMPTED - The answer provides substantive information about how the attention mechanism is used in both NLP and vision models, detailing the differences in their applications and characteristics. It explains the use of self-attention in NLP and the treatment of images as sequences of patches in vision models, along with the implications of these approaches. Although it acknowledges that the context does not provide specific mathematical formulations or detailed comparisons, it still offers valuable insights into the topic.\n",
      "Groundedness: 88%\n",
      "Quality: 65% (insufficient)\n",
      "Issues: partial_answer, missing_details\n",
      "Combined decision: RETRY\n",
      "============================================================\n",
      "\n",
      "\n",
      "Routing: answer_generation (attempt 3/3)\n",
      "\n",
      "============================================================\n",
      "ANSWER GENERATION\n",
      "Question: How is attention mechanism used differently in NLP vs vision models?\n",
      "Context size: 4083 chars\n",
      "Retrieval quality: 70%\n",
      "Generation attempt: 3/3\n",
      "============================================================\n",
      "\n",
      "RETRY MODE:\n",
      "Feedback:\n",
      "QUALITY ISSUES:\n",
      "Problems: partial_answer, missing_details\n",
      "Fix: Ensure all question parts are answered completely; Add more depth and explanation where the context provides supporting information\n",
      "\n",
      "\n",
      "============================================================\n",
      "ANSWER EVALUATION (Refusal + Groundedness + Quality)\n",
      "Generation attempt: 3\n",
      "Retrieval quality: 70%\n",
      "Refusal detection: ATTEMPTED - The answer provides substantive information about how the attention mechanism is used in both NLP and vision models, detailing the differences in their applications and characteristics. It explains the use of self-attention in NLP and the treatment of images as sequences of patches in vision models, along with their respective advantages. Although it acknowledges that the context does not provide specific mathematical formulations or detailed comparisons, it still offers valuable insights into the topic.\n",
      "Groundedness: 75%\n",
      "Quality: 65% (insufficient)\n",
      "Issues: partial_answer, missing_details\n",
      "Combined decision: RETRY\n",
      "============================================================\n",
      "\n",
      "\n",
      "Routing: END (max attempts reached)\n",
      "\n",
      "\n",
      "================================================================================\n",
      "EXECUTION TRACE\n",
      "----------------------------------------\n",
      "Retrieval Attempts: 2\n",
      "Strategy Used: hybrid\n",
      "Final Retrieval Quality: 70%\n",
      "Issues Detected: partial_coverage, missing_key_info\n",
      "\n",
      "Generation Attempts: 3\n",
      "Groundedness: 75%\n",
      "Answer Sufficient: False\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "----------------------------------------\n",
      "The attention mechanism is utilized differently in NLP and vision models primarily due to the nature of the input data and the architecture of the models.\n",
      "\n",
      "In NLP, Transformers are pre-trained on large text corpora and fine-tuned on smaller, task-specific datasets. This approach leverages self-attention to capture relationships between words in a sequence, allowing the model to understand context and semantics effectively. The attention mechanism helps the model weigh the importance of different words in relation to each other, facilitating a nuanced understanding of language (as noted in the document \"AN IMAGE IS WORTH 16X16 WORDS\").\n",
      "\n",
      "In contrast, vision models like the Vision Transformer (ViT) treat images as sequences of patches instead of individual pixels. This method avoids the computational inefficiency that would result from having each pixel attend to every other pixel, which would lead to quadratic costs. Instead, the ViT processes these patches through a standard Transformer encoder, which has shown to perform well on image classification tasks without relying on convolutional neural networks (CNNs). The ViT achieves state-of-the-art results on various image classification datasets while being computationally efficient to train.\n",
      "\n",
      "Moreover, the attention mechanism in vision models exhibits different characteristics. Some attention heads in the ViT attend to most of the image in the lower layers, indicating a global integration of information, while others maintain localized attention. This behavior contrasts with NLP models, where attention is typically more uniform across the sequence. The attention distance in vision models increases with network depth, allowing the model to focus on semantically relevant image regions for classification (as detailed in the document).\n",
      "\n",
      "However, the context does not provide specific mathematical formulations or detailed comparisons of the attention mechanisms used in both domains. It primarily discusses the application of self-attention in a general manner without delving into the intricate differences in how attention is computed in NLP versus vision tasks.\n"
     ]
    }
   ],
   "source": [
    "# Self-Correction Example\n",
    "# This query might trigger self-correction due to cross-domain scope\n",
    "\n",
    "correction_query = \"How is attention mechanism used differently in NLP vs vision models?\"\n",
    "\n",
    "print(f\"Query: {correction_query}\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nRunning Advanced RAG with potential self-correction...\\n\")\n",
    "\n",
    "initial_state = {\n",
    "    \"user_question\": correction_query,\n",
    "    \"baseline_query\": correction_query,\n",
    "    \"messages\": [],\n",
    "    \"retrieved_docs\": [],\n",
    "    \"retrieval_attempts\": 0,\n",
    "    \"query_expansions\": [],\n",
    "}\n",
    "config = {\"configurable\": {\"thread_id\": \"demo-selfcorrect\"}}\n",
    "\n",
    "result = advanced_rag_graph.invoke(initial_state, config=config)\n",
    "\n",
    "# Display self-correction trace\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"EXECUTION TRACE\")\n",
    "print(\"-\"*40)\n",
    "print(f\"Retrieval Attempts: {result.get('retrieval_attempts', 1)}\")\n",
    "print(f\"Strategy Used: {result.get('retrieval_strategy', 'semantic')}\")\n",
    "\n",
    "if result.get('strategy_changed'):\n",
    "    print(f\"Strategy Changed: Yes\")\n",
    "    print(f\"  Reason: {result.get('strategy_switch_reason', 'N/A')}\")\n",
    "\n",
    "quality = result.get('retrieval_quality_score', 0) or 0\n",
    "print(f\"Final Retrieval Quality: {quality:.0%}\")\n",
    "\n",
    "if result.get('retrieval_quality_issues'):\n",
    "    print(f\"Issues Detected: {', '.join(result['retrieval_quality_issues'])}\")\n",
    "\n",
    "if result.get('retrieval_improvement_suggestion'):\n",
    "    print(f\"Improvement Suggestion: {result['retrieval_improvement_suggestion']}\")\n",
    "\n",
    "print(f\"\\nGeneration Attempts: {result.get('generation_attempts', 1)}\")\n",
    "print(f\"Groundedness: {(result.get('groundedness_score', 0) or 0):.0%}\")\n",
    "print(f\"Answer Sufficient: {result.get('is_answer_sufficient', True)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANSWER:\")\n",
    "print(\"-\"*40)\n",
    "print(result.get('final_answer', 'No answer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11-metrics",
   "metadata": {},
   "source": [
    "## Evaluation Metrics Summary\n",
    "\n",
    "All tiers use **budget models** (GPT-4o-mini only) to isolate architectural improvements from model quality.\n",
    "\n",
    "### Standard Dataset (20 questions, k=4)\n",
    "\n",
    "| Tier | Precision@4 | Recall@4 | F1@4 | MRR | nDCG@4 |\n",
    "|------|-------------|----------|------|-----|--------|\n",
    "| Basic | 10.0% | 23.8% | 13.1% | 0.204 | 0.191 |\n",
    "| Intermediate | 17.5% | 40.0% | 23.0% | 0.425 | 0.384 |\n",
    "| Advanced | 20.0% | 43.3% | 25.9% | 0.550 | 0.443 |\n",
    "| **Multi-Agent** | **23.8%** | **47.1%** | **29.6%** | **0.558** | **0.464** |\n",
    "\n",
    "### Hard Dataset (10 questions, k=6, multi-document)\n",
    "\n",
    "| Tier | Precision@6 | Recall@6 | F1@6 | MRR | nDCG@6 |\n",
    "|------|-------------|----------|------|-----|--------|\n",
    "| Basic | 25.0% | 35.6% | 29.0% | 0.553 | 0.365 |\n",
    "| Intermediate | 23.3% | 33.1% | 27.0% | 0.533 | 0.358 |\n",
    "| Advanced | 28.3% | 38.4% | 32.1% | 0.600 | 0.422 |\n",
    "| **Multi-Agent** | **31.7%** | **42.3%** | **35.7%** | **0.667** | **0.464** |\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- **2.3x retrieval accuracy** (F1@4: 13.1% -> 29.6%) with budget models only\n",
    "- Multi-Agent shows +30% improvement over Basic on complex queries\n",
    "- Query decomposition helps find relevant documents across multiple aspects\n",
    "- Architecture provides value independent of model quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13-conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Architecture > Model Size** - The graph structure provides value independent of model quality. Budget tier demonstrates the RAG intelligence; higher tiers add polish.\n",
    "\n",
    "2. **Distributed Decision-Making** - No central orchestrator. The StateGraph itself is the agent, with routing functions encoding planning logic.\n",
    "\n",
    "3. **Quality-Driven Flow** - Every routing point evaluates results and decides next action. Poor retrieval triggers correction; poor generation triggers retry.\n",
    "\n",
    "4. **Multi-Agent for Complexity** - Query decomposition with parallel workers significantly improves retrieval on complex, multi-faceted questions.\n",
    "\n",
    "### Source Code\n",
    "\n",
    "```\n",
    "src/advanced_agentic_rag_langgraph/\n",
    "    core/              # State, model tiers, retriever setup\n",
    "    evaluation/        # Metrics framework (F1, MRR, nDCG)\n",
    "    orchestration/     # Main graph, nodes, routing\n",
    "    retrieval/         # Strategy selection, reranking\n",
    "    validation/        # HHEM hallucination detection\n",
    "    variants/          # Basic, Intermediate, Advanced, Multi-Agent\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced-agentic-rag-langgraph (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
