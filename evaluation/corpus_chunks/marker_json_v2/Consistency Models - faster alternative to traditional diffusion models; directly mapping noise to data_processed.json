{
  "source": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf",
  "processor": "marker",
  "processed_date": "2025-12-14T07:45:38.999504",
  "markdown": "## Consistency Models\n\n#### Yang Song <sup>1</sup> Prafulla Dhariwal <sup>1</sup> Mark Chen <sup>1</sup> Ilya Sutskever <sup>1</sup>\n\n## Abstract\n\nDiffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose *consistency models*, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-ofthe-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64 ˆ 64 for one-step generation. When trained in isolation, consistency models become a new family of generative models that can outperform existing one-step, non-adversarial generative models on standard benchmarks such as CIFAR-10, ImageNet 64 ˆ 64 and LSUN 256 ˆ 256.\n\n## <span id=\"page-0-1\"></span>1. Introduction\n\nDiffusion models [\\(Sohl-Dickstein et al.,](#page-11-0) [2015;](#page-11-0) [Song & Er](#page-11-1)[mon,](#page-11-1) [2019;](#page-11-1) [2020;](#page-11-2) [Ho et al.,](#page-9-0) [2020;](#page-9-0) [Song et al.,](#page-11-3) [2021\\)](#page-11-3), also known as score-based generative models, have achieved unprecedented success across multiple fields, including image generation [\\(Dhariwal & Nichol,](#page-9-1) [2021;](#page-9-1) [Nichol et al.,](#page-10-0) [2021;](#page-10-0) [Ramesh et al.,](#page-10-1) [2022;](#page-10-1) [Saharia et al.,](#page-11-4) [2022;](#page-11-4) [Rombach](#page-11-5) [et al.,](#page-11-5) [2022\\)](#page-11-5), audio synthesis [\\(Kong et al.,](#page-10-2) [2020;](#page-10-2) [Chen et al.,](#page-9-2) [2021;](#page-9-2) [Popov et al.,](#page-10-3) [2021\\)](#page-10-3), and video generation [\\(Ho et al.,](#page-9-3)\n\n*Proceedings of the* 40 th *International Conference on Machine Learning*, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).\n\n<span id=\"page-0-0\"></span>![](_page_0_Figure_9.jpeg)\n\nFigure 1: Given a Probability Flow (PF) ODE that smoothly converts data to noise, we learn to map any point (*e.g*., xt, xt <sup>1</sup> , and x<sup>T</sup> ) on the ODE trajectory to its origin (*e.g*., x0) for generative modeling. Models of these mappings are called consistency models, as their outputs are trained to be consistent for points on the same trajectory.\n\n[2022b](#page-9-3)[;a\\)](#page-9-4). A key feature of diffusion models is the iterative sampling process which progressively removes noise from random initial vectors. This iterative process provides a flexible trade-off of compute and sample quality, as using extra compute for more iterations usually yields samples of better quality. It is also the crux of many zero-shot data editing capabilities of diffusion models, enabling them to solve challenging inverse problems ranging from image inpainting, colorization, stroke-guided image editing, to Computed Tomography and Magnetic Resonance Imaging [\\(Song & Ermon,](#page-11-1) [2019;](#page-11-1) [Song et al.,](#page-11-3) [2021;](#page-11-3) [2022;](#page-11-6) [2023;](#page-11-7) [Kawar](#page-10-4) [et al.,](#page-10-4) [2021;](#page-10-4) [2022;](#page-10-5) [Chung et al.,](#page-9-5) [2023;](#page-9-5) [Meng et al.,](#page-10-6) [2021\\)](#page-10-6). However, compared to single-step generative models like GANs [\\(Goodfellow et al.,](#page-9-6) [2014\\)](#page-9-6), VAEs [\\(Kingma & Welling,](#page-10-7) [2014;](#page-10-7) [Rezende et al.,](#page-10-8) [2014\\)](#page-10-8), or normalizing flows [\\(Dinh](#page-9-7) [et al.,](#page-9-7) [2015;](#page-9-7) [2017;](#page-9-8) [Kingma & Dhariwal,](#page-10-9) [2018\\)](#page-10-9), the iterative generation procedure of diffusion models typically requires 10–2000 times more compute for sample generation [\\(Song](#page-11-2) [& Ermon,](#page-11-2) [2020;](#page-11-2) [Ho et al.,](#page-9-0) [2020;](#page-9-0) [Song et al.,](#page-11-3) [2021;](#page-11-3) [Zhang](#page-11-8) [& Chen,](#page-11-8) [2022;](#page-11-8) [Lu et al.,](#page-10-10) [2022\\)](#page-10-10), causing slow inference and limited real-time applications.\n\nOur objective is to create generative models that facilitate efficient, single-step generation without sacrificing important advantages of iterative sampling, such as trading compute for sample quality when necessary, as well as performing zero-shot data editing tasks. As illustrated in Fig. [1,](#page-0-0) we build on top of the probability flow (PF) ordinary differential equation (ODE) in continuous-time diffusion models [\\(Song et al.,](#page-11-3) [2021\\)](#page-11-3), whose trajectories smoothly transition\n\n<sup>1</sup>OpenAI, San Francisco, CA 94110, USA. Correspondence to: Yang Song <songyang@openai.com>.\n\nthe data distribution into a tractable noise distribution. We propose to learn a model that maps any point at any time step to the trajectory's starting point. A notable property of our model is self-consistency: points on the same trajectory map to the same initial point. We therefore refer to such models as **consistency models**. Consistency models allow us to generate data samples (initial points of ODE trajectories, e.g.,  $\\mathbf{x}_0$  in Fig. 1) by converting random noise vectors (endpoints of ODE trajectories, e.g.,  $\\mathbf{x}_T$  in Fig. 1) with only one network evaluation. Importantly, by chaining the outputs of consistency models at multiple time steps, we can improve sample quality and perform zero-shot data editing at the cost of more compute, similar to what iterative sampling enables for diffusion models.\n\nTo train a consistency model, we offer two methods based on enforcing the self-consistency property. The first method relies on using numerical ODE solvers and a pre-trained diffusion model to generate pairs of adjacent points on a PF ODE trajectory. By minimizing the difference between model outputs for these pairs, we can effectively distill a diffusion model into a consistency model, which allows generating high-quality samples with one network evaluation. By contrast, our second method eliminates the need for a pre-trained diffusion model altogether, allowing us to train a consistency model in isolation. This approach situates consistency models as an independent family of generative models. Importantly, neither approach necessitates adversarial training, and they both place minor constraints on the architecture, allowing the use of flexible neural networks for parameterizing consistency models.\n\nWe demonstrate the efficacy of consistency models on several image datasets, including CIFAR-10 (Krizhevsky et al., 2009), ImageNet  $64 \\times 64$  (Deng et al., 2009), and LSUN  $256 \\times 256$  (Yu et al., 2015). Empirically, we observe that as a distillation approach, consistency models outperform existing diffusion distillation methods like progressive distillation (Salimans & Ho, 2022) across a variety of datasets in few-step generation: On CIFAR-10, consistency models reach new state-of-the-art FIDs of 3.55 and 2.93 for one-step and two-step generation; on ImageNet  $64 \\times 64$ , it achieves record-breaking FIDs of 6.20 and 4.70 with one and two network evaluations respectively. When trained as standalone generative models, consistency models can match or surpass the quality of one-step samples from progressive distillation, despite having no access to pre-trained diffusion models. They are also able to outperform many GANs, and existing non-adversarial, single-step generative models across multiple datasets. Furthermore, we show that consistency models can be used to perform a wide range of zero-shot data editing tasks, including image denoising, interpolation, inpainting, colorization, super-resolution, and stroke-guided image editing (SDEdit, Meng et al. (2021)).\n\n#### <span id=\"page-1-3\"></span>2. Diffusion Models\n\nConsistency models are heavily inspired by the theory of continuous-time diffusion models (Song et al., 2021; Karras et al., 2022). Diffusion models generate data by progressively perturbing data to noise via Gaussian perturbations, then creating samples from noise via sequential denoising steps. Let  $p_{\\rm data}(\\mathbf{x})$  denote the data distribution. Diffusion models start by diffusing  $p_{\\rm data}(\\mathbf{x})$  with a stochastic differential equation (SDE) (Song et al., 2021)\n\n<span id=\"page-1-0\"></span>\n$$d\\mathbf{x}_t = \\boldsymbol{\\mu}(\\mathbf{x}_t, t) dt + \\sigma(t) d\\mathbf{w}_t, \\tag{1}$$\n\nwhere  $t \\in [0,T]$ , T>0 is a fixed constant,  $\\mu(\\cdot,\\cdot)$  and  $\\sigma(\\cdot)$  are the drift and diffusion coefficients respectively, and  $\\{\\mathbf{w}_t\\}_{t\\in[0,T]}$  denotes the standard Brownian motion. We denote the distribution of  $\\mathbf{x}_t$  as  $p_t(\\mathbf{x})$  and as a result  $p_0(\\mathbf{x}) \\equiv p_{\\text{data}}(\\mathbf{x})$ . A remarkable property of this SDE is the existence of an ordinary differential equation (ODE), dubbed the *Probability Flow (PF) ODE* by Song et al. (2021), whose solution trajectories sampled at t are distributed according to  $p_t(\\mathbf{x})$ :\n\n<span id=\"page-1-1\"></span>\n$$d\\mathbf{x}_{t} = \\left[ \\boldsymbol{\\mu}(\\mathbf{x}_{t}, t) - \\frac{1}{2} \\sigma(t)^{2} \\nabla \\log p_{t}(\\mathbf{x}_{t}) \\right] dt.$$\n (2)\n\nHere  $\\nabla \\log p_t(\\mathbf{x})$  is the *score function* of  $p_t(\\mathbf{x})$ ; hence diffusion models are also known as *score-based generative models* (Song & Ermon, 2019; 2020; Song et al., 2021).\n\nTypically, the SDE in Eq. (1) is designed such that  $p_T(\\mathbf{x})$  is close to a tractable Gaussian distribution  $\\pi(\\mathbf{x})$ . We hereafter adopt the settings in Karras et al. (2022), where  $\\mu(\\mathbf{x},t) = \\mathbf{0}$  and  $\\sigma(t) = \\sqrt{2t}$ . In this case, we have  $p_t(\\mathbf{x}) = p_{\\text{data}}(\\mathbf{x}) \\otimes \\mathcal{N}(\\mathbf{0}, t^2 \\mathbf{I})$ , where  $\\otimes$  denotes the convolution operation, and  $\\pi(\\mathbf{x}) = \\mathcal{N}(\\mathbf{0}, T^2 \\mathbf{I})$ . For sampling, we first train a *score model*  $s_{\\phi}(\\mathbf{x},t) \\approx \\nabla \\log p_t(\\mathbf{x})$  via *score matching* (Hyvärinen & Dayan, 2005; Vincent, 2011; Song et al., 2019; Song & Ermon, 2019; Ho et al., 2020), then plug it into Eq. (2) to obtain an empirical estimate of the PF ODE, which takes the form of\n\n<span id=\"page-1-2\"></span>\n$$\\frac{\\mathrm{d}\\mathbf{x}_t}{\\mathrm{d}t} = -t\\mathbf{s}_{\\phi}(\\mathbf{x}_t, t). \\tag{3}$$\n\nWe call Eq. (3) the *empirical PF ODE*. Next, we sample  $\\hat{\\mathbf{x}}_T \\sim \\pi = \\mathcal{N}(\\mathbf{0}, T^2 \\mathbf{I})$  to initialize the empirical PF ODE and solve it backwards in time with any numerical ODE solver, such as Euler (Song et al., 2020; 2021) and Heun solvers (Karras et al., 2022), to obtain the solution trajectory  $\\{\\hat{\\mathbf{x}}_t\\}_{t\\in[0,T]}$ . The resulting  $\\hat{\\mathbf{x}}_0$  can then be viewed as an approximate sample from the data distribution  $p_{\\text{data}}(\\mathbf{x})$ . To avoid numerical instability, one typically stops the solver at  $t=\\epsilon$ , where  $\\epsilon$  is a fixed small positive number, and accepts  $\\hat{\\mathbf{x}}_\\epsilon$  as the approximate sample. Following Karras et al. (2022), we rescale image pixel values to [-1,1], and set T=80,  $\\epsilon=0.002$ .\n\n<span id=\"page-2-0\"></span>![](_page_2_Picture_1.jpeg)\n\nFigure 2: Consistency models are trained to map points on any trajectory of the PF ODE to the trajectory's origin.\n\nDiffusion models are bottlenecked by their slow sampling speed. Clearly, using ODE solvers for sampling requires iterative evaluations of the score model sϕpx, tq, which is computationally costly. Existing methods for fast sampling include faster numerical ODE solvers [\\(Song et al.,](#page-11-13) [2020;](#page-11-13) [Zhang & Chen,](#page-11-8) [2022;](#page-11-8) [Lu et al.,](#page-10-10) [2022;](#page-10-10) [Dockhorn et al.,](#page-9-11) [2022\\)](#page-9-11), and distillation techniques [\\(Luhman & Luhman,](#page-10-13) [2021;](#page-10-13) [Sali](#page-11-10)[mans & Ho,](#page-11-10) [2022;](#page-11-10) [Meng et al.,](#page-10-14) [2022;](#page-10-14) [Zheng et al.,](#page-12-0) [2022\\)](#page-12-0). However, ODE solvers still need more than 10 evaluation steps to generate competitive samples. Most distillation methods like [Luhman & Luhman](#page-10-13) [\\(2021\\)](#page-10-13) and [Zheng et al.](#page-12-0) [\\(2022\\)](#page-12-0) rely on collecting a large dataset of samples from the diffusion model prior to distillation, which itself is computationally expensive. To our best knowledge, the only distillation approach that does not suffer from this drawback is progressive distillation (PD, [Salimans & Ho](#page-11-10) [\\(2022\\)](#page-11-10)), with which we compare consistency models extensively in our experiments.\n\n## <span id=\"page-2-2\"></span>3. Consistency Models\n\nWe propose consistency models, a new type of models that support single-step generation at the core of its design, while still allowing iterative generation for trade-offs between sample quality and compute, and zero-shot data editing. Consistency models can be trained in either the distillation mode or the isolation mode. In the former case, consistency models distill the knowledge of pre-trained diffusion models into a single-step sampler, significantly improving other distillation approaches in sample quality, while allowing zero-shot image editing applications. In the latter case, consistency models are trained in isolation, with no dependence on pretrained diffusion models. This makes them an independent new class of generative models.\n\nBelow we introduce the definition, parameterization, and sampling of consistency models, plus a brief discussion on their applications to zero-shot data editing.\n\nDefinition Given a solution trajectory txtu<sup>t</sup>Prϵ,T<sup>s</sup> of the PF ODE in Eq. [\\(2\\)](#page-1-1), we define the *consistency function* as f : pxt, tq ÞÑ xϵ. A consistency function has the property of *self-consistency*: its outputs are consistent for arbitrary pairs of pxt, tq that belong to the same PF ODE trajectory, *i.e*., fpxt, tq \" fpx<sup>t</sup> <sup>1</sup> , t<sup>1</sup> q for all t, t<sup>1</sup> P rϵ, Ts. As illustrated in Fig. [2,](#page-2-0) the goal of a *consistency model*, symbolized as fθ, is to estimate this consistency function f from data by learning to enforce the self-consistency property (details in Sections [4](#page-3-0) and [5\\)](#page-4-0). Note that a similar definition is used for neural flows [\\(Bilos et al.](#page-9-12) ˇ , [2021\\)](#page-9-12) in the context of neural ODEs [\\(Chen et al.,](#page-9-13) [2018\\)](#page-9-13). Compared to neural flows, however, we do not enforce consistency models to be invertible.\n\nParameterization For any consistency function fp¨, ¨q, we have fpxϵ, ϵq \" xϵ, *i.e*., fp¨, ϵq is an identity function. We call this constraint the *boundary condition*. All consistency models have to meet this boundary condition, as it plays a crucial role in the successful training of consistency models. This boundary condition is also the most confining architectural constraint on consistency models. For consistency models based on deep neural networks, we discuss two ways to implement this boundary condition *almost for free*. Suppose we have a free-form deep neural network Fθpx, tq whose output has the same dimensionality as x. The first way is to simply parameterize the consistency model as\n\n$$\\mathbf{f}_{\\theta}(\\mathbf{x},t) = \\begin{cases} \\mathbf{x} & t = \\epsilon \\\\ F_{\\theta}(\\mathbf{x},t) & t \\in (\\epsilon,T] \\end{cases}$$\n (4)\n\nThe second method is to parameterize the consistency model using skip connections, that is,\n\n<span id=\"page-2-1\"></span>\n$$f_{\\theta}(\\mathbf{x}, t) = c_{\\text{skip}}(t)\\mathbf{x} + c_{\\text{out}}(t)F_{\\theta}(\\mathbf{x}, t),$$\n (5)\n\nwhere cskipptq and coutptq are differentiable functions such that cskippϵq \" 1, and coutpϵq \" 0. This way, the consistency model is differentiable at t \" ϵ if Fθpx, tq, cskipptq, coutptq are all differentiable, which is critical for training continuous-time consistency models (Appendices [B.1](#page-17-0) and [B.2\\)](#page-21-0). The parameterization in Eq. [\\(5\\)](#page-2-1) bears strong resemblance to many successful diffusion models [\\(Karras et al.,](#page-10-12) [2022;](#page-10-12) [Balaji et al.,](#page-9-14) [2022\\)](#page-9-14), making it easier to borrow powerful diffusion model architectures for constructing consistency models. We therefore follow the second parameterization in all experiments.\n\nSampling With a well-trained consistency model fθp¨, ¨q, we can generate samples by sampling from the initial distribution xˆ<sup>T</sup> \" N p0, T<sup>2</sup>Iq and then evaluating the consistency model for xˆ<sup>ϵ</sup> \" fθpxˆ<sup>T</sup> , Tq. This involves only one forward pass through the consistency model and therefore *generates samples in a single step*. Importantly, one can also evaluate the consistency model multiple times by alternating denoising and noise injection steps for improved sample quality. Summarized in Algorithm [1,](#page-3-1) this *multistep* sampling procedure provides the flexibility to trade compute for sample quality. It also has important applications in zero-shot data editing. In practice, we find time points\n\n#### <span id=\"page-3-1\"></span>Algorithm 1 Multistep Consistency Sampling\n\n```\nInput: Consistency model f_{\\theta}(\\cdot,\\cdot), sequence of time points \\tau_1 > \\tau_2 > \\cdots > \\tau_{N-1}, initial noise \\hat{\\mathbf{x}}_T \\mathbf{x} \\leftarrow f_{\\theta}(\\hat{\\mathbf{x}}_T,T) for n=1 to N-1 do Sample \\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I}) \\hat{\\mathbf{x}}_{\\tau_n} \\leftarrow \\mathbf{x} + \\sqrt{\\tau_n^2 - \\epsilon^2}\\mathbf{z} \\mathbf{x} \\leftarrow f_{\\theta}(\\hat{\\mathbf{x}}_{\\tau_n},\\tau_n) end for Output: \\mathbf{x}\n```\n\n $\\{\\tau_1, \\tau_2, \\cdots, \\tau_{N-1}\\}$  in Algorithm 1 with a greedy algorithm, where the time points are pinpointed one at a time using ternary search to optimize the FID of samples obtained from Algorithm 1. This assumes that given prior time points, the FID is a unimodal function of the next time point. We find this assumption to hold empirically in our experiments, and leave the exploration of better strategies as future work.\n\n**Zero-Shot Data Editing** Similar to diffusion models, consistency models enable various data editing and manipulation applications in zero shot; they do not require explicit training to perform these tasks. For example, consistency models define a one-to-one mapping from a Gaussian noise vector to a data sample. Similar to latent variable models like GANs, VAEs, and normalizing flows, consistency models can easily interpolate between samples by traversing the latent space (Fig. 11). As consistency models are trained to recover  $\\mathbf{x}_{\\epsilon}$  from any noisy input  $\\mathbf{x}_{t}$  where  $t \\in [\\epsilon, T]$ , they can perform denoising for various noise levels (Fig. 12). Moreover, the multistep generation procedure in Algorithm 1 is useful for solving certain inverse problems in zero shot by using an iterative replacement procedure similar to that of diffusion models (Song & Ermon, 2019; Song et al., 2021; Ho et al., 2022b). This enables many applications in the context of image editing, including inpainting (Fig. 10), colorization (Fig. 8), super-resolution (Fig. 6b) and stroke-guided image editing (Fig. 13) as in SDEdit (Meng et al., 2021). In Section 6.3, we empirically demonstrate the power of consistency models on many zero-shot image editing tasks.\n\n#### <span id=\"page-3-0\"></span>4. Training Consistency Models via Distillation\n\nWe present our first method for training consistency models based on distilling a pre-trained score model  $s_{\\phi}(\\mathbf{x},t)$ . Our discussion revolves around the empirical PF ODE in Eq. (3), obtained by plugging the score model  $s_{\\phi}(\\mathbf{x},t)$  into the PF ODE. Consider discretizing the time horizon  $[\\epsilon,T]$  into N-1 sub-intervals, with boundaries  $t_1=\\epsilon < t_2 < \\cdots < t_N = T$ . In practice, we follow Karras et al. (2022) to determine the boundaries with the formula  $t_i = (\\epsilon^{1/\\rho} + i^{-1}/N - 1(T^{1/\\rho} - \\epsilon^{1/\\rho}))^{\\rho}$ , where  $\\rho = 7$ . When\n\nN is sufficiently large, we can obtain an accurate estimate of  $\\mathbf{x}_{t_n}$  from  $\\mathbf{x}_{t_{n+1}}$  by running one discretization step of a numerical ODE solver. This estimate, which we denote as  $\\hat{\\mathbf{x}}_{t_n}^{\\phi}$ , is defined by\n\n<span id=\"page-3-2\"></span>\n$$\\hat{\\mathbf{x}}_{t_n}^{\\phi} := \\mathbf{x}_{t_{n+1}} + (t_n - t_{n+1}) \\Phi(\\mathbf{x}_{t_{n+1}}, t_{n+1}; \\phi), \\quad (6)$$\n\nwhere  $\\Phi(\\cdots; \\phi)$  represents the update function of a one-step ODE solver applied to the empirical PF ODE. For example, when using the Euler solver, we have  $\\Phi(\\mathbf{x}, t; \\phi) = -ts_{\\phi}(\\mathbf{x}, t)$  which corresponds to the following update rule\n\n$$\\hat{\\mathbf{x}}_{t_n}^{\\phi} = \\mathbf{x}_{t_{n+1}} - (t_n - t_{n+1})t_{n+1}\\mathbf{s}_{\\phi}(\\mathbf{x}_{t_{n+1}}, t_{n+1}).$$\n\nFor simplicity, we only consider one-step ODE solvers in this work. It is straightforward to generalize our framework to multistep ODE solvers and we leave it as future work.\n\nDue to the connection between the PF ODE in Eq. (2) and the SDE in Eq. (1) (see Section 2), one can sample along the distribution of ODE trajectories by first sampling  $\\mathbf{x} \\sim p_{\\text{data}}$ , then adding Gaussian noise to  $\\mathbf{x}$ . Specifically, given a data point  $\\mathbf{x}$ , we can generate a pair of adjacent data points  $(\\hat{\\mathbf{x}}_{t_n}^{\\phi}, \\mathbf{x}_{t_{n+1}})$  on the PF ODE trajectory efficiently by sampling  $\\mathbf{x}$  from the dataset, followed by sampling  $\\mathbf{x}_{t_{n+1}}$  from the transition density of the SDE  $\\mathcal{N}(\\mathbf{x}, t_{n+1}^2 \\mathbf{I})$ , and then computing  $\\hat{\\mathbf{x}}_{t_n}^{\\phi}$  using one discretization step of the numerical ODE solver according to Eq. (6). Afterwards, we train the consistency model by minimizing its output differences on the pair  $(\\hat{\\mathbf{x}}_{t_n}^{\\phi}, \\mathbf{x}_{t_{n+1}})$ . This motivates our following *consistency distillation* loss for training consistency models.\n\n<span id=\"page-3-3\"></span>**Definition 1.** The consistency distillation loss is defined as\n\n$$\\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}; \\boldsymbol{\\phi}) := \\mathbb{E}[\\lambda(t_n) d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}), \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t}^{\\boldsymbol{\\phi}}, t_n))], \\quad (7)$$\n\nwhere the expectation is taken with respect to  $\\mathbf{x} \\sim p_{data}$ ,  $n \\sim \\mathcal{U}[\\![1,N-1]\\!]$ , and  $\\mathbf{x}_{t_{n+1}} \\sim \\mathcal{N}(\\mathbf{x};t_{n+1}^2\\mathbf{I})$ . Here  $\\mathcal{U}[\\![1,N-1]\\!]$  denotes the uniform distribution over  $\\{1,2,\\cdots,N-1\\}$ ,  $\\lambda(\\cdot) \\in \\mathbb{R}^+$  is a positive weighting function,  $\\hat{\\mathbf{x}}_{t_n}^{\\boldsymbol{\\phi}}$  is given by Eq. (6),  $\\boldsymbol{\\theta}^-$  denotes a running average of the past values of  $\\boldsymbol{\\theta}$  during the course of optimization, and  $d(\\cdot,\\cdot)$  is a metric function that satisfies  $\\forall \\mathbf{x},\\mathbf{y}:d(\\mathbf{x},\\mathbf{y})\\geqslant 0$  and  $d(\\mathbf{x},\\mathbf{y})=0$  if and only if  $\\mathbf{x}=\\mathbf{y}$ .\n\nUnless otherwise stated, we adopt the notations in Definition 1 throughout this paper, and use  $\\mathbb{E}[\\cdot]$  to denote the expectation over all random variables. In our experiments, we consider the squared  $\\ell_2$  distance  $d(\\mathbf{x}, \\mathbf{y}) = \\|\\mathbf{x} - \\mathbf{y}\\|_2^2$ ,  $\\ell_1$  distance  $d(\\mathbf{x}, \\mathbf{y}) = \\|\\mathbf{x} - \\mathbf{y}\\|_1$ , and the Learned Perceptual Image Patch Similarity (LPIPS, Zhang et al. (2018)). We find  $\\lambda(t_n) \\equiv 1$  performs well across all tasks and datasets. In practice, we minimize the objective by stochastic gradient descent on the model parameters  $\\theta$ , while updating  $\\theta^-$  with exponential moving average (EMA). That is, given a decay\n\n#### <span id=\"page-4-1\"></span>Algorithm 2 Consistency Distillation (CD)\n\nInput: dataset \n$$\\mathcal{D}$$\n, initial model parameter  $\\boldsymbol{\\theta}$ , learning rate  $\\eta$ , ODE solver  $\\Phi(\\cdot,\\cdot;\\boldsymbol{\\phi}),d(\\cdot,\\cdot),\\lambda(\\cdot),$  and  $\\mu$   $\\boldsymbol{\\theta}^-\\leftarrow\\boldsymbol{\\theta}$  repeat Sample  $\\mathbf{x}\\sim\\mathcal{D}$  and  $n\\sim\\mathcal{U}[\\![1,N-1]\\!]$  Sample  $\\mathbf{x}_{t_{n+1}}\\sim\\mathcal{N}(\\mathbf{x};t_{n+1}^2\\boldsymbol{I})$   $\\hat{\\mathbf{x}}_{t_n}^{\\boldsymbol{\\phi}}\\leftarrow\\mathbf{x}_{t_{n+1}}+(t_n-t_{n+1})\\Phi(\\mathbf{x}_{t_{n+1}},t_{n+1};\\boldsymbol{\\phi})$   $\\mathcal{L}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^-;\\boldsymbol{\\phi})\\leftarrow$   $\\lambda(t_n)d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\boldsymbol{f}_{\\boldsymbol{\\theta}^-}(\\hat{\\mathbf{x}}_{t_n}^{\\boldsymbol{\\phi}},t_n))$   $\\boldsymbol{\\theta}\\leftarrow\\boldsymbol{\\theta}-\\eta\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^-;\\boldsymbol{\\phi})$   $\\boldsymbol{\\theta}^-\\leftarrow\\text{stopgrad}(\\mu\\boldsymbol{\\theta}^-+(1-\\mu)\\boldsymbol{\\theta})$  until convergence\n\nrate  $0 \\le \\mu < 1$ , we perform the following update after each optimization step:\n\n$$\\boldsymbol{\\theta}^- \\leftarrow \\operatorname{stopgrad}(\\mu \\boldsymbol{\\theta}^- + (1 - \\mu)\\boldsymbol{\\theta}).$$\n (8)\n\nThe overall training procedure is summarized in Algorithm 2. In alignment with the convention in deep reinforcement learning (Mnih et al., 2013; 2015; Lillicrap et al., 2015) and momentum based contrastive learning (Grill et al., 2020; He et al., 2020), we refer to  $f_{\\theta^-}$  as the \"target network\", and  $f_{\\theta}$  as the \"online network\". We find that compared to simply setting  $\\theta^- = \\theta$ , the EMA update and \"stopgrad\" operator in Eq. (8) can greatly stabilize the training process and improve the final performance of the consistency model.\n\nBelow we provide a theoretical justification for consistency distillation based on asymptotic analysis.\n\n<span id=\"page-4-3\"></span>**Theorem 1.** Let  $\\Delta t := \\max_{n \\in [\\![ 1,N-1]\\!]} \\{|t_{n+1}-t_n|\\}$ , and  $f(\\cdot,\\cdot;\\phi)$  be the consistency function of the empirical PF ODE in Eq. (3). Assume  $f_{\\theta}$  satisfies the Lipschitz condition: there exists L > 0 such that for all  $t \\in [\\epsilon,T]$ ,  $\\mathbf{x}$ , and  $\\mathbf{y}$ , we have  $\\|f_{\\theta}(\\mathbf{x},t) - f_{\\theta}(\\mathbf{y},t)\\|_2 \\leq L \\|\\mathbf{x} - \\mathbf{y}\\|_2$ . Assume further that for all  $n \\in [\\![ 1,N-1]\\!]$ , the ODE solver called at  $t_{n+1}$  has local error uniformly bounded by  $O((t_{n+1}-t_n)^{p+1})$  with  $p \\geq 1$ . Then, if  $\\mathcal{L}_{OD}^{N}(\\theta,\\theta;\\phi) = 0$ , we have\n\n$$\\sup_{n,\\mathbf{x}} \\|\\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{x},t_n) - \\mathbf{f}(\\mathbf{x},t_n;\\boldsymbol{\\phi})\\|_2 = O((\\Delta t)^p).$$\n\n*Proof.* The proof is based on induction and parallels the classic proof of global error bounds for numerical ODE solvers (Süli & Mayers, 2003). We provide the full proof in Appendix A.2. □\n\nSince  $\\theta^-$  is a running average of the history of  $\\theta$ , we have  $\\theta^- = \\theta$  when the optimization of Algorithm 2 converges. That is, the target and online consistency models will eventually match each other. If the consistency model additionally achieves zero consistency distillation loss, then Theorem 1\n\n#### <span id=\"page-4-4\"></span>Algorithm 3 Consistency Training (CT)\n\nInput: dataset  $\\mathcal{D}$ , initial model parameter  $\\boldsymbol{\\theta}$ , learning rate  $\\eta$ , step schedule  $N(\\cdot)$ , EMA decay rate schedule  $\\mu(\\cdot)$ ,  $d(\\cdot,\\cdot)$ , and  $\\lambda(\\cdot)$   $\\boldsymbol{\\theta}^- \\leftarrow \\boldsymbol{\\theta}$  and  $k \\leftarrow 0$  repeat Sample  $\\mathbf{z} \\sim \\mathcal{D}$ , and  $n \\sim \\mathcal{U}[\\![1,N(k)-1]\\!]$  Sample  $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0},\\boldsymbol{I})$   $\\mathcal{L}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^-) \\leftarrow \\lambda(t_n)d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1}),\\boldsymbol{f}_{\\boldsymbol{\\theta}^-}(\\mathbf{x}+t_n\\mathbf{z},t_n))$   $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\eta\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^-)$   $\\boldsymbol{\\theta}^- \\leftarrow \\operatorname{stopgrad}(\\mu(k)\\boldsymbol{\\theta}^- + (1-\\mu(k))\\boldsymbol{\\theta})$   $k \\leftarrow k+1$  until convergence\n\n<span id=\"page-4-2\"></span>implies that, under some regularity conditions, the estimated consistency model can become arbitrarily accurate, as long as the step size of the ODE solver is sufficiently small. Importantly, our boundary condition  $f_{\\theta}(\\mathbf{x}, \\epsilon) \\equiv \\mathbf{x}$  precludes the trivial solution  $f_{\\theta}(\\mathbf{x}, t) \\equiv \\mathbf{0}$  from arising in consistency model training.\n\nThe consistency distillation loss  $\\mathcal{L}_{\\mathrm{CD}}^N(\\theta, \\theta^-; \\phi)$  can be extended to hold for infinitely many time steps  $(N \\to \\infty)$  if  $\\theta^- = \\theta$  or  $\\theta^- = \\mathrm{stopgrad}(\\theta)$ . The resulting continuous-time loss functions do not require specifying N nor the time steps  $\\{t_1, t_2, \\cdots, t_N\\}$ . Nonetheless, they involve Jacobian-vector products and require forward-mode automatic differentiation for efficient implementation, which may not be well-supported in some deep learning frameworks. We provide these continuous-time distillation loss functions in Theorems 3 to 5, and relegate details to Appendix B.1.\n\n#### <span id=\"page-4-0\"></span>5. Training Consistency Models in Isolation\n\nConsistency models can be trained without relying on any pre-trained diffusion models. This differs from existing diffusion distillation techniques, making consistency models a new independent family of generative models.\n\nRecall that in consistency distillation, we rely on a pretrained score model  $s_{\\phi}(\\mathbf{x},t)$  to approximate the ground truth score function  $\\nabla \\log p_t(\\mathbf{x})$ . It turns out that we can avoid this pre-trained score model altogether by leveraging the following unbiased estimator (Lemma 1 in Appendix A):\n\n$$\\nabla \\log p_t(\\mathbf{x}_t) = -\\mathbb{E}\\left[\\frac{\\mathbf{x}_t - \\mathbf{x}}{t^2} \\mid \\mathbf{x}_t\\right],$$\n\nwhere  $\\mathbf{x} \\sim p_{\\text{data}}$  and  $\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{x}; t^2 \\mathbf{I})$ . That is, given  $\\mathbf{x}$  and  $\\mathbf{x}_t$ , we can estimate  $\\nabla \\log p_t(\\mathbf{x}_t)$  with  $-(\\mathbf{x}_t - \\mathbf{x})/t^2$ .\n\nThis unbiased estimate suffices to replace the pre-trained diffusion model in consistency distillation when using the Euler method as the ODE solver in the limit of  $N \\to \\infty$ , as\n\njustified by the following result.\n\n<span id=\"page-5-2\"></span>**Theorem 2.** Let  $\\Delta t := \\max_{n \\in [\\![ 1,N-1 ]\\!]} \\{|t_{n+1}-t_n|\\}$ . Assume d and  $f_{\\theta^-}$  are both twice continuously differentiable with bounded second derivatives, the weighting function  $\\lambda(\\cdot)$  is bounded, and  $\\mathbb{E}[\\|\\nabla \\log p_{t_n}(\\mathbf{x}_{t_n})\\|_2^2] < \\infty$ . Assume further that we use the Euler ODE solver, and the pre-trained score model matches the ground truth, i.e.,  $\\forall t \\in [\\epsilon, T] : s_{\\phi}(\\mathbf{x}, t) \\equiv \\nabla \\log p_t(\\mathbf{x})$ . Then,\n\n$$\\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}; \\boldsymbol{\\phi}) = \\mathcal{L}_{CT}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}) + o(\\Delta t), \\tag{9}$$\n\nwhere the expectation is taken with respect to  $\\mathbf{x} \\sim p_{data}$ ,  $n \\sim \\mathcal{U}[\\![1,N-1]\\!]$ , and  $\\mathbf{x}_{t_{n+1}} \\sim \\mathcal{N}(\\mathbf{x};t_{n+1}^2\\mathbf{I})$ . The consistency training objective, denoted by  $\\mathcal{L}_{CT}^N(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^-)$ , is defined as\n\n$$\\mathbb{E}[\\lambda(t_n)d(\\mathbf{f}_{\\theta}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1}),\\mathbf{f}_{\\theta^-}(\\mathbf{x}+t_n\\mathbf{z},t_n))], (10)$$\n\nwhere  $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ . Moreover,  $\\mathcal{L}_{CT}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}) \\geqslant O(\\Delta t)$  if  $\\inf_{N} \\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}; \\boldsymbol{\\phi}) > 0$ .\n\n*Proof.* The proof is based on Taylor series expansion and properties of score functions (Lemma 1). A complete proof is provided in Appendix A.3.\n\nWe refer to Eq. (10) as the *consistency training* (CT) loss. Crucially,  $\\mathcal{L}(\\theta, \\theta^-)$  only depends on the online network  $f_{\\theta}$ , and the target network  $f_{\\theta^-}$ , while being completely agnostic to diffusion model parameters  $\\phi$ . The loss function  $\\mathcal{L}(\\theta, \\theta^-) \\geqslant O(\\Delta t)$  decreases at a slower rate than the remainder  $o(\\Delta t)$  and thus will dominate the loss in Eq. (9) as  $N \\to \\infty$  and  $\\Delta t \\to 0$ .\n\nFor improved practical performance, we propose to progressively increase N during training according to a schedule function  $N(\\cdot)$ . The intuition (cf., Fig. 3d) is that the consistency training loss has less \"variance\" but more \"bias\" with respect to the underlying consistency distillation loss (i.e., the left-hand side of Eq. (9)) when N is small  $(i.e., \\Delta t$  is large), which facilitates faster convergence at the beginning of training. On the contrary, it has more \"variance\" but less \"bias\" when N is large  $(i.e., \\Delta t$  is small), which is desirable when closer to the end of training. For best performance, we also find that  $\\mu$  should change along with N, according to a schedule function  $\\mu(\\cdot)$ . The full algorithm of consistency training is provided in Algorithm 3, and the schedule functions used in our experiments are given in Appendix C.\n\nSimilar to consistency distillation, the consistency training loss  $\\mathcal{L}_{\\mathrm{CT}}^N(\\theta,\\theta^-)$  can be extended to hold in continuous time  $(i.e., N \\to \\infty)$  if  $\\theta^- = \\mathrm{stopgrad}(\\theta)$ , as shown in Theorem 6. This continuous-time loss function does not require schedule functions for N or  $\\mu$ , but requires forward-mode automatic differentiation for efficient implementation. Unlike the discrete-time CT loss, there is no undesirable \"bias\" associated with the continuous-time objective, as we effectively take  $\\Delta t \\to 0$  in Theorem 2. We relegate more details to Appendix B.2.\n\n### <span id=\"page-5-3\"></span>6. Experiments\n\nWe employ consistency distillation and consistency training to learn consistency models on real image datasets, including CIFAR-10 (Krizhevsky et al., 2009), ImageNet 64 × 64 (Deng et al., 2009), LSUN Bedroom 256 × 256, and LSUN Cat 256 × 256 (Yu et al., 2015). Results are compared according to Fréchet Inception Distance (FID, Heusel et al. (2017), lower is better), Inception Score (IS, Salimans et al. (2016), higher is better), Precision (Prec., Kynkäänniemi et al. (2019), higher is better), and Recall (Rec., Kynkäänniemi et al. (2019), higher is better). Additional experimental details are provided in Appendix C.\n\n#### <span id=\"page-5-4\"></span><span id=\"page-5-1\"></span><span id=\"page-5-0\"></span>**6.1. Training Consistency Models**\n\nWe perform a series of experiments on CIFAR-10 to understand the effect of various hyperparameters on the performance of consistency models trained by consistency distillation (CD) and consistency training (CT). We first focus on the effect of the metric function  $d(\\cdot, \\cdot)$ , the ODE solver, and the number of discretization steps N in CD, then investigate the effect of the schedule functions  $N(\\cdot)$  and  $\\mu(\\cdot)$  in CT.\n\nTo set up our experiments for CD, we consider the squared  $\\ell_2$  distance  $d(\\mathbf{x}, \\mathbf{y}) = \\|\\mathbf{x} - \\mathbf{y}\\|_2^2$ ,  $\\ell_1$  distance  $d(\\mathbf{x}, \\mathbf{y}) = \\|\\mathbf{x} - \\mathbf{y}\\|_1$ , and the Learned Perceptual Image Patch Similarity (LPIPS, Zhang et al. (2018)) as the metric function. For the ODE solver, we compare Euler's forward method and Heun's second order method as detailed in Karras et al. (2022). For the number of discretization steps N, we compare  $N \\in \\{9, 12, 18, 36, 50, 60, 80, 120\\}$ . All consistency models trained by CD in our experiments are initialized with the corresponding pre-trained diffusion models, whereas models trained by CT are randomly initialized.\n\nAs visualized in Fig. 3a, the optimal metric for CD is LPIPS, which outperforms both  $\\ell_1$  and  $\\ell_2$  by a large margin over all training iterations. This is expected as the outputs of consistency models are images on CIFAR-10, and LPIPS is specifically designed for measuring the similarity between natural images. Next, we investigate which ODE solver and which discretization step N work the best for CD. As shown in Figs. 3b and 3c, Heun ODE solver and N=18 are the best choices. Both are in line with the recommendation of Karras et al. (2022) despite the fact that we are training consistency models, not diffusion models. Moreover, Fig. 3b shows that with the same N, Heun's second order solver uniformly outperforms Euler's first order solver. This corroborates with Theorem 1, which states that the optimal consistency models trained by higher order ODE solvers have smaller estimation errors with the same N. The results of Fig. 3c also indicate that once N is sufficiently large, the performance of CD becomes insensitive to N. Given these insights, we hereafter use LPIPS and Heun ODE solver for CD unless otherwise stated. For N in CD, we follow the\n\n<span id=\"page-6-0\"></span>![](_page_6_Figure_1.jpeg)\n\nFigure 3: Various factors that affect consistency distillation (CD) and consistency training (CT) on CIFAR-10. The best configuration for CD is LPIPS, Heun ODE solver, and N \" 18. Our adaptive schedule functions for N and µ make CT converge significantly faster than fixing them to be constants during the course of optimization.\n\n<span id=\"page-6-1\"></span>![](_page_6_Figure_3.jpeg)\n\nFigure 4: Multistep image generation with consistency distillation (CD). CD outperforms progressive distillation (PD) across all datasets and sampling steps. The only exception is single-step generation on Bedroom 256 ˆ 256.\n\nsuggestions in [Karras et al.](#page-10-12) [\\(2022\\)](#page-10-12) on CIFAR-10 and ImageNet 64 ˆ 64. We tune N separately on other datasets (details in Appendix [C\\)](#page-24-0).\n\nDue to the strong connection between CD and CT, we adopt LPIPS for our CT experiments throughout this paper. Unlike CD, there is no need for using Heun's second order solver in CT as the loss function does not rely on any particular numerical ODE solver. As demonstrated in Fig. [3d,](#page-6-0) the convergence of CT is highly sensitive to N—smaller N leads to faster convergence but worse samples, whereas larger N leads to slower convergence but better samples upon convergence. This matches our analysis in Section [5,](#page-4-0) and motivates our practical choice of progressively growing N and µ for CT to balance the trade-off between convergence speed and sample quality. As shown in Fig. [3d,](#page-6-0) adaptive schedules of N and µ significantly improve the convergence speed and sample quality of CT. In our experiments, we tune the schedules Np¨q and µp¨q separately for images of different resolutions, with more details in Appendix [C.](#page-24-0)\n\n#### <span id=\"page-6-2\"></span>6.2. Few-Step Image Generation\n\nDistillation In current literature, the most directly comparable approach to our consistency distillation (CD) is progressive distillation (PD, [Salimans & Ho](#page-11-10) [\\(2022\\)](#page-11-10)); both are thus far the only distillation approaches that *do not construct synthetic data before distillation*. In stark contrast, other distillation techniques, such as knowledge distillation [\\(Luhman](#page-10-13) [& Luhman,](#page-10-13) [2021\\)](#page-10-13) and DFNO [\\(Zheng et al.,](#page-12-0) [2022\\)](#page-12-0), have to prepare a large synthetic dataset by generating numerous samples from the diffusion model with expensive numerical ODE/SDE solvers. We perform comprehensive comparison for PD and CD on CIFAR-10, ImageNet 64ˆ64, and LSUN 256 ˆ 256, with all results reported in Fig. [4.](#page-6-1) All methods distill from an EDM [\\(Karras et al.,](#page-10-12) [2022\\)](#page-10-12) model that we pretrained in-house. We note that across all sampling iterations, *using the LPIPS metric uniformly improves PD compared to the squared* ℓ<sup>2</sup> *distance in the original paper of [Salimans](#page-11-10) [& Ho](#page-11-10) [\\(2022\\)](#page-11-10)*. Both PD and CD improve as we take more sampling steps. We find that CD uniformly outperforms PD across all datasets, sampling steps, and metric functions considered, except for single-step generation on Bedroom 256 ˆ 256, where CD with ℓ<sup>2</sup> slightly underperforms PD with ℓ2. As shown in Table [1,](#page-7-0) CD even outperforms distillation approaches that require synthetic dataset construction, such as Knowledge Distillation [\\(Luhman & Luhman,](#page-10-13) [2021\\)](#page-10-13) and DFNO [\\(Zheng et al.,](#page-12-0) [2022\\)](#page-12-0).\n\nDirect Generation In Tables [1](#page-7-0) and [2,](#page-7-0) we compare the sample quality of consistency training (CT) with other generative models using one-step and two-step generation. We also include PD and CD results for reference. Both tables report PD results obtained from the ℓ<sup>2</sup> metric function, as this is the default setting used in the original paper of [Salimans](#page-11-10)\n\nsynthetic data construction for distillation.\n\n<span id=\"page-7-0\"></span>Table 1: Sample quality on CIFAR-10. \\*Methods that require Table 2: Sample quality on ImageNet 64 × 64, and LSUN Bedroom & Cat  $256 \\times 256$ . †Distillation techniques.\n\n| METHOD                                          | NFE (↓) | FID (↓) | IS (↑) | METHOD                                 | NFE (↓) | FID (↓) | Prec. (†) | Rec. (†) |\n|-------------------------------------------------|---------|---------|--------|----------------------------------------|---------|---------|-----------|----------|\n| Diffusion + Samplers                            |         |         |        | ImageNet $64 \\times 64$                |         |         |           |          |\n| DDIM (Song et al., 2020)                        | 50      | 4.67    |        | PD <sup>†</sup> (Salimans & Ho, 2022)  | 1       | 15.39   | 0.59      | 0.62     |\n| DDIM (Song et al., 2020)                        | 20      | 6.84    |        | DFNO <sup>†</sup> (Zheng et al., 2022) | 1       | 8.35    |           |          |\n| DDIM (Song et al., 2020)                        | 10      | 8.23    |        | $\\mathbf{C}\\mathbf{D}^{\\dagger}$       | 1       | 6.20    | 0.68      | 0.63     |\n| DPM-solver-2 (Lu et al., 2022)                  | 10      | 5.94    |        | PD <sup>†</sup> (Salimans & Ho, 2022)  | 2       | 8.95    | 0.63      | 0.65     |\n| DPM-solver-fast (Lu et al., 2022)               | 10      | 4.70    |        | $\\mathbf{C}\\mathbf{D}^{\\dagger}$       | 2       | 4.70    | 0.69      | 0.64     |\n| 3-DEIS (Zhang & Chen, 2022)                     | 10      | 4.17    |        | ADM (Dhariwal & Nichol, 2021)          | 250     | 2.07    | 0.74      | 0.63     |\n| Diffusion + Distillation                        |         |         |        | EDM (Karras et al., 2022)              | 79      | 2.44    | 0.71      | 0.67     |\n| Knowledge Distillation* (Luhman & Luhman, 2021) | 1       | 9.36    |        | BigGAN-deep (Brock et al., 2019)       | 1       | 4.06    | 0.79      | 0.48     |\n| DFNO* (Zheng et al., 2022)                      | 1       | 4.12    |        | CT                                     | 1       | 13.0    | 0.71      | 0.47     |\n| 1-Rectified Flow (+distill)* (Liu et al., 2022) | 1       | 6.18    | 9.08   | CT                                     | 2       | 11.1    | 0.69      | 0.56     |\n| 2-Rectified Flow (+distill)* (Liu et al., 2022) | 1       | 4.85    | 9.01   |                                        |         |         |           |          |\n| 3-Rectified Flow (+distill)* (Liu et al., 2022) | 1       | 5.21    | 8.79   | LSUN Bedroom 256 × 256                 |         |         |           |          |\n| PD (Salimans & Ho, 2022)                        | 1       | 8.34    | 8.69   | PD <sup>†</sup> (Salimans & Ho, 2022)  | 1       | 16.92   | 0.47      | 0.27     |\n| CD                                              | 1       | 3.55    | 9.48   | PD <sup>†</sup> (Salimans & Ho, 2022)  | 2       | 8.47    | 0.56      | 0.39     |\n| PD (Salimans & Ho, 2022)                        | 2       | 5.58    | 9.05   | $\\mathbf{C}\\mathbf{D}^{\\dagger}$       | 1       | 7.80    | 0.66      | 0.34     |\n| CD                                              | 2       | 2.93    | 9.75   | CD <sup>†</sup>                        | 2       | 5.22    | 0.68      | 0.39     |\n| Direct Generation                               |         |         |        | DDPM (Ho et al., 2020)                 | 1000    | 4.89    | 0.60      | 0.45     |\n| BigGAN (Brock et al., 2019)                     | 1       | 14.7    | 9.22   | ADM (Dhariwal & Nichol, 2021)          | 1000    | 1.90    | 0.66      | 0.51     |\n| Diffusion GAN (Xiao et al., 2022)               | 1       | 14.6    | 8.93   | EDM (Karras et al., 2022)              | 79      | 3.57    | 0.66      | 0.45     |\n| AutoGAN (Gong et al., 2019)                     | 1       | 12.4    | 8.55   | PGGAN (Karras et al., 2018)            | 1       | 8.34    |           |          |\n| E2GAN (Tian et al., 2020)                       | 1       | 11.3    | 8.51   | PG-SWGAN (Wu et al., 2019)             | 1       | 8.0     |           |          |\n| ViTGAN (Lee et al., 2021)                       | 1       | 6.66    | 9.30   | TDPM (GAN) (Zheng et al., 2023)        | 1       | 5.24    |           |          |\n| TransGAN (Jiang et al., 2021)                   | 1       | 9.26    | 9.05   | StyleGAN2 (Karras et al., 2020)        | 1       | 2.35    | 0.59      | 0.48     |\n| StyleGAN2-ADA (Karras et al., 2020)             | 1       | 2.92    | 9.83   | CT                                     | 1       | 16.0    | 0.60      | 0.17     |\n| StyleGAN-XL (Sauer et al., 2022)                | 1       | 1.85    |        | CT                                     | 2       | 7.85    | 0.68      | 0.33     |\n| Score SDE (Song et al., 2021)                   | 2000    | 2.20    | 9.89   | <b>LSUN Cat 256</b> × <b>256</b>       |         |         |           |          |\n| DDPM (Ho et al., 2020)                          | 1000    | 3.17    | 9.46   |                                        | 1       | 20.6    | 0.51      | 0.25     |\n| LSGM (Vahdat et al., 2021)                      | 147     | 2.10    |        | PD <sup>†</sup> (Salimans & Ho, 2022)  | -       | 29.6    | 0.51      | 0.25     |\n| PFGM (Xu et al., 2022)                          | 110     | 2.35    | 9.68   | PD <sup>†</sup> (Salimans & Ho, 2022)  | 2       | 15.5    | 0.59      | 0.36     |\n| EDM (Karras et al., 2022)                       | 35      | 2.04    | 9.84   | CD <sup>†</sup>                        | 1       | 11.0    | 0.65      | 0.36     |\n| 1-Rectified Flow (Liu et al., 2022)             | 1       | 378     | 1.13   | $\\mathbf{C}\\mathbf{D}^{\\dagger}$       | 2       | 8.84    | 0.66      | 0.40     |\n| Glow (Kingma & Dhariwal, 2018)                  | 1       | 48.9    | 3.92   | DDPM (Ho et al., 2020)                 | 1000    | 17.1    | 0.53      | 0.48     |\n| Residual Flow (Chen et al., 2019)               | 1       | 46.4    |        | ADM (Dhariwal & Nichol, 2021)          | 1000    | 5.57    | 0.63      | 0.52     |\n| GLFlow (Xiao et al., 2019)                      | 1       | 44.6    |        | EDM (Karras et al., 2022)              | 79      | 6.69    | 0.70      | 0.43     |\n| DenseFlow (Grcić et al., 2021)                  | 1       | 34.9    |        | PGGAN (Karras et al., 2018)            | 1       | 37.5    |           |          |\n| DC-VAE (Parmar et al., 2021)                    | 1       | 17.9    | 8.20   | StyleGAN2 (Karras et al., 2020)        | 1       | 7.25    | 0.58      | 0.43     |\n| CT                                              | 1       | 8.70    | 8.49   | CT                                     | 1       | 20.7    | 0.56      | 0.23     |\n| CT                                              | 2       | 5.83    | 8.85   | CT                                     | 2       | 11.7    | 0.63      | 0.36     |\n\n<span id=\"page-7-1\"></span>![](_page_7_Figure_4.jpeg)\n\nFigure 5: Samples generated by EDM (top), CT + single-step generation (middle), and CT + 2-step generation (Bottom). All corresponding images are generated from the same initial noise.\n\n<span id=\"page-8-0\"></span>![](_page_8_Figure_1.jpeg)\n\n(a) *Left*: The gray-scale image. *Middle*: Colorized images. *Right*: The ground-truth image.\n\n![](_page_8_Figure_3.jpeg)\n\n(b) *Left*: The downsampled image (32 ˆ 32). *Middle*: Full resolution images (256 ˆ 256). *Right*: The ground-truth image (256 ˆ 256).\n\n![](_page_8_Figure_5.jpeg)\n\n(c) *Left*: A stroke input provided by users. *Right*: Stroke-guided image generation.\n\nFigure 6: Zero-shot image editing with a consistency model trained by consistency distillation on LSUN Bedroom 256ˆ256.\n\n[& Ho](#page-11-10) [\\(2022\\)](#page-11-10). For fair comparison, we ensure PD and CD distill the same EDM models. In Tables [1](#page-7-0) and [2,](#page-7-0) we observe that CT outperforms existing single-step, non-adversarial generative models, *i.e*., VAEs and normalizing flows, by a significant margin on CIFAR-10. Moreover, *CT achieves comparable quality to one-step samples from PD without relying on distillation*. In Fig. [5,](#page-7-1) we provide EDM samples (top), single-step CT samples (middle), and two-step CT samples (bottom). In Appendix [E,](#page-27-0) we show additional samples for both CD and CT in Figs. [14](#page-34-0) to [21.](#page-41-0) Importantly, *all samples obtained from the same initial noise vector share significant structural similarity*, even though CT and EDM models are trained independently from one another. This indicates that CT is less likely to suffer from mode collapse, as EDMs do not.\n\n#### <span id=\"page-8-1\"></span>6.3. Zero-Shot Image Editing\n\nSimilar to diffusion models, consistency models allow zeroshot image editing by modifying the multistep sampling process in Algorithm [1.](#page-3-1) We demonstrate this capability with a consistency model trained on the LSUN bedroom dataset using consistency distillation. In Fig. [6a,](#page-8-0) we show such a consistency model can colorize gray-scale bedroom images at test time, even though it has never been trained on colorization tasks. In Fig. [6b,](#page-8-0) we show the same consistency model can generate high-resolution images from low-resolution inputs. In Fig. [6c,](#page-8-0) we additionally demonstrate that it can generate images based on stroke inputs created by humans, as in SDEdit for diffusion models [\\(Meng](#page-10-6) [et al.,](#page-10-6) [2021\\)](#page-10-6). Again, this editing capability is zero-shot, as the model has not been trained on stroke inputs. In Appendix [D,](#page-25-0) we additionally demonstrate the zero-shot\n\ncapability of consistency models on inpainting (Fig. [10\\)](#page-30-0), interpolation (Fig. [11\\)](#page-31-0) and denoising (Fig. [12\\)](#page-32-0), with more examples on colorization (Fig. [8\\)](#page-28-0), super-resolution (Fig. [9\\)](#page-29-0) and stroke-guided image generation (Fig. [13\\)](#page-33-0).\n\n## <span id=\"page-8-2\"></span>7. Conclusion\n\nWe have introduced consistency models, a type of generative models that are specifically designed to support one-step and few-step generation. We have empirically demonstrated that our consistency distillation method outshines the existing distillation techniques for diffusion models on multiple image benchmarks and small sampling iterations. Furthermore, as a standalone generative model, consistency models generate better samples than existing single-step generation models except for GANs. Similar to diffusion models, they also allow zero-shot image editing applications such as inpainting, colorization, super-resolution, denoising, interpolation, and stroke-guided image generation.\n\nIn addition, consistency models share striking similarities with techniques employed in other fields, including deep Q-learning [\\(Mnih et al.,](#page-10-16) [2015\\)](#page-10-16) and momentum-based contrastive learning [\\(Grill et al.,](#page-9-15) [2020;](#page-9-15) [He et al.,](#page-9-16) [2020\\)](#page-9-16). This offers exciting prospects for cross-pollination of ideas and methods among these diverse fields.\n\n## Acknowledgements\n\nWe thank Alex Nichol for reviewing the manuscript and providing valuable feedback, Chenlin Meng for providing stroke inputs needed in our stroke-guided image generation experiments, and the OpenAI Algorithms team.\n\n## References\n\n- <span id=\"page-9-14\"></span>Balaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Kreis, K., Aittala, M., Aila, T., Laine, S., Catanzaro, B., Karras, T., and Liu, M.-Y. ediff-i: Text-to-image diffusion models with ensemble of expert denoisers. *arXiv preprint arXiv:2211.01324*, 2022.\n- <span id=\"page-9-12\"></span>Bilos, M., Sommer, J., Rangapuram, S. S., Januschowski, T., ˇ and Gunnemann, S. Neural flows: Efficient alternative to ¨ neural odes. *Advances in Neural Information Processing Systems*, 34:21325–21337, 2021.\n- <span id=\"page-9-18\"></span>Brock, A., Donahue, J., and Simonyan, K. Large scale GAN training for high fidelity natural image synthesis. In *International Conference on Learning Representations*, 2019. URL [https://openreview.net/forum?](https://openreview.net/forum?id=B1xsqj09Fm) [id=B1xsqj09Fm](https://openreview.net/forum?id=B1xsqj09Fm).\n- <span id=\"page-9-2\"></span>Chen, N., Zhang, Y., Zen, H., Weiss, R. J., Norouzi, M., and Chan, W. Wavegrad: Estimating gradients for waveform generation. In *International Conference on Learning Representations (ICLR)*, 2021.\n- <span id=\"page-9-13\"></span>Chen, R. T., Rubanova, Y., Bettencourt, J., and Duvenaud, D. K. Neural Ordinary Differential Equations. In *Advances in neural information processing systems*, pp. 6571–6583, 2018.\n- <span id=\"page-9-20\"></span>Chen, R. T., Behrmann, J., Duvenaud, D. K., and Jacobsen, J.-H. Residual flows for invertible generative modeling. In *Advances in Neural Information Processing Systems*, pp. 9916–9926, 2019.\n- <span id=\"page-9-5\"></span>Chung, H., Kim, J., Mccann, M. T., Klasky, M. L., and Ye, J. C. Diffusion posterior sampling for general noisy inverse problems. In *International Conference on Learning Representations*, 2023. URL [https://openreview.](https://openreview.net/forum?id=OnD9zGAGT0k) [net/forum?id=OnD9zGAGT0k](https://openreview.net/forum?id=OnD9zGAGT0k).\n- <span id=\"page-9-9\"></span>Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In *2009 IEEE conference on computer vision and pattern recognition*, pp. 248–255. Ieee, 2009.\n- <span id=\"page-9-1\"></span>Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. *Advances in Neural Information Processing Systems (NeurIPS)*, 2021.\n- <span id=\"page-9-7\"></span>Dinh, L., Krueger, D., and Bengio, Y. NICE: Non-linear independent components estimation. *International Conference in Learning Representations Workshop Track*, 2015.\n- <span id=\"page-9-8\"></span>Dinh, L., Sohl-Dickstein, J., and Bengio, S. Density estimation using real NVP. In *5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings*.\n\n- OpenReview.net, 2017. URL [https://openreview.](https://openreview.net/forum?id=HkpbnH9lx) [net/forum?id=HkpbnH9lx](https://openreview.net/forum?id=HkpbnH9lx).\n- <span id=\"page-9-11\"></span>Dockhorn, T., Vahdat, A., and Kreis, K. Genie: Higherorder denoising diffusion solvers. *arXiv preprint arXiv:2210.05475*, 2022.\n- <span id=\"page-9-19\"></span>Gong, X., Chang, S., Jiang, Y., and Wang, Z. Autogan: Neural architecture search for generative adversarial networks. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pp. 3224–3234, 2019.\n- <span id=\"page-9-6\"></span>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. In *Advances in neural information processing systems*, pp. 2672–2680, 2014.\n- <span id=\"page-9-21\"></span>Grcic, M., Grubi ´ siˇ c, I., and ´ Segvi ˇ c, S. Densely connected ´ normalizing flows. *Advances in Neural Information Processing Systems*, 34:23968–23982, 2021.\n- <span id=\"page-9-15\"></span>Grill, J.-B., Strub, F., Altche, F., Tallec, C., Richemond, P., ´ Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., et al. Bootstrap your own latent-a new approach to self-supervised learning. *Advances in neural information processing systems*, 33:21271–21284, 2020.\n- <span id=\"page-9-16\"></span>He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Momentum contrast for unsupervised visual representation learning. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pp. 9729–9738, 2020.\n- <span id=\"page-9-17\"></span>Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In *Advances in Neural Information Processing Systems*, pp. 6626–6637, 2017.\n- <span id=\"page-9-0\"></span>Ho, J., Jain, A., and Abbeel, P. Denoising Diffusion Probabilistic Models. *Advances in Neural Information Processing Systems*, 33, 2020.\n- <span id=\"page-9-4\"></span>Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J., et al. Imagen video: High definition video generation with diffusion models. *arXiv preprint arXiv:2210.02303*, 2022a.\n- <span id=\"page-9-3\"></span>Ho, J., Salimans, T., Gritsenko, A. A., Chan, W., Norouzi, M., and Fleet, D. J. Video diffusion models. In *ICLR Workshop on Deep Generative Models for Highly Structured Data*, 2022b. URL [https://openreview.](https://openreview.net/forum?id=BBelR2NdDZ5) [net/forum?id=BBelR2NdDZ5](https://openreview.net/forum?id=BBelR2NdDZ5).\n- <span id=\"page-9-10\"></span>Hyvarinen, A. and Dayan, P. Estimation of non-normalized ¨ statistical models by score matching. *Journal of Machine Learning Research (JMLR)*, 6(4), 2005.\n\n- <span id=\"page-10-21\"></span>Jiang, Y., Chang, S., and Wang, Z. Transgan: Two pure transformers can make one strong gan, and that can scale up. *Advances in Neural Information Processing Systems*, 34:14745–14758, 2021.\n- <span id=\"page-10-24\"></span>Karras, T., Aila, T., Laine, S., and Lehtinen, J. Progressive growing of GANs for improved quality, stability, and variation. In *International Conference on Learning Representations*, 2018. URL [https://openreview.](https://openreview.net/forum?id=Hk99zCeAb) [net/forum?id=Hk99zCeAb](https://openreview.net/forum?id=Hk99zCeAb).\n- <span id=\"page-10-22\"></span>Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., and Aila, T. Analyzing and improving the image quality of stylegan. 2020.\n- <span id=\"page-10-12\"></span>Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based generative models. In *Proc. NeurIPS*, 2022.\n- <span id=\"page-10-4\"></span>Kawar, B., Vaksman, G., and Elad, M. Snips: Solving noisy inverse problems stochastically. *arXiv preprint arXiv:2105.14951*, 2021.\n- <span id=\"page-10-5\"></span>Kawar, B., Elad, M., Ermon, S., and Song, J. Denoising diffusion restoration models. In *Advances in Neural Information Processing Systems*, 2022.\n- <span id=\"page-10-9\"></span>Kingma, D. P. and Dhariwal, P. Glow: Generative flow with invertible 1x1 convolutions. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), *Advances in Neural Information Processing Systems 31*, pp. 10215–10224. 2018.\n- <span id=\"page-10-7\"></span>Kingma, D. P. and Welling, M. Auto-encoding variational bayes. In *International Conference on Learning Representations*, 2014.\n- <span id=\"page-10-2\"></span>Kong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro, B. DiffWave: A Versatile Diffusion Model for Audio Synthesis. *arXiv preprint arXiv:2009.09761*, 2020.\n- <span id=\"page-10-11\"></span>Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009.\n- <span id=\"page-10-18\"></span>Kynka¨anniemi, T., Karras, T., Laine, S., Lehtinen, J., and ¨ Aila, T. Improved precision and recall metric for assessing generative models. *Advances in Neural Information Processing Systems*, 32, 2019.\n- <span id=\"page-10-20\"></span>Lee, K., Chang, H., Jiang, L., Zhang, H., Tu, Z., and Liu, C. Vitgan: Training gans with vision transformers. *arXiv preprint arXiv:2107.04589*, 2021.\n- <span id=\"page-10-17\"></span>Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. *arXiv preprint arXiv:1509.02971*, 2015.\n\n- <span id=\"page-10-25\"></span>Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and Han, J. On the variance of the adaptive learning rate and beyond. *arXiv preprint arXiv:1908.03265*, 2019.\n- <span id=\"page-10-19\"></span>Liu, X., Gong, C., and Liu, Q. Flow straight and fast: Learning to generate and transfer data with rectified flow. *arXiv preprint arXiv:2209.03003*, 2022.\n- <span id=\"page-10-10\"></span>Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. *arXiv preprint arXiv:2206.00927*, 2022.\n- <span id=\"page-10-13\"></span>Luhman, E. and Luhman, T. Knowledge distillation in iterative generative models for improved sampling speed. *arXiv preprint arXiv:2101.02388*, 2021.\n- <span id=\"page-10-6\"></span>Meng, C., Song, Y., Song, J., Wu, J., Zhu, J.-Y., and Ermon, S. Sdedit: Image synthesis and editing with stochastic differential equations. *arXiv preprint arXiv:2108.01073*, 2021.\n- <span id=\"page-10-14\"></span>Meng, C., Gao, R., Kingma, D. P., Ermon, S., Ho, J., and Salimans, T. On distillation of guided diffusion models. *arXiv preprint arXiv:2210.03142*, 2022.\n- <span id=\"page-10-15\"></span>Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing atari with deep reinforcement learning. *arXiv preprint arXiv:1312.5602*, 2013.\n- <span id=\"page-10-16\"></span>Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement learning. *nature*, 518(7540): 529–533, 2015.\n- <span id=\"page-10-0\"></span>Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., and Chen, M. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. *arXiv preprint arXiv:2112.10741*, 2021.\n- <span id=\"page-10-23\"></span>Parmar, G., Li, D., Lee, K., and Tu, Z. Dual contradistinctive generative autoencoder. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 823–832, 2021.\n- <span id=\"page-10-3\"></span>Popov, V., Vovk, I., Gogoryan, V., Sadekova, T., and Kudinov, M. Grad-TTS: A diffusion probabilistic model for text-to-speech. *arXiv preprint arXiv:2105.06337*, 2021.\n- <span id=\"page-10-1\"></span>Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with clip latents. *arXiv preprint arXiv:2204.06125*, 2022.\n- <span id=\"page-10-8\"></span>Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic backpropagation and approximate inference in deep generative models. In *Proceedings of the 31st International Conference on Machine Learning*, pp. 1278–1286, 2014.\n\n- <span id=\"page-11-5\"></span>Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 10684–10695, 2022.\n- <span id=\"page-11-4\"></span>Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S., Lopes, R. G., et al. Photorealistic text-to-image diffusion models with deep language understanding. *arXiv preprint arXiv:2205.11487*, 2022.\n- <span id=\"page-11-10\"></span>Salimans, T. and Ho, J. Progressive distillation for fast sampling of diffusion models. In *International Conference on Learning Representations*, 2022. URL [https:](https://openreview.net/forum?id=TIdIXIpzhoI) [//openreview.net/forum?id=TIdIXIpzhoI](https://openreview.net/forum?id=TIdIXIpzhoI).\n- <span id=\"page-11-15\"></span>Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen, X. Improved techniques for training gans. In *Advances in neural information processing systems*, pp. 2234–2242, 2016.\n- <span id=\"page-11-18\"></span>Sauer, A., Schwarz, K., and Geiger, A. Stylegan-xl: Scaling stylegan to large diverse datasets. In *ACM SIGGRAPH 2022 conference proceedings*, pp. 1–10, 2022.\n- <span id=\"page-11-0\"></span>Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep Unsupervised Learning Using Nonequilibrium Thermodynamics. In *International Conference on Machine Learning*, pp. 2256–2265, 2015.\n- <span id=\"page-11-13\"></span>Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. *arXiv preprint arXiv:2010.02502*, 2020.\n- <span id=\"page-11-7\"></span>Song, J., Vahdat, A., Mardani, M., and Kautz, J. Pseudoinverse-guided diffusion models for inverse problems. In *International Conference on Learning Representations*, 2023. URL [https://openreview.net/](https://openreview.net/forum?id=9_gsMA8MRKQ) [forum?id=9\\\\_gsMA8MRKQ](https://openreview.net/forum?id=9_gsMA8MRKQ).\n- <span id=\"page-11-1\"></span>Song, Y. and Ermon, S. Generative Modeling by Estimating Gradients of the Data Distribution. In *Advances in Neural Information Processing Systems*, pp. 11918–11930, 2019.\n- <span id=\"page-11-2\"></span>Song, Y. and Ermon, S. Improved Techniques for Training Score-Based Generative Models. *Advances in Neural Information Processing Systems*, 33, 2020.\n- <span id=\"page-11-12\"></span>Song, Y., Garg, S., Shi, J., and Ermon, S. Sliced score matching: A scalable approach to density and score estimation. In *Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 2019, Tel Aviv, Israel, July 22-25, 2019*, pp. 204, 2019.\n- <span id=\"page-11-3\"></span>Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. In *International Conference on Learning Representations*,\n\n- 2021. URL [https://openreview.net/forum?](https://openreview.net/forum?id=PxTIG12RRHS) [id=PxTIG12RRHS](https://openreview.net/forum?id=PxTIG12RRHS).\n- <span id=\"page-11-6\"></span>Song, Y., Shen, L., Xing, L., and Ermon, S. Solving inverse problems in medical imaging with score-based generative models. In *International Conference on Learning Representations*, 2022. URL [https://openreview.](https://openreview.net/forum?id=vaRCHVj0uGI) [net/forum?id=vaRCHVj0uGI](https://openreview.net/forum?id=vaRCHVj0uGI).\n- <span id=\"page-11-14\"></span>Suli, E. and Mayers, D. F. ¨ *An introduction to numerical analysis*. Cambridge university press, 2003.\n- <span id=\"page-11-17\"></span>Tian, Y., Wang, Q., Huang, Z., Li, W., Dai, D., Yang, M., Wang, J., and Fink, O. Off-policy reinforcement learning for efficient and effective gan architecture search. In *Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VII 16*, pp. 175–192. Springer, 2020.\n- <span id=\"page-11-19\"></span>Vahdat, A., Kreis, K., and Kautz, J. Score-based generative modeling in latent space. *Advances in Neural Information Processing Systems*, 34:11287–11302, 2021.\n- <span id=\"page-11-11\"></span>Vincent, P. A Connection Between Score Matching and Denoising Autoencoders. *Neural Computation*, 23(7): 1661–1674, 2011.\n- <span id=\"page-11-22\"></span>Wu, J., Huang, Z., Acharya, D., Li, W., Thoma, J., Paudel, D. P., and Gool, L. V. Sliced wasserstein generative models. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 3713– 3722, 2019.\n- <span id=\"page-11-21\"></span>Xiao, Z., Yan, Q., and Amit, Y. Generative latent flow. *arXiv preprint arXiv:1905.10485*, 2019.\n- <span id=\"page-11-16\"></span>Xiao, Z., Kreis, K., and Vahdat, A. Tackling the generative learning trilemma with denoising diffusion GANs. In *International Conference on Learning Representations*, 2022. URL [https://openreview.net/forum?](https://openreview.net/forum?id=JprM0p-q0Co) [id=JprM0p-q0Co](https://openreview.net/forum?id=JprM0p-q0Co).\n- <span id=\"page-11-20\"></span>Xu, Y., Liu, Z., Tegmark, M., and Jaakkola, T. S. Poisson flow generative models. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), *Advances in Neural Information Processing Systems*, 2022. URL [https:](https://openreview.net/forum?id=voV_TRqcWh) [//openreview.net/forum?id=voV\\\\_TRqcWh](https://openreview.net/forum?id=voV_TRqcWh).\n- <span id=\"page-11-9\"></span>Yu, F., Seff, A., Zhang, Y., Song, S., Funkhouser, T., and Xiao, J. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. *arXiv preprint arXiv:1506.03365*, 2015.\n- <span id=\"page-11-8\"></span>Zhang, Q. and Chen, Y. Fast sampling of diffusion models with exponential integrator. *arXiv preprint arXiv:2204.13902*, 2022.\n\n- <span id=\"page-12-1\"></span>Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, O. The unreasonable effectiveness of deep features as a perceptual metric. In *CVPR*, 2018.\n- <span id=\"page-12-0\"></span>Zheng, H., Nie, W., Vahdat, A., Azizzadenesheli, K., and Anandkumar, A. Fast sampling of diffusion models via operator learning. *arXiv preprint arXiv:2211.13449*, 2022.\n- <span id=\"page-12-2\"></span>Zheng, H., He, P., Chen, W., and Zhou, M. Truncated diffusion probabilistic models and diffusion-based adversarial auto-encoders. In *The Eleventh International Conference on Learning Representations*, 2023. URL [https:](https://openreview.net/forum?id=HDxgaKk956l) [//openreview.net/forum?id=HDxgaKk956l](https://openreview.net/forum?id=HDxgaKk956l).\n\n#### Consistency Models\n\n## Contents\n\n| 1 | Introduction                                                | 1  |\n|---|-------------------------------------------------------------|----|\n| 2 | Diffusion Models                                            | 2  |\n| 3 | Consistency Models                                          | 3  |\n| 4 | Training Consistency Models via Distillation                | 4  |\n| 5 | Training Consistency Models in Isolation                    | 5  |\n| 6 | Experiments                                                 | 6  |\n|   | 6.1<br>Training Consistency Models                          | 6  |\n|   | 6.2<br>Few-Step Image Generation<br>                        | 7  |\n|   | 6.3<br>Zero-Shot Image Editing<br>                          | 9  |\n| 7 | Conclusion                                                  | 9  |\n|   | Appendices                                                  | 15 |\n|   | Appendix A<br>Proofs                                        | 15 |\n|   | A.1<br>Notations<br>                                        | 15 |\n|   | A.2<br>Consistency Distillation                             | 15 |\n|   | A.3<br>Consistency Training<br>                             | 16 |\n|   | Appendix B<br>Continuous-Time Extensions                    | 18 |\n|   | B.1<br>Consistency Distillation in Continuous Time          | 18 |\n|   | B.2<br>Consistency Training in Continuous Time<br>          | 22 |\n|   | B.3<br>Experimental Verifications<br>                       | 24 |\n|   | Appendix C<br>Additional Experimental Details               | 25 |\n|   | Model Architectures<br>                                     | 25 |\n|   | Parameterization for Consistency Models<br>                 | 25 |\n|   | Schedule Functions for Consistency Training<br>             | 26 |\n|   | Training Details                                            | 26 |\n|   | Appendix D<br>Additional Results on Zero-Shot Image Editing | 26 |\n|   | Inpainting                                                  | 27 |\n|   | Colorization<br>                                            | 27 |\n|   | Super-resolution<br>                                        | 28 |\n\n| Appendix E | Additional Samples from Consistency Models | 28 |\n|------------|--------------------------------------------|----|\n|            | Interpolation                              | 28 |\n|            | Denoising                                  | 28 |\n|            | Stroke-guided image generation             | 28 |\n\n# <span id=\"page-14-2\"></span>**Appendices**\n\n#### <span id=\"page-14-1\"></span>A. Proofs\n\n#### <span id=\"page-14-3\"></span>A.1. Notations\n\nWe use  $f_{\\theta}(\\mathbf{x},t)$  to denote a consistency model parameterized by  $\\theta$ , and  $f(\\mathbf{x},t;\\phi)$  the consistency function of the empirical PF ODE in Eq. (3). Here  $\\phi$  symbolizes its dependency on the pre-trained score model  $s_{\\phi}(\\mathbf{x},t)$ . For the consistency function of the PF ODE in Eq. (2), we denote it as  $f(\\mathbf{x},t)$ . Given a multi-variate function  $h(\\mathbf{x},\\mathbf{y})$ , we let  $\\partial_1 h(\\mathbf{x},\\mathbf{y})$  denote the Jacobian of h over  $\\mathbf{x}$ , and analogously  $\\partial_2 h(\\mathbf{x},\\mathbf{y})$  denote the Jacobian of h over h0. Unless otherwise stated, h1 is supposed to be a random variable sampled from the data distribution h2 h3, h4 is sampled uniformly at random from h5. Furthermore, recall that we define\n\n$$\\hat{\\mathbf{x}}_{t_n}^{\\phi} := \\mathbf{x}_{t_{n+1}} + (t_n - t_{n+1}) \\Phi(\\mathbf{x}_{t_{n+1}}, t_{n+1}; \\phi)$$\n\nwhere  $\\Phi(\\dots; \\phi)$  denotes the update function of a one-step ODE solver for the empirical PF ODE defined by the score model  $s_{\\phi}(\\mathbf{x}, t)$ . By default,  $\\mathbb{E}[\\cdot]$  denotes the expectation over all relevant random variables in the expression.\n\n#### <span id=\"page-14-0\"></span>A.2. Consistency Distillation\n\n**Theorem 1.** Let  $\\Delta t := \\max_{n \\in [\\![1,N-1]\\!]} \\{|t_{n+1} - t_n|\\}$ , and  $f(\\cdot,\\cdot;\\phi)$  be the consistency function of the empirical PF ODE in Eq. (3). Assume  $f_{\\boldsymbol{\\theta}}$  satisfies the Lipschitz condition: there exists L > 0 such that for all  $t \\in [\\epsilon,T]$ ,  $\\mathbf{x}$ , and  $\\mathbf{y}$ , we have  $\\|f_{\\boldsymbol{\\theta}}(\\mathbf{x},t) - f_{\\boldsymbol{\\theta}}(\\mathbf{y},t)\\|_2 \\le L \\|\\mathbf{x} - \\mathbf{y}\\|_2$ . Assume further that for all  $n \\in [\\![1,N-1]\\!]$ , the ODE solver called at  $t_{n+1}$  has local error uniformly bounded by  $O((t_{n+1} - t_n)^{p+1})$  with  $p \\ge 1$ . Then, if  $\\mathcal{L}_{CD}^N(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}; \\phi) = 0$ , we have\n\n$$\\sup_{n,\\mathbf{x}} \\|\\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{x},t_n) - \\mathbf{f}(\\mathbf{x},t_n;\\boldsymbol{\\phi})\\|_2 = O((\\Delta t)^p).$$\n\n*Proof.* From  $\\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}; \\boldsymbol{\\phi}) = 0$ , we have\n\n$$\\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}; \\boldsymbol{\\phi}) = \\mathbb{E}[\\lambda(t_n)d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}), \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\hat{\\mathbf{x}}_{t_n}^{\\boldsymbol{\\phi}}, t_n))] = 0.$$\n(11)\n\nAccording to the definition, we have  $p_{t_n}(\\mathbf{x}_{t_n}) = p_{\\text{data}}(\\mathbf{x}) \\otimes \\mathcal{N}(\\mathbf{0}, t_n^2 \\mathbf{I})$  where  $t_n \\ge \\epsilon > 0$ . It follows that  $p_{t_n}(\\mathbf{x}_{t_n}) > 0$  for every  $\\mathbf{x}_{t_n}$  and  $1 \\le n \\le N$ . Therefore, Eq. (11) entails\n\n$$\\lambda(t_n)d(\\mathbf{f}_{\\theta}(\\mathbf{x}_{t_{n+1}}, t_{n+1}), \\mathbf{f}_{\\theta}(\\hat{\\mathbf{x}}_{t_n}^{\\phi}, t_n)) \\equiv 0.$$\n(12)\n\nBecause  $\\lambda(\\cdot) > 0$  and  $d(\\mathbf{x}, \\mathbf{y}) = 0 \\Leftrightarrow \\mathbf{x} = \\mathbf{y}$ , this further implies that\n\n<span id=\"page-14-5\"></span><span id=\"page-14-4\"></span>\n$$\\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}) \\equiv \\mathbf{f}_{\\boldsymbol{\\theta}}(\\hat{\\mathbf{x}}_{t_n}^{\\boldsymbol{\\phi}}, t_n). \\tag{13}$$\n\nNow let  $e_n$  represent the error vector at  $t_n$ , which is defined as\n\n$$e_n := f_{\\theta}(\\mathbf{x}_{t_n}, t_n) - f(\\mathbf{x}_{t_n}, t_n; \\phi).$$\n\nWe can easily derive the following recursion relation\n\n$$e_{n+1} = f_{\\theta}(\\mathbf{x}_{t_{n+1}}, t_{n+1}) - f(\\mathbf{x}_{t_{n+1}}, t_{n+1}; \\phi)$$\n\n$$\\stackrel{(i)}{=} \\mathbf{f}_{\\theta}(\\hat{\\mathbf{x}}_{t_{n}}^{\\phi}, t_{n}) - \\mathbf{f}(\\mathbf{x}_{t_{n}}, t_{n}; \\phi) \n= \\mathbf{f}_{\\theta}(\\hat{\\mathbf{x}}_{t_{n}}^{\\phi}, t_{n}) - \\mathbf{f}_{\\theta}(\\mathbf{x}_{t_{n}}, t_{n}) + \\mathbf{f}_{\\theta}(\\mathbf{x}_{t_{n}}, t_{n}) - \\mathbf{f}(\\mathbf{x}_{t_{n}}, t_{n}; \\phi) \n= \\mathbf{f}_{\\theta}(\\hat{\\mathbf{x}}_{t_{n}}^{\\phi}, t_{n}) - \\mathbf{f}_{\\theta}(\\mathbf{x}_{t_{n}}, t_{n}) + \\mathbf{e}_{n},$$\n(14)\n\nwhere (i) is due to Eq. (13) and  $f(\\mathbf{x}_{t_{n+1}}, t_{n+1}; \\phi) = f(\\mathbf{x}_{t_n}, t_n; \\phi)$ . Because  $f_{\\theta}(\\cdot, t_n)$  has Lipschitz constant L, we have\n\n<span id=\"page-15-2\"></span>\n$$\\begin{aligned} \\|\\boldsymbol{e}_{n+1}\\|_{2} &\\leq \\|\\boldsymbol{e}_{n}\\|_{2} + L \\|\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}} - \\mathbf{x}_{t_{n}}\\|_{2} \\\\ &\\stackrel{(i)}{=} \\|\\boldsymbol{e}_{n}\\|_{2} + L \\cdot O((t_{n+1} - t_{n})^{p+1}) \\\\ &= \\|\\boldsymbol{e}_{n}\\|_{2} + O((t_{n+1} - t_{n})^{p+1}), \\end{aligned}$$\n\nwhere (i) holds because the ODE solver has local error bounded by  $O((t_{n+1} - t_n)^{p+1})$ . In addition, we observe that  $e_1 = \\mathbf{0}$ , because\n\n$$egin{aligned} oldsymbol{e}_1 &= oldsymbol{f}_{oldsymbol{ heta}}(\\mathbf{x}_{t_1}, t_1) - oldsymbol{f}(\\mathbf{x}_{t_1}, t_1; oldsymbol{\\phi}) \\ &\\stackrel{(ii)}{=} \\mathbf{x}_{t_1} - oldsymbol{f}(\\mathbf{x}_{t_1}, t_1; oldsymbol{\\phi}) \\ &\\stackrel{(iii)}{=} \\mathbf{x}_{t_1} - \\mathbf{x}_{t_1} \\ &= oldsymbol{0} \\end{aligned}$$\n\nHere (i) is true because the consistency model is parameterized such that  $f(\\mathbf{x}_{t_1}, t_1; \\phi) = \\mathbf{x}_{t_1}$  and (ii) is entailed by the definition of  $f(\\cdot, \\cdot; \\phi)$ . This allows us to perform induction on the recursion formula Eq. (14) to obtain\n\n$$\\|\\boldsymbol{e}_{n}\\|_{2} \\leq \\|\\boldsymbol{e}_{1}\\|_{2} + \\sum_{k=1}^{n-1} O((t_{k+1} - t_{k})^{p+1})$$\n\n$$= \\sum_{k=1}^{n-1} O((t_{k+1} - t_{k})^{p+1})$$\n\n$$= \\sum_{k=1}^{n-1} (t_{k+1} - t_{k}) O((t_{k+1} - t_{k})^{p})$$\n\n$$\\leq \\sum_{k=1}^{n-1} (t_{k+1} - t_{k}) O((\\Delta t)^{p})$$\n\n$$= O((\\Delta t)^{p}) \\sum_{k=1}^{n-1} (t_{k+1} - t_{k})$$\n\n$$= O((\\Delta t)^{p}) (t_{n} - t_{1})$$\n\n$$\\leq O((\\Delta t)^{p}) (T - \\epsilon)$$\n\n$$= O((\\Delta t)^{p}),$$\n\nwhich completes the proof.\n\n#### <span id=\"page-15-1\"></span>A.3. Consistency Training\n\nThe following lemma provides an unbiased estimator for the score function, which is crucial to our proof for Theorem 2.\n\n<span id=\"page-15-0\"></span>**Lemma 1.** Let \n$$\\mathbf{x} \\sim p_{data}(\\mathbf{x})$$\n,  $\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{x}; t^2 \\mathbf{I})$ , and  $p_t(\\mathbf{x}_t) = p_{data}(\\mathbf{x}) \\otimes \\mathcal{N}(\\mathbf{0}, t^2 \\mathbf{I})$ . We have  $\\nabla \\log p_t(\\mathbf{x}) = -\\mathbb{E}[\\frac{\\mathbf{x}_t - \\mathbf{x}}{t^2} \\mid \\mathbf{x}_t]$ .\n\n*Proof.* According to the definition of  $p_t(\\mathbf{x}_t)$ , we have  $\\nabla \\log p_t(\\mathbf{x}_t) = \\nabla_{\\mathbf{x}_t} \\log \\int p_{\\text{data}}(\\mathbf{x}) p(\\mathbf{x}_t \\mid \\mathbf{x}) \\, d\\mathbf{x}$ , where  $p(\\mathbf{x}_t \\mid \\mathbf{x}) = \\mathcal{N}(\\mathbf{x}_t; \\mathbf{x}, t^2 \\mathbf{I})$ . This expression can be further simplified to yield\n\n$$\\nabla \\log p_t(\\mathbf{x}_t) = \\frac{\\int p_{\\text{data}}(\\mathbf{x}) \\nabla_{\\mathbf{x}_t} p(\\mathbf{x}_t \\mid \\mathbf{x}) \\, d\\mathbf{x}}{\\int p_{\\text{data}}(\\mathbf{x}) p(\\mathbf{x}_t \\mid \\mathbf{x}) \\, d\\mathbf{x}}$$\n\n$$\\begin{split} &= \\frac{\\int p_{\\text{data}}(\\mathbf{x}) p(\\mathbf{x}_t \\mid \\mathbf{x}) \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t \\mid \\mathbf{x}) \\, d\\mathbf{x}}{\\int p_{\\text{data}}(\\mathbf{x}) p(\\mathbf{x}_t \\mid \\mathbf{x}) \\, d\\mathbf{x}} \\\\ &= \\frac{\\int p_{\\text{data}}(\\mathbf{x}) p(\\mathbf{x}_t \\mid \\mathbf{x}) \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t \\mid \\mathbf{x}) \\, d\\mathbf{x}}{p_t(\\mathbf{x}_t)} \\\\ &= \\int \\frac{p_{\\text{data}}(\\mathbf{x}) p(\\mathbf{x}_t \\mid \\mathbf{x})}{p_t(\\mathbf{x}_t)} \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t \\mid \\mathbf{x}) \\, d\\mathbf{x} \\\\ &\\stackrel{(i)}{=} \\int p(\\mathbf{x} \\mid \\mathbf{x}_t) \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t \\mid \\mathbf{x}) \\, d\\mathbf{x} \\\\ &= \\mathbb{E}[\\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t \\mid \\mathbf{x}) \\mid \\mathbf{x}_t] \\\\ &= -\\mathbb{E}\\left[\\frac{\\mathbf{x}_t - \\mathbf{x}}{t^2} \\mid \\mathbf{x}_t\\right], \\end{split}$$\n\n<span id=\"page-16-0\"></span>\n\nwhere (i) is due to Bayes' rule.\n\n**Theorem 2.** Let  $\\Delta t := \\max_{n \\in [\\![1,N-1]\\!]} \\{|t_{n+1} - t_n|\\}$ . Assume d and  $f_{\\theta^-}$  are both twice continuously differentiable with bounded second derivatives, the weighting function  $\\lambda(\\cdot)$  is bounded, and  $\\mathbb{E}[\\|\\nabla \\log p_{t_n}(\\mathbf{x}_{t_n})\\|_2^2] < \\infty$ . Assume further that we use the Euler ODE solver, and the pre-trained score model matches the ground truth, i.e.,  $\\forall t \\in [\\epsilon, T] : s_{\\phi}(\\mathbf{x}, t) \\equiv \\nabla \\log p_t(\\mathbf{x})$ . Then,\n\n$$\\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}; \\boldsymbol{\\phi}) = \\mathcal{L}_{CT}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}) + o(\\Delta t),$$\n\nwhere the expectation is taken with respect to  $\\mathbf{x} \\sim p_{data}$ ,  $n \\sim \\mathcal{U}[1, N-1]$ , and  $\\mathbf{x}_{t_{n+1}} \\sim \\mathcal{N}(\\mathbf{x}; t_{n+1}^2 \\mathbf{I})$ . The consistency training objective, denoted by  $\\mathcal{L}_{CT}^N(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^-)$ , is defined as\n\n$$\\mathbb{E}[\\lambda(t_n)d(\\mathbf{f}_{\\theta}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1}),\\mathbf{f}_{\\theta^-}(\\mathbf{x}+t_n\\mathbf{z},t_n))],$$\n\nwhere  $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ . Moreover,  $\\mathcal{L}_{CT}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}) \\geqslant O(\\Delta t)$  if  $\\inf_{N} \\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}; \\boldsymbol{\\phi}) > 0$ .\n\n*Proof.* With Taylor expansion, we have\n\n$$\\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}; \\boldsymbol{\\phi}) = \\mathbb{E}[\\lambda(t_{n})d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}), \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}}, t_{n})] \\\\\n= \\mathbb{E}[\\lambda(t_{n})d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}), \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}} + (t_{n+1} - t_{n})t_{n+1}\\nabla \\log p_{t_{n+1}}(\\mathbf{x}_{t_{n+1}}), t_{n}))] \\\\\n= \\mathbb{E}[\\lambda(t_{n})d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}), \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}) + \\partial_{1}\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}}, t_{n+1})(t_{n+1} - t_{n})t_{n+1}\\nabla \\log p_{t_{n+1}}(\\mathbf{x}_{t_{n+1}}) \\\\\n+ \\partial_{2}\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}}, t_{n+1})(t_{n} - t_{n+1}) + o(|t_{n+1} - t_{n}|)] \\\\\n= \\mathbb{E}\\{\\lambda(t_{n})d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}), \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}}, t_{n+1})) + \\lambda(t_{n})\\partial_{2}d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}), \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}))[\\\\\n\\partial_{1}\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}}, t_{n+1})(t_{n+1} - t_{n})t_{n+1}\\nabla \\log p_{t_{n+1}}(\\mathbf{x}_{t_{n+1}}) + \\partial_{2}\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}}, t_{n+1})(t_{n} - t_{n+1}) + o(|t_{n+1} - t_{n}|)]\\} \\\\\n= \\mathbb{E}[\\lambda(t_{n})d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}), \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}))] \\\\\n+ \\mathbb{E}\\{\\lambda(t_{n})\\partial_{2}d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}), \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}))[\\partial_{1}\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}}, t_{n+1})(t_{n} - t_{n+1})]\\} + \\mathbb{E}[o(|t_{n+1} - t_{n}|)]. \\\\\n(15)$$\n\nThen, we apply Lemma 1 to Eq. (15) and use Taylor expansion in the reverse direction to obtain\n\n$$\\begin{split} &\\mathcal{L}_{\\mathrm{CD}}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{-};\\boldsymbol{\\phi}) \\\\ =& \\mathbb{E}\\big[\\lambda(t_{n})d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1}))\\big] \\\\ &+ \\mathbb{E}\\left\\{\\lambda(t_{n})\\partial_{2}d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1}))\\left[\\partial_{1}\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1})(t_{n}-t_{n+1})t_{n+1}\\mathbb{E}\\left[\\frac{\\mathbf{x}_{t_{n+1}}-\\mathbf{x}}{t_{n+1}^{2}}\\Big|\\mathbf{x}_{t_{n+1}}\\right]\\right]\\right\\} \\\\ &+ \\mathbb{E}\\{\\lambda(t_{n})\\partial_{2}d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1}))[\\partial_{2}\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1})(t_{n}-t_{n+1})]\\} + \\mathbb{E}[o(|t_{n+1}-t_{n}|)] \\\\ \\stackrel{(i)}{=} \\mathbb{E}[\\lambda(t_{n})d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1}))] \\\\ &+ \\mathbb{E}\\left\\{\\lambda(t_{n})\\partial_{2}d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1}))\\left[\\partial_{1}\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1})(t_{n}-t_{n+1})t_{n+1}\\left(\\frac{\\mathbf{x}_{t_{n+1}}-\\mathbf{x}}{t_{n+1}^{2}}\\right)\\right]\\right\\} \\end{split}$$\n\n$$+ \\mathbb{E}\\{\\lambda(t_{n})\\partial_{2}d(\\mathbf{f}_{\\theta}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\mathbf{f}_{\\theta^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1}))[\\partial_{2}\\mathbf{f}_{\\theta^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1})(t_{n}-t_{n+1})]\\} + \\mathbb{E}[o(|t_{n+1}-t_{n}|)]$$\n\n$$= \\mathbb{E}\\left[\\lambda(t_{n})d(\\mathbf{f}_{\\theta}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\mathbf{f}_{\\theta^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1}))\\right]$$\n\n$$+ \\lambda(t_{n})\\partial_{2}d(\\mathbf{f}_{\\theta}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\mathbf{f}_{\\theta^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1}))\\left[\\partial_{2}\\mathbf{f}_{\\theta^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1})(t_{n}-t_{n+1})t_{n+1}\\left(\\frac{\\mathbf{x}_{t_{n+1}}-\\mathbf{x}}{t_{n+1}^{2}}\\right)\\right]$$\n\n$$+ \\lambda(t_{n})\\partial_{2}d(\\mathbf{f}_{\\theta}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\mathbf{f}_{\\theta^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1}))\\left[\\partial_{2}\\mathbf{f}_{\\theta^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1})(t_{n}-t_{n+1})t_{n+1}\\left(\\frac{\\mathbf{x}_{t_{n+1}}-\\mathbf{x}}{t_{n+1}^{2}}\\right)\\right]$$\n\n$$+ \\mathbb{E}[o(|t_{n+1}-t_{n}|)]$$\n\n$$= \\mathbb{E}\\left[\\lambda(t_{n})d\\left(\\mathbf{f}_{\\theta}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\mathbf{f}_{\\theta^{-}}\\left(\\mathbf{x}_{t_{n+1}}+(t_{n}-t_{n+1})t_{n+1}\\frac{\\mathbf{x}_{t_{n+1}}-\\mathbf{x}}{t_{n+1}^{2}},t_{n}\\right)\\right)\\right] + \\mathbb{E}[o(|t_{n+1}-t_{n}|)]$$\n\n$$= \\mathbb{E}\\left[\\lambda(t_{n})d\\left(\\mathbf{f}_{\\theta}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\mathbf{f}_{\\theta^{-}}\\left(\\mathbf{x}_{t_{n+1}}+(t_{n}-t_{n+1})\\frac{\\mathbf{x}_{t_{n+1}}-\\mathbf{x}}{t_{n+1}},t_{n}\\right)\\right)\\right] + \\mathbb{E}[o(|t_{n+1}-t_{n}|)]$$\n\n$$= \\mathbb{E}\\left[\\lambda(t_{n})d\\left(\\mathbf{f}_{\\theta}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1}),\\mathbf{f}_{\\theta^{-}}\\left(\\mathbf{x}+t_{n+1}\\mathbf{z}+(t_{n}-t_{n+1})\\mathbf{z},t_{n}\\right)\\right)\\right] + \\mathbb{E}[o(|t_{n+1}-t_{n}|)]$$\n\n$$= \\mathbb{E}\\left[\\lambda(t_{n})d\\left(\\mathbf{f}_{\\theta}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1}),\\mathbf{f}_{\\theta^{-}}\\left(\\mathbf{x}+t_{n}\\mathbf{z},t_{n}\\right)\\right)\\right] + \\mathbb{E}[o(|t_{n+1}-t_{n}|)]$$\n\n$$= \\mathbb{E}\\left[\\lambda(t_{n})d\\left(\\mathbf{f}_{\\theta}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1}),\\mathbf{f}_{\\theta^{-}}\\left(\\mathbf{x}+t_{n}\\mathbf{z},t_{n}\\right)\\right)\\right] + \\mathbb{E}[o(|t_{n+1}-t_{n}|)]$$\n\n$$= \\mathbb{E}\\left[\\lambda(t_{n})d\\left(\\mathbf{f}_{\\theta}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1}),\\mathbf{f}_{\\theta^{-}}\\left(\\mathbf{x}+t_{n}\\mathbf{z},t_{n}\\right)\\right)\\right] + \\mathbb{E}[o(|t_{n+1}-t_{n}|)]$$\n\n$$= \\mathbb{E}\\left[\\lambda(t_{n})d\\left(\\mathbf{f}_{\\theta}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1}),\\mathbf{f}_{\\theta^{-}}\\left(\\mathbf{x}+t_{n}\\mathbf{z},t_{n}\\right)\\right)\\right] + \\mathbb{E}[o(|t_{n+1}-t_{n}|)]$$\n\nwhere (i) is due to the law of total expectation, and  $\\mathbf{z} := \\frac{\\mathbf{x}_{t_{n+1}} - \\mathbf{x}}{t_{n+1}} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ . This implies  $\\mathcal{L}_{\\mathrm{CD}}^N(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^-; \\boldsymbol{\\phi}) = \\mathcal{L}_{\\mathrm{CT}}^N(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^-) + o(\\Delta t)$  and thus completes the proof for Eq. (9). Moreover, we have  $\\mathcal{L}_{\\mathrm{CT}}^N(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^-) \\geqslant O(\\Delta t)$  whenever  $\\inf_N \\mathcal{L}_{\\mathrm{CD}}^N(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^-; \\boldsymbol{\\phi}) > 0$ . Otherwise,  $\\mathcal{L}_{\\mathrm{CT}}^N(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^-) < O(\\Delta t)$  and thus  $\\lim_{\\Delta t \\to 0} \\mathcal{L}_{\\mathrm{CD}}^N(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^-; \\boldsymbol{\\phi}) = 0$ , which is a clear contradiction to  $\\inf_N \\mathcal{L}_{\\mathrm{CD}}^N(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^-; \\boldsymbol{\\phi}) > 0$ .\n\n**Remark 1.** When the condition  $\\mathcal{L}_{CT}^N(\\theta, \\theta^-) \\ge O(\\Delta t)$  is not satisfied, such as in the case where  $\\theta^- = \\text{stopgrad}(\\theta)$ , the validity of  $\\mathcal{L}_{CT}^N(\\theta, \\theta^-)$  as a training objective for consistency models can still be justified by referencing the result provided in Theorem 6.\n\n#### <span id=\"page-17-2\"></span>**B.** Continuous-Time Extensions\n\nThe consistency distillation and consistency training objectives can be generalized to hold for infinite time steps  $(N \\to \\infty)$  under suitable conditions.\n\n#### <span id=\"page-17-0\"></span>**B.1. Consistency Distillation in Continuous Time**\n\nDepending on whether  $\\theta^- = \\theta$  or  $\\theta^- = \\operatorname{stopgrad}(\\theta)$  (same as setting  $\\mu = 0$ ), there are two possible continuous-time extensions for the consistency distillation objective  $\\mathcal{L}_{\\mathrm{CD}}^N(\\theta, \\theta^-; \\phi)$ . Given a twice continuously differentiable metric function  $d(\\mathbf{x}, \\mathbf{y})$ , we define  $G(\\mathbf{x})$  as a matrix, whose (i, j)-th entry is given by\n\n$$[G(\\mathbf{x})]_{ij} \\coloneqq \\frac{\\partial^2 d(\\mathbf{x}, \\mathbf{y})}{\\partial y_i \\partial y_j} \\bigg|_{\\mathbf{y} = \\mathbf{x}}.$$\n\nSimilarly, we define H(x) as\n\n$$[\\boldsymbol{H}(\\mathbf{x})]_{ij} \\coloneqq \\frac{\\partial^2 d(\\mathbf{y}, \\mathbf{x})}{\\partial y_i \\partial y_j} \\bigg|_{\\mathbf{y} = \\mathbf{x}}.$$\n\nThe matrices G and H play a crucial role in forming continuous-time objectives for consistency distillation. Additionally, we denote the Jacobian of  $f_{\\theta}(\\mathbf{x},t)$  with respect to  $\\mathbf{x}$  as  $\\frac{\\partial f_{\\theta}(\\mathbf{x},t)}{\\partial \\mathbf{x}}$ .\n\nWhen  $\\theta^- = \\theta$  (with no stopgrad operator), we have the following theoretical result.\n\n<span id=\"page-17-1\"></span>**Theorem 3.** Let  $t_n = \\tau(\\frac{n-1}{N-1})$ , where  $n \\in [1, N]$ , and  $\\tau(\\cdot)$  is a strictly monotonic function with  $\\tau(0) = \\epsilon$  and  $\\tau(1) = T$ . Assume  $\\tau$  is continuously differentiable in [0, 1], d is three times continuously differentiable with bounded third derivatives,\n\nand  $f_{\\theta}$  is twice continuously differentiable with bounded first and second derivatives. Assume further that the weighting function  $\\lambda(\\cdot)$  is bounded, and  $\\sup_{\\mathbf{x},t\\in[\\epsilon,T]}\\|\\mathbf{s}_{\\phi}(\\mathbf{x},t)\\|_2 < \\infty$ . Then with the Euler solver in consistency distillation, we have\n\n<span id=\"page-18-2\"></span><span id=\"page-18-0\"></span>\n$$\\lim_{N \\to \\infty} (N-1)^2 \\mathcal{L}_{CD}^N(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}; \\boldsymbol{\\phi}) = \\mathcal{L}_{CD}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}; \\boldsymbol{\\phi}), \\tag{17}$$\n\nwhere  $\\mathcal{L}_{CD}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}; \\boldsymbol{\\phi})$  is defined as\n\n$$\\frac{1}{2}\\mathbb{E}\\left[\\frac{\\lambda(t)}{[(\\tau^{-1})'(t)]^2}\\left(\\frac{\\partial \\boldsymbol{f_{\\theta}}(\\mathbf{x}_t,t)}{\\partial t} - t\\frac{\\partial \\boldsymbol{f_{\\theta}}(\\mathbf{x}_t,t)}{\\partial \\mathbf{x}_t}\\boldsymbol{s_{\\phi}}(\\mathbf{x}_t,t)\\right)^{\\mathsf{T}}\\boldsymbol{G}(\\boldsymbol{f_{\\theta}}(\\mathbf{x}_t,t))\\left(\\frac{\\partial \\boldsymbol{f_{\\theta}}(\\mathbf{x}_t,t)}{\\partial t} - t\\frac{\\partial \\boldsymbol{f_{\\theta}}(\\mathbf{x}_t,t)}{\\partial \\mathbf{x}_t}\\boldsymbol{s_{\\phi}}(\\mathbf{x}_t,t)\\right)\\right].$$\n(18)\n\nHere the expectation above is taken over  $\\mathbf{x} \\sim p_{\\text{data}}$ ,  $u \\sim \\mathcal{U}[0, 1]$ ,  $t = \\tau(u)$ , and  $\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{x}, t^2 \\mathbf{I})$ .\n\n*Proof.* Let  $\\Delta u = \\frac{1}{N-1}$  and  $u_n = \\frac{n-1}{N-1}$ . First, we can derive the following equation with Taylor expansion:\n\n$$f_{\\theta}(\\hat{\\mathbf{x}}_{t_{n}}^{\\phi}, t_{n}) - f_{\\theta}(\\mathbf{x}_{t_{n+1}}, t_{n+1}) = f_{\\theta}(\\mathbf{x}_{t_{n+1}} + t_{n+1}s_{\\phi}(\\mathbf{x}_{t_{n+1}}, t_{n+1})\\tau'(u_{n})\\Delta u, t_{n}) - f_{\\theta}(\\mathbf{x}_{t_{n+1}}, t_{n+1})$$\n\n$$= t_{n+1} \\frac{\\partial f_{\\theta}(\\mathbf{x}_{t_{n+1}}, t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}} s_{\\phi}(\\mathbf{x}_{t_{n+1}}, t_{n+1})\\tau'(u_{n})\\Delta u - \\frac{\\partial f_{\\theta}(\\mathbf{x}_{t_{n+1}}, t_{n+1})}{\\partial t_{n+1}}\\tau'(u_{n})\\Delta u + O((\\Delta u)^{2}), \\tag{19}$$\n\nNote that  $\\tau'(u_n) = \\frac{1}{\\tau^{-1}(t_{n+1})}$ . Then, we apply Taylor expansion to the consistency distillation loss, which gives\n\n$$(N-1)^{2}\\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta};\\boldsymbol{\\phi}) = \\frac{1}{(\\Delta u)^{2}}\\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta};\\boldsymbol{\\phi}) = \\frac{1}{(\\Delta u)^{2}}\\mathbb{E}[\\lambda(t_{n})d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n})]$$\n\n$$\\stackrel{(i)}{=} \\frac{1}{2(\\Delta u)^{2}} \\left( \\mathbb{E}\\{\\lambda(t_{n})\\tau'(u_{n})^{2}[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})]^{\\mathsf{T}}\\boldsymbol{G}(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})) \\right)$$\n\n$$\\cdot [\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})] + \\mathbb{E}[O(|\\Delta u|^{3})] \\right)$$\n\n$$\\stackrel{(ii)}{=} \\frac{1}{2}\\mathbb{E}\\left[\\lambda(t_{n})\\tau'(u_{n})^{2}\\left(\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial t_{n+1}} - t_{n+1}\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}}\\boldsymbol{s}_{\\boldsymbol{\\phi}}(\\mathbf{x}_{t_{n+1}},t_{n+1})\\right)^{\\mathsf{T}}\\boldsymbol{G}(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}))\\right]$$\n\n$$\\cdot \\left(\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial t_{n+1}} - t_{n+1}\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}}\\boldsymbol{s}_{\\boldsymbol{\\phi}}(\\mathbf{x}_{t_{n+1}},t_{n+1})\\right)^{\\mathsf{T}}\\boldsymbol{G}(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}))\\right]$$\n\n$$\\cdot \\left(\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial t_{n+1}} - t_{n+1}\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}}\\boldsymbol{s}_{\\boldsymbol{\\phi}}(\\mathbf{x}_{t_{n+1}},t_{n+1})\\right)^{\\mathsf{T}}\\boldsymbol{G}(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})\\right)$$\n\n$$\\cdot \\left(\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial t_{n+1}} - t_{n+1}\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}}\\boldsymbol{s}_{\\boldsymbol{\\phi}}(\\mathbf{x}_{t_{n+1}},t_{n+1})\\right)^{\\mathsf{T}}\\boldsymbol{G}(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})\\right)$$\n\n$$\\cdot \\left(\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial t_{n+1}} - t_{n+1}\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}}\\boldsymbol{s}_{\\boldsymbol{\\phi}}(\\mathbf{x}_{t_{n+1}},t_{n+1})\\right)^{\\mathsf{T}}\\boldsymbol{G}(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})\\right)$$\n\nwhere we obtain (i) by expanding  $d(\\mathbf{f}_{\\theta}(\\mathbf{x}_{t_{n+1}}, t_{n+1}), \\cdot)$  to second order and observing  $d(\\mathbf{x}, \\mathbf{x}) \\equiv 0$  and  $\\nabla_{\\mathbf{y}} d(\\mathbf{x}, \\mathbf{y})|_{\\mathbf{y} = \\mathbf{x}} \\equiv \\mathbf{0}$ . We obtain (ii) using Eq. (19). By taking the limit for both sides of Eq. (20) as  $\\Delta u \\to 0$  or equivalently  $N \\to \\infty$ , we arrive at Eq. (17), which completes the proof.\n\n**Remark 2.** Although Theorem 3 assumes the Euler ODE solver for technical simplicity, we believe an analogous result can be derived for more general solvers, since all ODE solvers should perform similarly as  $N \\to \\infty$ . We leave a more general version of Theorem 3 as future work.\n\n**Remark 3.** Theorem 3 implies that consistency models can be trained by minimizing  $\\mathcal{L}_{CD}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}; \\boldsymbol{\\phi})$ . In particular, when  $d(\\mathbf{x}, \\mathbf{y}) = \\|\\mathbf{x} - \\mathbf{y}\\|_2^2$ , we have\n\n<span id=\"page-18-1\"></span>\n$$\\mathcal{L}_{CD}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}; \\boldsymbol{\\phi}) = \\mathbb{E}\\left[\\frac{\\lambda(t)}{[(\\tau^{-1})'(t)]^2} \\left\\| \\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_t, t)}{\\partial t} - t \\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_t, t)}{\\partial \\mathbf{x}_t} \\boldsymbol{s}_{\\boldsymbol{\\phi}}(\\mathbf{x}_t, t) \\right\\|_2^2\\right]. \\tag{21}$$\n\n<span id=\"page-18-3\"></span>However, this continuous-time objective requires computing Jacobian-vector products as a subroutine to evaluate the loss function, which can be slow and laborious to implement in deep learning frameworks that do not support forward-mode automatic differentiation.\n\n**Remark 4.** If  $f_{\\theta}(\\mathbf{x}, t)$  matches the ground truth consistency function for the empirical PF ODE of  $s_{\\phi}(\\mathbf{x}, t)$ , then\n\n$$\\frac{\\partial \\mathbf{f}_{\\theta}(\\mathbf{x}, t)}{\\partial t} - t \\frac{\\partial \\mathbf{f}_{\\theta}(\\mathbf{x}, t)}{\\partial \\mathbf{x}} \\mathbf{s}_{\\phi}(\\mathbf{x}, t) \\equiv 0$$\n\nand therefore  $\\mathcal{L}_{CD}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}; \\boldsymbol{\\phi}) = 0$ . This can be proved by noting that  $f_{\\boldsymbol{\\theta}}(\\mathbf{x}_t, t) \\equiv \\mathbf{x}_{\\epsilon}$  for all  $t \\in [\\epsilon, T]$ , and then taking the time-derivative of this identity:\n\n$$f_{\\theta}(\\mathbf{x}_{t}, t) \\equiv \\mathbf{x}_{\\epsilon}$$\n\n$$\\iff \\frac{\\partial f_{\\theta}(\\mathbf{x}_{t}, t)}{\\partial \\mathbf{x}_{t}} \\frac{\\partial \\mathbf{x}_{t}}{\\partial t} + \\frac{\\partial f_{\\theta}(\\mathbf{x}_{t}, t)}{\\partial t} \\equiv 0$$\n\n$$\\iff \\frac{\\partial f_{\\theta}(\\mathbf{x}_{t}, t)}{\\partial \\mathbf{x}_{t}} [-ts_{\\phi}(\\mathbf{x}_{t}, t)] + \\frac{\\partial f_{\\theta}(\\mathbf{x}_{t}, t)}{\\partial t} \\equiv 0$$\n\n$$\\iff \\frac{\\partial f_{\\theta}(\\mathbf{x}_{t}, t)}{\\partial t} - t \\frac{\\partial f_{\\theta}(\\mathbf{x}_{t}, t)}{\\partial \\mathbf{x}_{t}} s_{\\phi}(\\mathbf{x}_{t}, t) \\equiv 0.$$\n\nThe above observation provides another motivation for  $\\mathcal{L}^{\\infty}_{CD}(\\theta, \\theta; \\phi)$ , as it is minimized if and only if the consistency model matches the ground truth consistency function.\n\nFor some metric functions, such as the  $\\ell_1$  norm, the Hessian  $G(\\mathbf{x})$  is zero so Theorem 3 is vacuous. Below we show that a non-vacuous statement holds for the  $\\ell_1$  norm with just a small modification of the proof for Theorem 3.\n\n<span id=\"page-19-3\"></span>**Theorem 4.** Let  $t_n = \\tau(\\frac{n-1}{N-1})$ , where  $n \\in [\\![1,N]\\!]$ , and  $\\tau(\\cdot)$  is a strictly monotonic function with  $\\tau(0) = \\epsilon$  and  $\\tau(1) = T$ . Assume  $\\tau$  is continuously differentiable in  $[\\![0,1]\\!]$ , and  $f_{\\theta}$  is twice continuously differentiable with bounded first and second derivatives. Assume further that the weighting function  $\\lambda(\\cdot)$  is bounded, and  $\\sup_{\\mathbf{x},t\\in[\\epsilon,T]}\\|\\mathbf{s}_{\\phi}(\\mathbf{x},t)\\|_2 < \\infty$ . Suppose we use the Euler ODE solver, and set  $d(\\mathbf{x},\\mathbf{y}) = \\|\\mathbf{x} - \\mathbf{y}\\|_1$  in consistency distillation. Then we have\n\n<span id=\"page-19-2\"></span><span id=\"page-19-1\"></span>\n$$\\lim_{N \\to \\infty} (N-1) \\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}; \\boldsymbol{\\phi}) = \\mathcal{L}_{CD, \\ell_{1}}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}; \\boldsymbol{\\phi}), \\tag{22}$$\n\nwhere\n\n$$\\mathcal{L}_{CD, \\ \\ell_1}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}; \\boldsymbol{\\phi}) \\coloneqq \\mathbb{E}\\left[\\frac{\\lambda(t)}{(\\tau^{-1})'(t)} \\left\\| t \\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_t, t)}{\\partial \\mathbf{x}_t} \\boldsymbol{s}_{\\boldsymbol{\\phi}}(\\mathbf{x}_t, t) - \\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_t, t)}{\\partial t} \\right\\|_{1}\\right]$$\n\nwhere the expectation above is taken over  $\\mathbf{x} \\sim p_{data}$ ,  $u \\sim \\mathcal{U}[0,1]$ ,  $t = \\tau(u)$ , and  $\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{x}, t^2 \\mathbf{I})$ .\n\n*Proof.* Let  $\\Delta u = \\frac{1}{N-1}$  and  $u_n = \\frac{n-1}{N-1}$ . We have\n\n$$(N-1)\\mathcal{L}_{\\text{CD}}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta};\\boldsymbol{\\phi}) = \\frac{1}{\\Delta u}\\mathcal{L}_{\\text{CD}}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta};\\boldsymbol{\\phi}) = \\frac{1}{\\Delta u}\\mathbb{E}[\\lambda(t_{n})\\|\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n})\\|_{1}]$$\n\n$$\\stackrel{(i)}{=} \\frac{1}{\\Delta u}\\mathbb{E}\\left[\\lambda(t_{n})\\|t_{n+1}\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}}\\boldsymbol{s}_{\\boldsymbol{\\phi}}(\\mathbf{x}_{t_{n+1}},t_{n+1})\\boldsymbol{\\tau}'(u_{n}) - \\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial t_{n+1}}\\boldsymbol{\\tau}'(u_{n}) + O((\\Delta u)^{2})\\|_{1}\\right]$$\n\n$$=\\mathbb{E}\\left[\\lambda(t_{n})\\boldsymbol{\\tau}'(u_{n})\\|t_{n+1}\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}}\\boldsymbol{s}_{\\boldsymbol{\\phi}}(\\mathbf{x}_{t_{n+1}},t_{n+1}) - \\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial t_{n+1}} + O(\\Delta u)\\|_{1}\\right]$$\n\n$$=\\mathbb{E}\\left[\\frac{\\lambda(t_{n})}{(\\boldsymbol{\\tau}^{-1})'(t_{n})}\\|t_{n+1}\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}}\\boldsymbol{s}_{\\boldsymbol{\\phi}}(\\mathbf{x}_{t_{n+1}},t_{n+1}) - \\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial t_{n+1}} + O(\\Delta u)\\|_{1}\\right]$$\n\n$$(23)$$\n\nwhere (i) is obtained by plugging Eq. (19) into the previous equation. Taking the limit for both sides of Eq. (23) as  $\\Delta u \\to 0$  or equivalently  $N \\to \\infty$  leads to Eq. (22), which completes the proof.\n\n**Remark 5.** According to Theorem 4, consistency models can be trained by minimizing  $\\mathcal{L}_{CD, \\ell_1}^{\\infty}(\\theta, \\theta; \\phi)$ . Moreover, the same reasoning in Remark 4 can be applied to show that  $\\mathcal{L}_{CD, \\ell_1}^{\\infty}(\\theta, \\theta; \\phi) = 0$  if and only if  $f_{\\theta}(\\mathbf{x}_t, t) = \\mathbf{x}_{\\epsilon}$  for all  $\\mathbf{x}_t \\in \\mathbb{R}^d$  and  $t \\in [\\epsilon, T]$ .\n\n<span id=\"page-19-0\"></span>In the second case where  $\\theta^- = \\operatorname{stopgrad}(\\theta)$ , we can derive a so-called \"pseudo-objective\" whose gradient matches the gradient of  $\\mathcal{L}_{\\operatorname{CD}}^N(\\theta, \\theta^-; \\phi)$  in the limit of  $N \\to \\infty$ . Minimizing this pseudo-objective with gradient descent gives another way to train consistency models via distillation. This pseudo-objective is provided by the theorem below.\n\n**Theorem 5.** Let  $t_n = \\tau(\\frac{n-1}{N-1})$ , where  $n \\in [\\![1,N]\\!]$ , and  $\\tau(\\cdot)$  is a strictly monotonic function with  $\\tau(0) = \\epsilon$  and  $\\tau(1) = T$ . Assume  $\\tau$  is continuously differentiable in  $[\\![0,1]\\!]$ , d is three times continuously differentiable with bounded third derivatives, and  $f_{\\theta}$  is twice continuously differentiable with bounded first and second derivatives. Assume further that the weighting function  $\\lambda(\\cdot)$  is bounded,  $\\sup_{\\mathbf{x},t\\in[\\epsilon,T]}\\|\\mathbf{s}_{\\phi}(\\mathbf{x},t)\\|_2 < \\infty$ , and  $\\sup_{\\mathbf{x},t\\in[\\epsilon,T]}\\|\\nabla_{\\theta}f_{\\theta}(\\mathbf{x},t)\\|_2 < \\infty$ . Suppose we use the Euler ODE solver, and  $\\theta^- = \\operatorname{stopgrad}(\\theta)$  in consistency distillation. Then,\n\n<span id=\"page-20-1\"></span><span id=\"page-20-0\"></span>\n$$\\lim_{N \\to \\infty} (N-1) \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}; \\boldsymbol{\\phi}) = \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{CD}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}; \\boldsymbol{\\phi}), \\tag{24}$$\n\nwhere\n\n$$\\mathcal{L}_{CD}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}; \\boldsymbol{\\phi}) := \\mathbb{E}\\left[\\frac{\\lambda(t)}{(\\tau^{-1})'(t)} \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t}, t)^{\\mathsf{T}} \\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t}, t)) \\left(\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t}, t)}{\\partial t} - t \\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t}, t)}{\\partial \\mathbf{x}_{t}} \\boldsymbol{s}_{\\boldsymbol{\\phi}}(\\mathbf{x}_{t}, t)\\right)\\right]. \\tag{25}$$\n\nHere the expectation above is taken over  $\\mathbf{x} \\sim p_{data}$ ,  $u \\sim \\mathcal{U}[0, 1]$ ,  $t = \\tau(u)$ , and  $\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{x}, t^2 \\mathbf{I})$ .\n\n*Proof.* We denote  $\\Delta u = \\frac{1}{N-1}$  and  $u_n = \\frac{n-1}{N-1}$ . First, we leverage Taylor series expansion to obtain\n\n$$(N-1)\\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{-};\\boldsymbol{\\phi}) = \\frac{1}{\\Delta u}\\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{-};\\boldsymbol{\\phi}) = \\frac{1}{\\Delta u}\\mathbb{E}[\\lambda(t_{n})d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n})]$$\n\n$$\\stackrel{(i)}{=} \\frac{1}{2\\Delta u}\\left(\\mathbb{E}\\{\\lambda(t_{n})[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))\\right)$$\n\n$$\\cdot [\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n})]\\} + \\mathbb{E}[O(|\\Delta u|^{2})]$$\n\n$$(26)$$\n\nwhere (i) is derived by expanding  $d(\\cdot, f_{\\theta^-}(\\hat{\\mathbf{x}}_{t_n}^{\\phi}, t_n))$  to second order and leveraging  $d(\\mathbf{x}, \\mathbf{x}) \\equiv 0$  and  $\\nabla_{\\mathbf{y}} d(\\mathbf{y}, \\mathbf{x})|_{\\mathbf{y} = \\mathbf{x}} \\equiv \\mathbf{0}$ . Next, we compute the gradient of Eq. (26) with respect to  $\\boldsymbol{\\theta}$  and simplify the result to obtain\n\n$$(N-1)\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{-};\\boldsymbol{\\phi}) = \\frac{1}{\\Delta u}\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{-};\\boldsymbol{\\phi})$$\n\n$$= \\frac{1}{2\\Delta u}\\nabla_{\\boldsymbol{\\theta}}\\mathbb{E}\\{\\lambda(t_{n})[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n})]\\} + \\mathbb{E}[O(|\\Delta u|^{2})]$$\n\n$$\\stackrel{(ii)}{=} \\frac{1}{\\Delta u}\\mathbb{E}\\{\\lambda(t_{n})[\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n})]\\} + \\mathbb{E}[O(|\\Delta u|^{2})]$$\n\n$$\\stackrel{(ii)}{=} \\frac{1}{\\Delta u}\\mathbb{E}\\{\\lambda(t_{n})[\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))[\\boldsymbol{t}_{n+1}\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}}\\boldsymbol{\\tau}'(u_{n})\\Delta u]\\} + \\mathbb{E}[O(|\\Delta u|)]$$\n\n$$=\\mathbb{E}\\{\\lambda(t_{n})[\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))[\\boldsymbol{t}_{n+1}\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}}\\boldsymbol{\\tau}'(u_{n})\\Delta u]\\} + \\mathbb{E}[O(|\\Delta u|)]$$\n\n$$=\\nabla_{\\boldsymbol{\\theta}}\\mathbb{E}\\{\\lambda(t_{n})[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))[\\boldsymbol{t}_{n+1}\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}}\\boldsymbol{\\tau}'(u_{n})]\\} + \\mathbb{E}[O(|\\Delta u|)]$$\n\n$$=\\nabla_{\\boldsymbol{\\theta}}\\mathbb{E}\\{\\lambda(t_{n})[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))[\\boldsymbol{t}_{n+1}\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}}\\boldsymbol{\\tau}'(u_{n})]\\} + \\mathbb{E}[O(|\\Delta u|)]$$\n\n$$=\\nabla_{\\boldsymbol{\\theta}}\\mathbb{E}\\{\\lambda(t_{n})[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))[\\boldsymbol{t}_{n+1}\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}}\\boldsymbol{\\tau}'(u_{n})]\\} + \\mathbb{E}[O(|\\Delta u|)]$$\n\n$$=\\nabla_{\\boldsymbol{\\theta}}\\mathbb{E}\\{\\lambda(t_{n})[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))[\\boldsymbol{t}_{n+1}\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}}\\boldsymbol{\\tau}'(u_{n})]\\} + \\mathbb{E}[O(|\\Delta u|)]$$\n\n$$=\\nabla_{\\boldsymbol{\\theta}}\\mathbb{E}\\{\\lambda(t_{n})[\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{$$\n\nHere (i) results from the chain rule, and (ii) follows from Eq. (19) and  $f_{\\theta}(\\mathbf{x}, t) \\equiv f_{\\theta^{-}}(\\mathbf{x}, t)$ , since  $\\theta^{-} = \\text{stopgrad}(\\theta)$ . Taking the limit for both sides of Eq. (28) as  $\\Delta u \\to 0$  (or  $N \\to \\infty$ ) yields Eq. (24), which completes the proof.\n\n**Remark 6.** When  $d(\\mathbf{x}, \\mathbf{y}) = \\|\\mathbf{x} - \\mathbf{y}\\|_2^2$ , the pseudo-objective  $\\mathcal{L}_{CD}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^-; \\boldsymbol{\\phi})$  can be simplified to\n\n<span id=\"page-21-2\"></span>\n$$\\mathcal{L}_{CD}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}; \\boldsymbol{\\phi}) = 2\\mathbb{E}\\left[\\frac{\\lambda(t)}{(\\tau^{-1})'(t)} \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t}, t)^{\\mathsf{T}} \\left(\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t}, t)}{\\partial t} - t \\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t}, t)}{\\partial \\mathbf{x}_{t}} \\boldsymbol{s}_{\\boldsymbol{\\phi}}(\\mathbf{x}_{t}, t)\\right)\\right]. \\tag{28}$$\n\n**Remark 7.** The objective  $\\mathcal{L}^{\\infty}_{CD}(\\theta, \\theta^-; \\phi)$  defined in Theorem 5 is only meaningful in terms of its gradient—one cannot measure the progress of training by tracking the value of  $\\mathcal{L}^{\\infty}_{CD}(\\theta, \\theta^-; \\phi)$ , but can still apply gradient descent to this objective to distill consistency models from pre-trained diffusion models. Because this objective is not a typical loss function, we refer to it as the \"pseudo-objective\" for consistency distillation.\n\n<span id=\"page-21-5\"></span>**Remark 8.** Following the same reasoning in Remark 4, we can easily derive that  $\\mathcal{L}_{CD}^{\\infty}(\\theta, \\theta^-; \\phi) = 0$  and  $\\nabla_{\\theta} \\mathcal{L}_{CD}^{\\infty}(\\theta, \\theta^-; \\phi) = 0$  if  $f_{\\theta}(\\mathbf{x}, t)$  matches the ground truth consistency function for the empirical PF ODE that involves  $s_{\\phi}(\\mathbf{x}, t)$ . However, the converse does not hold true in general. This distinguishes  $\\mathcal{L}_{CD}^{\\infty}(\\theta, \\theta^-; \\phi)$  from  $\\mathcal{L}_{CD}^{\\infty}(\\theta, \\theta; \\phi)$ , the latter of which is a true loss function.\n\n#### <span id=\"page-21-0\"></span>**B.2.** Consistency Training in Continuous Time\n\nA remarkable observation is that the pseudo-objective in Theorem 5 can be estimated without any pre-trained diffusion models, which enables direct consistency training of consistency models. More precisely, we have the following result.\n\n<span id=\"page-21-1\"></span>**Theorem 6.** Let  $t_n = \\tau(\\frac{n-1}{N-1})$ , where  $n \\in [\\![1,N]\\!]$ , and  $\\tau(\\cdot)$  is a strictly monotonic function with  $\\tau(0) = \\epsilon$  and  $\\tau(1) = T$ . Assume  $\\tau$  is continuously differentiable in  $[\\![0,1]\\!]$ , d is three times continuously differentiable with bounded third derivatives, and  $f_{\\theta}$  is twice continuously differentiable with bounded first and second derivatives. Assume further that the weighting function  $\\lambda(\\cdot)$  is bounded,  $\\mathbb{E}[\\|\\nabla \\log p_{t_n}(\\mathbf{x}_{t_n})\\|_2^2] < \\infty$ ,  $\\sup_{\\mathbf{x},t \\in [\\epsilon,T]} \\|\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\mathbf{x},t)\\|_2 < \\infty$ , and  $\\phi$  represents diffusion model parameters that satisfy  $s_{\\boldsymbol{\\phi}}(\\mathbf{x},t) \\equiv \\nabla \\log p_t(\\mathbf{x})$ . Then if  $\\boldsymbol{\\theta}^- = \\operatorname{stopgrad}(\\boldsymbol{\\theta})$ , we have\n\n<span id=\"page-21-4\"></span><span id=\"page-21-3\"></span>\n$$\\lim_{N \\to \\infty} (N-1) \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}; \\boldsymbol{\\phi}) = \\lim_{N \\to \\infty} (N-1) \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{CT}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}) = \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{CT}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}), \\tag{29}$$\n\nwhere  $\\mathcal{L}_{CD}^{N}$  uses the Euler ODE solver, and\n\n$$\\mathcal{L}_{CT}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}) := \\mathbb{E}\\left[\\frac{\\lambda(t)}{(\\tau^{-1})'(t)} \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t}, t)^{\\mathsf{T}} \\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t}, t)) \\left(\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t}, t)}{\\partial t} + \\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t}, t)}{\\partial \\mathbf{x}_{t}} \\cdot \\frac{\\mathbf{x}_{t} - \\mathbf{x}}{t}\\right)\\right].$$\n(30)\n\nHere the expectation above is taken over  $\\mathbf{x} \\sim p_{data}$ ,  $u \\sim \\mathcal{U}[0,1]$ ,  $t = \\tau(u)$ , and  $\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{x}, t^2 \\mathbf{I})$ .\n\n*Proof.* The proof mostly follows that of Theorem 5. First, we leverage Taylor series expansion to obtain\n\n$$(N-1)\\mathcal{L}_{\\mathrm{CT}}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{-}) = \\frac{1}{\\Delta u}\\mathcal{L}_{\\mathrm{CT}}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{-}) = \\frac{1}{\\Delta u}\\mathbb{E}[\\lambda(t_{n})d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1}),\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}+t_{n}\\mathbf{z},t_{n}))]$$\n\n$$\\stackrel{(i)}{=} \\frac{1}{2\\Delta u}\\left(\\mathbb{E}\\{\\lambda(t_{n})[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1})-\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}+t_{n}\\mathbf{z},t_{n})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}+t_{n}\\mathbf{z},t_{n}))\\right)$$\n\n$$\\cdot[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1})-\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}+t_{n}\\mathbf{z},t_{n})]\\}+\\mathbb{E}[O(|\\Delta u|^{3})]$$\n\n$$=\\frac{1}{2\\Delta u}\\mathbb{E}\\{\\lambda(t_{n})[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1})-\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}+t_{n}\\mathbf{z},t_{n})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}+t_{n}\\mathbf{z},t_{n}))$$\n\n$$\\cdot[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1})-\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}+t_{n}\\mathbf{z},t_{n})]\\}+\\mathbb{E}[O(|\\Delta u|^{2})]$$\n\nwhere  $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ , (i) is derived by first expanding  $d(\\cdot, \\mathbf{f}_{\\theta^-}(\\mathbf{x} + t_n \\mathbf{z}, t_n))$  to second order, and then noting that  $d(\\mathbf{x}, \\mathbf{x}) \\equiv 0$  and  $\\nabla_{\\mathbf{y}} d(\\mathbf{y}, \\mathbf{x})|_{\\mathbf{y} = \\mathbf{x}} \\equiv \\mathbf{0}$ . Next, we compute the gradient of Eq. (31) with respect to  $\\boldsymbol{\\theta}$  and simplify the result to obtain\n\n$$(N-1)\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{\\mathrm{CT}}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{-}) = \\frac{1}{\\Delta u}\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{\\mathrm{CT}}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{-})$$\n\n$$= \\frac{1}{2\\Delta u}\\nabla_{\\boldsymbol{\\theta}}\\mathbb{E}\\{\\lambda(t_{n})[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}+t_{n}\\mathbf{z},t_{n})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}+t_{n}\\mathbf{z},t_{n}))$$\n\n$$\\cdot [\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}+t_{n}\\mathbf{z},t_{n})]\\} + \\mathbb{E}[O(|\\Delta u|^{2})]$$\n\n<span id=\"page-22-1\"></span>\n$$\\frac{(i)}{\\Delta u} \\mathbb{E}\\{\\lambda(t_n)[\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\mathbf{x} + t_{n+1}\\mathbf{z}, t_{n+1})]^{\\mathsf{T}} \\boldsymbol{H}(f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x} + t_n\\mathbf{z}, t_n)) \\\\\n\\cdot [f_{\\boldsymbol{\\theta}}(\\mathbf{x} + t_{n+1}\\mathbf{z}, t_{n+1}) - f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x} + t_n\\mathbf{z}, t_n)]\\} + \\mathbb{E}[O(|\\Delta u|^2)] \\\\\n\\stackrel{(ii)}{=} \\frac{1}{\\Delta u} \\mathbb{E}\\{\\lambda(t_n)[\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\mathbf{x} + t_{n+1}\\mathbf{z}, t_{n+1})]^{\\mathsf{T}} \\boldsymbol{H}(f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x} + t_n\\mathbf{z}, t_n)) \\Big[ \\tau'(u_n) \\Delta u \\partial_1 f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x} + t_n\\mathbf{z}, t_n)\\mathbf{z} \\\\\n+ \\partial_2 f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x} + t_n\\mathbf{z}, t_n) \\tau'(u_n) \\Delta u \\Big] \\Big\\} + \\mathbb{E}[O(|\\Delta u|)] \\\\\n= \\mathbb{E}\\{\\lambda(t_n) \\tau'(u_n)[\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\mathbf{x} + t_{n+1}\\mathbf{z}, t_{n+1})]^{\\mathsf{T}} \\boldsymbol{H}(f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x} + t_n\\mathbf{z}, t_n)) \\Big[ \\partial_1 f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x} + t_n\\mathbf{z}, t_n)\\mathbf{z} \\\\\n+ \\partial_2 f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x} + t_n\\mathbf{z}, t_n) \\Big] \\Big\\} + \\mathbb{E}[O(|\\Delta u|)] \\\\\n= \\nabla_{\\boldsymbol{\\theta}} \\mathbb{E}\\{\\lambda(t_n) \\tau'(u_n)[f_{\\boldsymbol{\\theta}}(\\mathbf{x} + t_{n+1}\\mathbf{z}, t_{n+1})]^{\\mathsf{T}} \\boldsymbol{H}(f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x} + t_n\\mathbf{z}, t_n)) \\Big[ \\partial_1 f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x} + t_n\\mathbf{z}, t_n)\\mathbf{z} \\\\\n+ \\partial_2 f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x} + t_n\\mathbf{z}, t_n) \\Big] \\Big\\} + \\mathbb{E}[O(|\\Delta u|)] \\\\\n= \\nabla_{\\boldsymbol{\\theta}} \\mathbb{E}\\{\\lambda(t_n) \\tau'(u_n)[f_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}}, t_{n+1})]^{\\mathsf{T}} \\boldsymbol{H}(f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n)) \\Big[ \\partial_1 f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n) \\frac{\\mathbf{x}_{t_n} - \\mathbf{x}}{t_n} + \\partial_2 f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n) \\Big] \\Big\\} + \\mathbb{E}[O(|\\Delta u|)] \\\\\n= \\nabla_{\\boldsymbol{\\theta}} \\mathbb{E}\\{\\lambda(t_n) \\tau'(t_n)[f_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}}, t_{n+1})]^{\\mathsf{T}} \\boldsymbol{H}(f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n)) \\Big[ \\partial_1 f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n) \\frac{\\mathbf{x}_{t_n} - \\mathbf{x}}{t_n} + \\partial_2 f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n) \\Big] \\Big\\} + \\mathbb{E}[O(|\\Delta u|)] \\\\\n= \\nabla_{\\boldsymbol{\\theta}} \\mathbb{E}\\{\\lambda(t_n) \\tau'(t_n)[f_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}}, t_{n+1})]^{\\mathsf{T}} \\boldsymbol{H}(f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n)) \\Big[ \\partial_1 f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n) \\frac{\\mathbf{x}_{t_n} - \\mathbf{x}}{t_n} + \\partial_2 f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n) \\Big] \\Big\\} + \\mathbb{E}[O(|\\Delta u|)] \\\\\n= \\nabla_{\\boldsymbol{\\theta}} \\mathbb{E}\\{\\lambda(t_n) \\tau'(t_n)[f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n)] \\Big\\} + \\mathbb{E}[O(|\\Delta u|)] \\\\\n= \\nabla_{\\boldsymbol{\\theta}} \\mathbb{E}\\{\\lambda(t_n) \\tau'(t_n)[f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n)] \\Big\\} + \\mathbb{E}[O(|\\Delta u|)] \\\\\n= \\nabla_{\\boldsymbol{\\theta}^{-}} \\mathbb{E}[h_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n)] \\Big\\} + \\mathbb{E}[h_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n) \\Big\\} \\\\\n= \\nabla_{\\boldsymbol{\\theta}^{-}} \\mathbb{E}[h_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n)] \\Big\\} + \\mathbb{E}[h_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n)] \\\\\n= \\nabla_{\\boldsymbol{\\theta}^{-}} \\mathbb{E}[h_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}$$\n\nHere (i) results from the chain rule, and (ii) follows from Taylor expansion. Taking the limit for both sides of Eq. (33) as  $\\Delta u \\to 0$  or  $N \\to \\infty$  yields the second equality in Eq. (29).\n\nNow we prove the first equality. Applying Taylor expansion again, we obtain\n\n<span id=\"page-22-0\"></span>\n$$(N-1)\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{\\text{CD}}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{-};\\boldsymbol{\\phi}) = \\frac{1}{\\Delta u}\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{\\text{CD}}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{-};\\boldsymbol{\\phi}) = \\frac{1}{\\Delta u}\\nabla_{\\boldsymbol{\\theta}}\\mathbb{E}[\\lambda(t_{n})d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))]$$\n\n$$= \\frac{1}{\\Delta u}\\mathbb{E}[\\lambda(t_{n})\\nabla_{\\boldsymbol{\\theta}}d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))]$$\n\n$$= \\frac{1}{\\Delta u}\\mathbb{E}[\\lambda(t_{n})\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})^{\\mathsf{T}}\\partial_{1}d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))]$$\n\n$$= \\frac{1}{\\Delta u}\\mathbb{E}\\left\\{\\lambda(t_{n})\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})^{\\mathsf{T}}\\left[\\partial_{1}d(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}),\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))\\right.\\right.$$\n\n$$\\left. + \\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n})) + O(|\\Delta u|^{2})\\right]\\right\\}$$\n\n$$= \\frac{1}{\\Delta u}\\mathbb{E}\\{\\lambda(t_{n})\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})^{\\mathsf{T}}[\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))] + O(|\\Delta u|^{2})\\}$$\n\n$$= \\frac{1}{\\Delta u}\\mathbb{E}\\{\\lambda(t_{n})\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})^{\\mathsf{T}}[\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))] + O(|\\Delta u|^{2})\\}$$\n\n$$= \\frac{1}{\\Delta u}\\mathbb{E}\\{\\lambda(t_{n})[\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}+t_{n}\\mathbf{z},t_{n}))$$\n\n$$\\cdot [\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}+t_{n}\\mathbf{z},t_{n})]\\} + \\mathbb{E}[O(|\\Delta u|^{2})]$$\n\nwhere (i) holds because  $\\mathbf{x}_{t_{n+1}} = \\mathbf{x} + t_{n+1}\\mathbf{z}$  and  $\\hat{\\mathbf{x}}_{t_n}^{\\phi} = \\mathbf{x}_{t_{n+1}} - (t_n - t_{n+1})t_{n+1}\\frac{-(\\mathbf{x}_{t_{n+1}} - \\mathbf{x})}{t_{n+1}^2} = \\mathbf{x}_{t_{n+1}} + (t_n - t_{n+1})\\mathbf{z} = \\mathbf{x}_{t_n}\\mathbf{z}$ . Because (i) matches Eq. (32), we can use the same reasoning procedure from Eq. (32) to Eq. (33) to conclude  $\\lim_{N\\to\\infty}(N-1)\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{\\mathrm{CD}}^N(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^-;\\boldsymbol{\\phi}) = \\lim_{N\\to\\infty}(N-1)\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{\\mathrm{CT}}^N(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^-)$ , completing the proof.\n\n**Remark 9.** Note that  $\\mathcal{L}_{CT}^{\\infty}(\\theta, \\theta^{-})$  does not depend on the diffusion model parameter  $\\phi$  and hence can be optimized without any pre-trained diffusion models.\n\n<span id=\"page-23-1\"></span>![](_page_23_Figure_1.jpeg)\n\nFigure 7: Comparing discrete consistency distillation/training algorithms with continuous counterparts.\n\n**Remark 10.** When  $d(\\mathbf{x}, \\mathbf{y}) = \\|\\mathbf{x} - \\mathbf{y}\\|_2^2$ , the continuous-time consistency training objective becomes\n\n$$\\mathcal{L}_{CT}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}) = 2\\mathbb{E}\\left[\\frac{\\lambda(t)}{(\\tau^{-1})'(t)}\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t}, t)^{\\mathsf{T}}\\left(\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t}, t)}{\\partial t} + \\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t}, t)}{\\partial \\mathbf{x}_{t}} \\cdot \\frac{\\mathbf{x}_{t} - \\mathbf{x}}{t}\\right)\\right].$$\n(34)\n\n**Remark 11.** Similar to  $\\mathcal{L}^{\\infty}_{CD}(\\theta, \\theta^-; \\phi)$  in Theorem 5,  $\\mathcal{L}^{\\infty}_{CT}(\\theta, \\theta^-)$  is a pseudo-objective; one cannot track training by monitoring the value of  $\\mathcal{L}^{\\infty}_{CT}(\\theta, \\theta^-)$ , but can still apply gradient descent on this loss function to train a consistency model  $f_{\\theta}(\\mathbf{x},t)$  directly from data. Moreover, the same observation in Remark 8 holds true:  $\\mathcal{L}^{\\infty}_{CT}(\\theta, \\theta^-) = 0$  and  $\\nabla_{\\theta}\\mathcal{L}^{\\infty}_{CT}(\\theta, \\theta^-) = 0$  if  $f_{\\theta}(\\mathbf{x},t)$  matches the ground truth consistency function for the PF ODE.\n\n#### <span id=\"page-23-0\"></span>**B.3. Experimental Verifications**\n\nTo experimentally verify the efficacy of our continuous-time CD and CT objectives, we train consistency models with a variety of loss functions on CIFAR-10. All results are provided in Fig. 7. We set  $\\lambda(t) = (\\tau^{-1})'(t)$  for all continuous-time experiments. Other hyperparameters are the same as in Table 3. We occasionally modify some hyperparameters for improved performance. For distillation, we compare the following objectives:\n\n- CD  $(\\ell_2)$ : Consistency distillation  $\\mathcal{L}_{CD}^N$  with N=18 and the  $\\ell_2$  metric.\n- CD  $(\\ell_1)$ : Consistency distillation  $\\mathcal{L}_{CD}^N$  with N=18 and the  $\\ell_1$  metric. We set the learning rate to 2e-4.\n- CD (LPIPS): Consistency distillation  $\\mathcal{L}_{\\text{CD}}^{N}$  with N=18 and the LPIPS metric.\n- $CD^{\\infty}$  ( $\\ell_2$ ): Consistency distillation  $\\mathcal{L}_{CD}^{\\infty}$  in Theorem 3 with the  $\\ell_2$  metric. We set the learning rate to 1e-3 and dropout to 0.13.\n- $CD^{\\infty}$  ( $\\ell_1$ ): Consistency distillation  $\\mathcal{L}_{CD}^{\\infty}$  in Theorem 4 with the  $\\ell_1$  metric. We set the learning rate to 1e-3 and dropout to 0.3.\n- CD $^{\\infty}$  (stopgrad,  $\\ell_2$ ): Consistency distillation  $\\mathcal{L}_{CD}^{\\infty}$  in Theorem 5 with the  $\\ell_2$  metric. We set the learning rate to 5e-6.\n- $CD^{\\infty}$  (stopgrad, LPIPS): Consistency distillation  $\\mathcal{L}_{CD}^{\\infty}$  in Theorem 5 with the LPIPS metric. We set the learning rate to 5e-6.\n\nWe did not investigate using the LPIPS metric in Theorem 3 because minimizing the resulting objective would require back-propagating through second order derivatives of the VGG network used in LPIPS, which is computationally expensive and prone to numerical instability. As revealed by Fig. 7a, the stopgrad version of continuous-time distillation (Theorem 5) works better than the non-stopgrad version (Theorem 3) for both the LPIPS and  $\\ell_2$  metrics, and the LPIPS metric works the best for all distillation approaches. Additionally, discrete-time consistency distillation outperforms continuous-time\n\n<span id=\"page-24-1\"></span>\n\n| Hyperparameter         | CIFAR-10 |        | ImageNet $64 \\times 64$ |          | LSUN $256 \\times 256$ |          |\n|------------------------|----------|--------|-------------------------|----------|-----------------------|----------|\n|                        | CD       | CT     | CD                      | CT       | CD                    | CT       |\n| Learning rate          | 4e-4     | 4e-4   | 8e-6                    | 8e-6     | 1e-5                  | 1e-5     |\n| Batch size             | 512      | 512    | 2048                    | 2048     | 2048                  | 2048     |\n| $\\mu$                  | 0        |        | 0.95                    |          | 0.95                  |          |\n| $\\mu_0$                |          | 0.9    |                         | 0.95     |                       | 0.95     |\n| $s_0$                  |          | 2      |                         | 2        |                       | 2        |\n| $s_1$                  |          | 150    |                         | 200      |                       | 150      |\n| N                      | 18       |        | 40                      |          | 40                    |          |\n| ODE solver             | Heun     |        | Heun                    |          | Heun                  |          |\n| EMA decay rate         | 0.9999   | 0.9999 | 0.999943                | 0.999943 | 0.999943              | 0.999943 |\n| Training iterations    | 800k     | 800k   | 600k                    | 800k     | 600k                  | 1000k    |\n| Mixed-Precision (FP16) | No       | No     | Yes                     | Yes      | Yes                   | Yes      |\n| Dropout probability    | 0.0      | 0.0    | 0.0                     | 0.0      | 0.0                   | 0.0      |\n| Number of GPUs         | 8        | 8      | 64                      | 64       | 64                    | 64       |\n\nTable 3: Hyperparameters used for training CD and CT models\n\nconsistency distillation, possibly due to the larger variance in continuous-time objectives, and the fact that one can use effective higher-order ODE solvers in discrete-time objectives.\n\nFor consistency training (CT), we find it important to initialize consistency models from a pre-trained EDM model in order to stabilize training when using continuous-time objectives. We hypothesize that this is caused by the large variance in our continuous-time loss functions. For fair comparison, we thus initialize all consistency models from the same pre-trained EDM model on CIFAR-10 for both discrete-time and continuous-time CT, even though the former works well with random initialization. We leave variance reduction techniques for continuous-time CT to future research.\n\nWe empirically compare the following objectives:\n\n- CT (LPIPS): Consistency training  $\\mathcal{L}_{\\text{CT}}^N$  with N=120 and the LPIPS metric. We set the learning rate to 4e-4, and the EMA decay rate for the target network to 0.99. We do not use the schedule functions for N and  $\\mu$  here because they cause slower learning when the consistency model is initialized from a pre-trained EDM model.\n- $CT^{\\infty}$  ( $\\ell_2$ ): Consistency training  $\\mathcal{L}_{CT}^{\\infty}$  with the  $\\ell_2$  metric. We set the learning rate to 5e-6.\n- $CT^{\\infty}$  (LPIPS): Consistency training  $\\mathcal{L}_{CT}^{\\infty}$  with the LPIPS metric. We set the learning rate to 5e-6.\n\nAs shown in Fig. 7b, the LPIPS metric leads to improved performance for continuous-time CT. We also find that continuous-time CT outperforms discrete-time CT with the same LPIPS metric. This is likely due to the bias in discrete-time CT, as  $\\Delta t > 0$  in Theorem 2 for discrete-time objectives, whereas continuous-time CT has no bias since it implicitly drives  $\\Delta t$  to 0.\n\n#### <span id=\"page-24-0\"></span>C. Additional Experimental Details\n\n**Model Architectures** We follow Song et al. (2021); Dhariwal & Nichol (2021) for model architectures. Specifically, we use the NCSN++ architecture in Song et al. (2021) for all CIFAR-10 experiments, and take the corresponding network architectures from Dhariwal & Nichol (2021) when performing experiments on ImageNet  $64 \\times 64$ , LSUN Bedroom  $256 \\times 256$  and LSUN Cat  $256 \\times 256$ .\n\n**Parameterization for Consistency Models** We use the same architectures for consistency models as those used for EDMs. The only difference is we slightly modify the skip connections in EDM to ensure the boundary condition holds for consistency models. Recall that in Section 3 we propose to parameterize a consistency model in the following form:\n\n<span id=\"page-24-2\"></span>\n$$f_{\\theta}(\\mathbf{x}, t) = c_{\\text{skip}}(t)\\mathbf{x} + c_{\\text{out}}(t)F_{\\theta}(\\mathbf{x}, t).$$\n\nIn EDM (Karras et al., 2022), authors choose\n\n$$c_{\nm skip}(t) = \frac{\\sigma_{\nm data}^2}{t^2 + \\sigma_{\nm data}^2}, \\quad c_{\nm out}(t) = \frac{\\sigma_{\nm data}t}{\\sqrt{\\sigma_{\nm data}^2 + t^2}},$$\n\nwhere  $\\sigma_{\\text{data}} = 0.5$ . However, this choice of  $c_{\\text{skip}}$  and  $c_{\\text{out}}$  does not satisfy the boundary condition when the smallest time instant  $\\epsilon \\neq 0$ . To remedy this issue, we modify them to\n\n$$c_{\nm skip}(t) = \frac{\\sigma_{\nm data}^2}{(t-\\epsilon)^2 + \\sigma_{\nm data}^2}, \\quad c_{\nm out}(t) = \frac{\\sigma_{\nm data}(t-\\epsilon)}{\\sqrt{\\sigma_{\nm data}^2 + t^2}},$$\n\nwhich clearly satisfies  $c_{\\text{skip}}(\\epsilon) = 1$  and  $c_{\\text{out}}(\\epsilon) = 0$ .\n\nSchedule Functions for Consistency Training As discussed in Section 5, consistency generation requires specifying schedule functions  $N(\\cdot)$  and  $\\mu(\\cdot)$  for best performance. Throughout our experiments, we use schedule functions that take the form below:\n\n$$N(k) = \\left[ \\sqrt{\\frac{k}{K} ((s_1 + 1)^2 - s_0^2) + s_0^2} - 1 \\right] + 1$$\n$$\\mu(k) = \\exp\\left(\\frac{s_0 \\log \\mu_0}{N(k)}\\right),$$\n\nwhere K denotes the total number of training iterations,  $s_0$  denotes the initial discretization steps,  $s_1 > s_0$  denotes the target discretization steps at the end of training, and  $\\mu_0 > 0$  denotes the EMA decay rate at the beginning of model training.\n\n**Training Details** In both consistency distillation and progressive distillation, we distill EDMs (Karras et al., 2022). We trained these EDMs ourselves according to the specifications given in Karras et al. (2022). The original EDM paper did not provide hyperparameters for the LSUN Bedroom  $256 \\times 256$  and Cat  $256 \\times 256$  datasets, so we mostly used the same hyperparameters as those for the ImageNet  $64 \\times 64$  dataset. The difference is that we trained for 600k and 300k iterations for the LSUN Bedroom and Cat datasets respectively, and reduced the batch size from 4096 to 2048.\n\nWe used the same EMA decay rate for LSUN  $256 \\times 256$  datasets as for the ImageNet  $64 \\times 64$  dataset. For progressive distillation, we used the same training settings as those described in Salimans & Ho (2022) for CIFAR-10 and ImageNet  $64 \\times 64$ . Although the original paper did not test on LSUN  $256 \\times 256$  datasets, we used the same settings for ImageNet  $64 \\times 64$  and found them to work well.\n\nIn all distillation experiments, we initialized the consistency model with pre-trained EDM weights. For consistency training, we initialized the model randomly, just as we did for training the EDMs. We trained all consistency models with the Rectified Adam optimizer (Liu et al., 2019), with no learning rate decay or warm-up, and no weight decay. We also applied EMA to the weights of the online consistency models in both consistency distillation and consistency training, as well as to the weights of the training online consistency models according to Karras et al. (2022). For LSUN  $256 \\times 256$  datasets, we chose the EMA decay rate to be the same as that for ImageNet  $64 \\times 64$ , except for consistency distillation on LSUN Bedroom  $256 \\times 256$ , where we found that using zero EMA worked better.\n\nWhen using the LPIPS metric on CIFAR-10 and ImageNet  $64 \\times 64$ , we rescale images to resolution  $224 \\times 224$  with bilinear upsampling before feeding them to the LPIPS network. For LSUN  $256 \\times 256$ , we evaluated LPIPS without rescaling inputs. In addition, we performed horizontal flips for data augmentation for all models and on all datasets. We trained all models on a cluster of Nvidia A100 GPUs. Additional hyperparameters for consistency training and distillation are listed in Table 3.\n\n#### <span id=\"page-25-0\"></span>D. Additional Results on Zero-Shot Image Editing\n\nWith consistency models, we can perform a variety of zero-shot image editing tasks. As an example, we present additional results on colorization (Fig. 8), super-resolution (Fig. 9), inpainting (Fig. 10), interpolation (Fig. 11), denoising (Fig. 12), and stroke-guided image generation (SDEdit, Meng et al. (2021), Fig. 13). The consistency model used here is trained via consistency distillation on the LSUN Bedroom  $256 \\times 256$ .\n\nAll these image editing tasks, except for image interpolation and denoising, can be performed via a small modification to the multistep sampling algorithm in Algorithm 1. The resulting pseudocode is provided in Algorithm 4. Here y is a reference image that guides sample generation,  $\\Omega$  is a binary mask,  $\\odot$  computes element-wise products, and A is an invertible linear transformation that maps images into a latent space where the conditional information in y is infused into the iterative\n\n#### <span id=\"page-26-0\"></span>**Algorithm 4** Zero-Shot Image Editing\n\n1: **Input:** Consistency model  $f_{\\theta}(\\cdot,\\cdot)$ , sequence of time points  $t_1 > t_2 > \\cdots > t_N$ , reference image y, invertible linear transformation A, and binary image mask  $\\Omega$ \n\n2: \n$$\\mathbf{y} \\leftarrow \\mathbf{A}^{-1}[(\\mathbf{A}\\mathbf{y}) \\odot (1 - \\mathbf{\\Omega}) + \\mathbf{0} \\odot \\mathbf{\\Omega}]$$\n\n3: Sample  $\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{y}, t_1^2 \\mathbf{I})$ \n\n4:  $\\mathbf{x} \\leftarrow \\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}, t_1)$ \n\n5:  $\\mathbf{x} \\leftarrow \\mathbf{A}^{-1}[(\\mathbf{A}\\mathbf{y}) \\odot (1 - \\mathbf{\\Omega}) + (\\mathbf{A}\\mathbf{x}) \\odot \\mathbf{\\Omega}]$ \n\n6: **for** n = 2 **to** N **do** \n\nSample  $\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{x}, (t_n^2 - \\epsilon^2)\\mathbf{I})$ 7:\n\n $\\mathbf{x} \\leftarrow \\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}, t_n)$   $\\mathbf{x} \\leftarrow \\mathbf{A}^{-1}[(\\mathbf{A}\\mathbf{y}) \\odot (1 - \\mathbf{\\Omega}) + (\\mathbf{A}\\mathbf{x}) \\odot \\mathbf{\\Omega}]$ \n\n10: **end for** \n\n11: **Output: x** \n\ngeneration procedure by masking with  $\\Omega$ . Unless otherwise stated, we choose\n\n$$t_i = \\left(T^{1/\\rho} + \\frac{i-1}{N-1} (\\epsilon^{1/\\rho} - T^{1/\\rho})\\right)^{\\rho}$$\n\nin our experiments, where N=40 for LSUN Bedroom  $256\\times256$ .\n\nBelow we describe how to perform each task using Algorithm 4.\n\n**Inpainting** When using Algorithm 4 for inpainting, we let y be an image where missing pixels are masked out,  $\\Omega$  be a binary mask where 1 indicates the missing pixels, and A be the identity transformation.\n\n<span id=\"page-26-1\"></span>Colorization The algorithm for image colorization is similar, as colorization becomes a special case of inpainting once we transform data into a decoupled space. Specifically, let  $\\mathbf{y} \\in \\mathbb{R}^{h \\times w \\times 3}$  be a gray-scale image that we aim to colorize, where all channels of y are assumed to be the same, i.e., y[:,:,0] = y[:,:,1] = y[:,:,2] in NumPy notation. In our experiments, each channel of this gray scale image is obtained from a colorful image by averaging the RGB channels with\n\n$$0.2989R + 0.5870G + 0.1140B$$\n.\n\nWe define  $\\Omega \\in \\{0,1\\}^{h \\times w \\times 3}$  to be a binary mask such that\n\n$$\\Omega[i, j, k] = \\begin{cases} 1, & k = 1 \\text{ or } 2 \\\\ 0, & k = 0 \\end{cases}.$$\n\nLet  $Q \\in \\mathbb{R}^{3\\times 3}$  be an orthogonal matrix whose first column is proportional to the vector (0.2989, 0.5870, 0.1140). This orthogonal matrix can be obtained easily via QR decomposition, and we use the following in our experiments\n\n$$\\mathbf{Q} = \\begin{pmatrix} 0.4471 & -0.8204 & 0.3563 \\\\ 0.8780 & 0.4785 & 0 \\\\ 0.1705 & -0.3129 & -0.9343 \\end{pmatrix}.$$\n\nWe then define the linear transformation  $A: \\mathbf{x} \\in \\mathbb{R}^{h \\times w \\times 3} \\mapsto \\mathbf{y} \\in \\mathbb{R}^{h \\times w \\times 3}$ , where\n\n$$\\mathbf{y}[i,j,k] = \\sum_{l=0}^{2} \\mathbf{x}[i,j,l] \\mathbf{Q}[l,k].$$\n\nBecause Q is orthogonal, the inversion  $A^{-1}: \\mathbf{y} \\in \\mathbb{R}^{h \\times w} \\mapsto \\mathbf{x} \\in \\mathbb{R}^{h \\times w \\times 3}$  is easy to compute, where\n\n$$\\mathbf{x}[i,j,k] = \\sum_{l=0}^{2} \\mathbf{y}[i,j,l] \\mathbf{Q}[k,l].$$\n\nWith A and  $\\Omega$  defined as above, we can now use Algorithm 4 for image colorization.\n\n**Super-resolution** With a similar strategy, we employ Algorithm 4 for image super-resolution. For simplicity, we assume that the down-sampled image is obtained by averaging non-overlapping patches of size  $p \\times p$ . Suppose the shape of full resolution images is  $h \\times w \\times 3$ . Let  $\\mathbf{y} \\in \\mathbb{R}^{h \\times w \\times 3}$  denote a low-resolution image naively up-sampled to full resolution, where pixels in each non-overlapping patch share the same value. Additionally, let  $\\mathbf{\\Omega} \\in \\{0,1\\}^{h/p \\times w/p \\times p^2 \\times 3}$  be a binary mask such that\n\n$$\\mathbf{\\Omega}[i,j,k,l] = \\begin{cases} 1, & k \\geqslant 1 \\\\ 0, & k = 0 \\end{cases}.$$\n\nSimilar to image colorization, super-resolution requires an orthogonal matrix  $Q \\in \\mathbb{R}^{p^2 \\times p^2}$  whose first column is  $(1/p, 1/p, \\cdots, 1/p)$ . This orthogonal matrix can be obtained with QR decomposition. To perform super-resolution, we define the linear transformation  $A : \\mathbf{x} \\in \\mathbb{R}^{h \\times w \\times 3} \\mapsto \\mathbf{y} \\in \\mathbb{R}^{h/p \\times w/p \\times p^2 \\times 3}$ , where\n\n$$\\mathbf{y}[i,j,k,l] = \\sum_{m=0}^{p^2-1} \\mathbf{x}[i \\times p + (m-m \\bmod p)/p, j \\times p + m \\bmod p, l] \\mathbf{Q}[m,k].$$\n\nThe inverse transformation  $A^{-1}: \\mathbf{y} \\in \\mathbb{R}^{h/p \\times w/p \\times p^2 \\times 3} \\mapsto \\mathbf{x} \\in \\mathbb{R}^{h \\times w \\times 3}$  is easy to derive, with\n\n$$\\mathbf{x}[i,j,k,l] = \\sum_{m=0}^{p^2-1} \\mathbf{y}[i \\times p + (m-m \\bmod p)/p, j \\times p + m \\bmod p, l] \\mathbf{Q}[k,m].$$\n\nAbove definitions of A and  $\\Omega$  allow us to use Algorithm 4 for image super-resolution.\n\n**Stroke-guided image generation** We can also use Algorithm 4 for stroke-guided image generation as introduced in SDEdit (Meng et al., 2021). Specifically, we let  $\\mathbf{y} \\in \\mathbb{R}^{h \\times w \\times 3}$  be a stroke painting. We set  $\\mathbf{A} = \\mathbf{I}$ , and define  $\\mathbf{\\Omega} \\in \\mathbb{R}^{h \\times w \\times 3}$  as a matrix of ones. In our experiments, we set  $t_1 = 5.38$  and  $t_2 = 2.24$ , with N = 2.\n\n**Denoising** It is possible to denoise images perturbed with various scales of Gaussian noise using a single consistency model. Suppose the input image  $\\mathbf{x}$  is perturbed with  $\\mathcal{N}(\\mathbf{0}; \\sigma^2 \\mathbf{I})$ . As long as  $\\sigma \\in [\\epsilon, T]$ , we can evaluate  $\\mathbf{f}_{\\theta}(\\mathbf{x}, \\sigma)$  to produce the denoised image.\n\n**Interpolation** We can interpolate between two images generated by consistency models. Suppose the first sample  $\\mathbf{x}_1$  is produced by noise vector  $\\mathbf{z}_1$ , and the second sample  $\\mathbf{x}_2$  is produced by noise vector  $\\mathbf{z}_2$ . In other words,  $\\mathbf{x}_1 = f_{\\theta}(\\mathbf{z}_1, T)$  and  $\\mathbf{x}_2 = f_{\\theta}(\\mathbf{z}_2, T)$ . To interpolate between  $\\mathbf{x}_1$  and  $\\mathbf{x}_2$ , we first use spherical linear interpolation to get\n\n$$\\mathbf{z} = \\frac{\\sin[(1-\\alpha)\\psi]}{\\sin(\\psi)}\\mathbf{z}_1 + \\frac{\\sin(\\alpha\\psi)}{\\sin(\\psi)}\\mathbf{z}_2,$$\n\nwhere  $\\alpha \\in [0, 1]$  and  $\\psi = \\arccos(\\frac{\\mathbf{z}_1^\\mathsf{T} \\mathbf{z}_2}{\\|\\mathbf{z}_1\\|_2 \\|\\mathbf{z}_2\\|_2})$ , then evaluate  $f_{\\theta}(\\mathbf{z}, T)$  to produce the interpolated image.\n\n#### <span id=\"page-27-0\"></span>E. Additional Samples from Consistency Models\n\nWe provide additional samples from consistency distillation (CD) and consistency training (CT) on CIFAR-10 (Figs. 14 and 18), ImageNet  $64 \\times 64$  (Figs. 15 and 19), LSUN Bedroom  $256 \\times 256$  (Figs. 16 and 20) and LSUN Cat  $256 \\times 256$  (Figs. 17 and 21).\n\n<span id=\"page-28-0\"></span>![](_page_28_Picture_1.jpeg)\n\nFigure 8: Gray-scale images (left), colorized images by a consistency model (middle), and ground truth (right).\n\n<span id=\"page-29-0\"></span>![](_page_29_Picture_1.jpeg)\n\nFigure 9: Downsampled images of resolution 32 ˆ 32 (left), full resolution (256 ˆ 256) images generated by a consistency model (middle), and ground truth images of resolution 256 ˆ 256 (right).\n\n<span id=\"page-30-0\"></span>![](_page_30_Picture_1.jpeg)\n\nFigure 10: Masked images (left), imputed images by a consistency model (middle), and ground truth (right).\n\n<span id=\"page-31-0\"></span>![](_page_31_Picture_1.jpeg)\n\nFigure 11: Interpolating between leftmost and rightmost images with spherical linear interpolation. All samples are generated by a consistency model trained on LSUN Bedroom 256 ˆ 256.\n\n<span id=\"page-32-0\"></span>![](_page_32_Figure_1.jpeg)\n\nFigure 12: Single-step denoising with a consistency model. The leftmost images are ground truth. For every two rows, the top row shows noisy images with different noise levels, while the bottom row gives denoised images.\n\n<span id=\"page-33-0\"></span>![](_page_33_Picture_1.jpeg)\n\nFigure 13: SDEdit with a consistency model. The leftmost images are stroke painting inputs. Images on the right side are the results of stroke-guided image generation (SDEdit).\n\n<span id=\"page-34-0\"></span>![](_page_34_Figure_1.jpeg)\n\nFigure 14: Uncurated samples from CIFAR-10 32 ˆ 32. All corresponding samples use the same initial noise.\n\n<span id=\"page-35-0\"></span>![](_page_35_Figure_1.jpeg)\n\nFigure 15: Uncurated samples from ImageNet 64 ˆ 64. All corresponding samples use the same initial noise.\n\n<span id=\"page-36-0\"></span>![](_page_36_Picture_1.jpeg)\n\n(a) EDM (FID=3.57)\n\n![](_page_36_Picture_3.jpeg)\n\n(b) CD with single-step generation (FID=7.80)\n\n![](_page_36_Picture_5.jpeg)\n\n(c) CD with two-step generation (FID=5.22)\n\nFigure 16: Uncurated samples from LSUN Bedroom 256 ˆ 256. All corresponding samples use the same initial noise.\n\n<span id=\"page-37-0\"></span>![](_page_37_Picture_1.jpeg)\n\n(a) EDM (FID=6.69)\n\n![](_page_37_Picture_3.jpeg)\n\n(b) CD with single-step generation (FID=10.99)\n\n![](_page_37_Picture_5.jpeg)\n\n(c) CD with two-step generation (FID=8.84)\n\nFigure 17: Uncurated samples from LSUN Cat 256 ˆ 256. All corresponding samples use the same initial noise.\n\n<span id=\"page-38-0\"></span>![](_page_38_Figure_1.jpeg)\n\nFigure 18: Uncurated samples from CIFAR-10 32 ˆ 32. All corresponding samples use the same initial noise.\n\n<span id=\"page-39-0\"></span>![](_page_39_Figure_1.jpeg)\n\nFigure 19: Uncurated samples from ImageNet 64 ˆ 64. All corresponding samples use the same initial noise.\n\n<span id=\"page-40-0\"></span>![](_page_40_Picture_1.jpeg)\n\n(a) EDM (FID=3.57)\n\n![](_page_40_Picture_3.jpeg)\n\n(b) CT with single-step generation (FID=16.00)\n\n![](_page_40_Picture_5.jpeg)\n\n(c) CT with two-step generation (FID=7.80)\n\nFigure 20: Uncurated samples from LSUN Bedroom 256 ˆ 256. All corresponding samples use the same initial noise.\n\n<span id=\"page-41-0\"></span>![](_page_41_Picture_1.jpeg)\n\n(a) EDM (FID=6.69)\n\n![](_page_41_Picture_3.jpeg)\n\n(b) CT with single-step generation (FID=20.70)\n\n![](_page_41_Picture_5.jpeg)\n\n(c) CT with two-step generation (FID=11.76)\n\nFigure 21: Uncurated samples from LSUN Cat 256 ˆ 256. All corresponding samples use the same initial noise.",
  "chunks": [
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_0",
      "content": "## Consistency Models\n\n#### Yang Song <sup>1</sup> Prafulla Dhariwal <sup>1</sup> Mark Chen <sup>1</sup> Ilya Sutskever <sup>1</sup>",
      "metadata": {
        "chunk_index": 0,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 132
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_1",
      "content": "## Abstract",
      "metadata": {
        "chunk_index": 1,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 11
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_2",
      "content": "Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose *consistency models*, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether",
      "metadata": {
        "chunk_index": 2,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 740
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_3",
      "content": ". Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-ofthe-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64 ˆ 64 for one-step generation. When trained in isolation, consistency models become a new family of generative models that can outperform existing one-step, non-adversarial generative models on standard benchmarks such as CIFAR-10, ImageNet 64 ˆ 64 and LSUN 256 ˆ 256.",
      "metadata": {
        "chunk_index": 3,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 635
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_4",
      "content": "## <span id=\"page-0-1\"></span>1. Introduction\n\nDiffusion models [\\(Sohl-Dickstein et al.,](#page-11-0) [2015;](#page-11-0) [Song & Er](#page-11-1)[mon,](#page-11-1) [2019;](#page-11-1) [2020;](#page-11-2) [Ho et al.,](#page-9-0) [2020;](#page-9-0) [Song et al.,](#page-11-3) [2021\\)](#page-11-3), also known as score-based generative models, have achieved unprecedented success across multiple fields, including image generation [\\(Dhariwal & Nichol,](#page-9-1) [2021;](#page-9-1) [Nichol et al.,](#page-10-0) [2021;](#page-10-0) [Ramesh et al.,](#page-10-1) [2022;](#page-10-1) [Saharia et al.,](#page-11-4) [2022;](#page-11-4) [Rombach](#page-11-5) [et al.,](#page-11-5) [2022\\)](#page-11-5), audio synthesis [\\(Kong et al.,](#page-10-2) [2020;](#page-10-2) [Chen et al.,](#page-9-2) [2021;](#page-9-2) [Popov et al.,](#page-10-3) [2021\\)](#page-10-3), and video generation [\\(Ho et al.,](#page-9-3)",
      "metadata": {
        "chunk_index": 4,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 902
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_5",
      "content": "*Proceedings of the* 40 th *International Conference on Machine Learning*, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).\n\n<span id=\"page-0-0\"></span>![](_page_0_Figure_9.jpeg)\n\nFigure 1: Given a Probability Flow (PF) ODE that smoothly converts data to noise, we learn to map any point (*e.g*., xt, xt <sup>1</sup> , and x<sup>T</sup> ) on the ODE trajectory to its origin (*e.g*., x0) for generative modeling. Models of these mappings are called consistency models, as their outputs are trained to be consistent for points on the same trajectory.",
      "metadata": {
        "chunk_index": 5,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 572
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_6",
      "content": "[2022b](#page-9-3)[;a\\)](#page-9-4). A key feature of diffusion models is the iterative sampling process which progressively removes noise from random initial vectors. This iterative process provides a flexible trade-off of compute and sample quality, as using extra compute for more iterations usually yields samples of better quality. It is also the crux of many zero-shot data editing capabilities of diffusion models, enabling them to solve challenging inverse problems ranging from image inpainting, colorization, stroke-guided image editing, to Computed Tomography and Magnetic Resonance Imaging [\\(Song & Ermon,](#page-11-1) [2019;](#page-11-1) [Song et al.,](#page-11-3) [2021;](#page-11-3) [2022;](#page-11-6) [2023;](#page-11-7) [Kawar](#page-10-4) [et al.,](#page-10-4) [2021;](#page-10-4) [2022;](#page-10-5) [Chung et al.,](#page-9-5) [2023;](#page-9-5) [Meng et al.,](#page-10-6) [2021\\)](#page-10-6)",
      "metadata": {
        "chunk_index": 6,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 914
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_7",
      "content": ". However, compared to single-step generative models like GANs [\\(Goodfellow et al.,](#page-9-6) [2014\\)](#page-9-6), VAEs [\\(Kingma & Welling,](#page-10-7) [2014;](#page-10-7) [Rezende et al.,](#page-10-8) [2014\\)](#page-10-8), or normalizing flows [\\(Dinh](#page-9-7) [et al.,](#page-9-7) [2015;](#page-9-7) [2017;](#page-9-8) [Kingma & Dhariwal,](#page-10-9) [2018\\)](#page-10-9), the iterative generation procedure of diffusion models typically requires 10–2000 times more compute for sample generation [\\(Song](#page-11-2) [& Ermon,](#page-11-2) [2020;](#page-11-2) [Ho et al.,](#page-9-0) [2020;](#page-9-0) [Song et al.,](#page-11-3) [2021;](#page-11-3) [Zhang](#page-11-8) [& Chen,](#page-11-8) [2022;](#page-11-8) [Lu et al.,](#page-10-10) [2022\\)](#page-10-10), causing slow inference and limited real-time applications.",
      "metadata": {
        "chunk_index": 7,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 830
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_8",
      "content": "Our objective is to create generative models that facilitate efficient, single-step generation without sacrificing important advantages of iterative sampling, such as trading compute for sample quality when necessary, as well as performing zero-shot data editing tasks. As illustrated in Fig. [1,](#page-0-0) we build on top of the probability flow (PF) ordinary differential equation (ODE) in continuous-time diffusion models [\\(Song et al.,](#page-11-3) [2021\\)](#page-11-3), whose trajectories smoothly transition\n\n<sup>1</sup>OpenAI, San Francisco, CA 94110, USA. Correspondence to: Yang Song <songyang@openai.com>.",
      "metadata": {
        "chunk_index": 8,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 619
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_9",
      "content": "<sup>1</sup>OpenAI, San Francisco, CA 94110, USA. Correspondence to: Yang Song <songyang@openai.com>.\n\nthe data distribution into a tractable noise distribution. We propose to learn a model that maps any point at any time step to the trajectory's starting point. A notable property of our model is self-consistency: points on the same trajectory map to the same initial point. We therefore refer to such models as **consistency models**. Consistency models allow us to generate data samples (initial points of ODE trajectories, e.g.,  $\\mathbf{x}_0$  in Fig. 1) by converting random noise vectors (endpoints of ODE trajectories, e.g.,  $\\mathbf{x}_T$  in Fig. 1) with only one network evaluation. Importantly, by chaining the outputs of consistency models at multiple time steps, we can improve sample quality and perform zero-shot data editing at the cost of more compute, similar to what iterative sampling enables for diffusion models.",
      "metadata": {
        "chunk_index": 9,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 938
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_10",
      "content": "To train a consistency model, we offer two methods based on enforcing the self-consistency property. The first method relies on using numerical ODE solvers and a pre-trained diffusion model to generate pairs of adjacent points on a PF ODE trajectory. By minimizing the difference between model outputs for these pairs, we can effectively distill a diffusion model into a consistency model, which allows generating high-quality samples with one network evaluation. By contrast, our second method eliminates the need for a pre-trained diffusion model altogether, allowing us to train a consistency model in isolation. This approach situates consistency models as an independent family of generative models. Importantly, neither approach necessitates adversarial training, and they both place minor constraints on the architecture, allowing the use of flexible neural networks for parameterizing consistency models.",
      "metadata": {
        "chunk_index": 10,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 912
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_11",
      "content": "We demonstrate the efficacy of consistency models on several image datasets, including CIFAR-10 (Krizhevsky et al., 2009), ImageNet  $64 \\times 64$  (Deng et al., 2009), and LSUN  $256 \\times 256$  (Yu et al., 2015). Empirically, we observe that as a distillation approach, consistency models outperform existing diffusion distillation methods like progressive distillation (Salimans & Ho, 2022) across a variety of datasets in few-step generation: On CIFAR-10, consistency models reach new state-of-the-art FIDs of 3.55 and 2.93 for one-step and two-step generation; on ImageNet  $64 \\times 64$ , it achieves record-breaking FIDs of 6.20 and 4.70 with one and two network evaluations respectively. When trained as standalone generative models, consistency models can match or surpass the quality of one-step samples from progressive distillation, despite having no access to pre-trained diffusion models",
      "metadata": {
        "chunk_index": 11,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 904
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_12",
      "content": ". They are also able to outperform many GANs, and existing non-adversarial, single-step generative models across multiple datasets. Furthermore, we show that consistency models can be used to perform a wide range of zero-shot data editing tasks, including image denoising, interpolation, inpainting, colorization, super-resolution, and stroke-guided image editing (SDEdit, Meng et al. (2021)).",
      "metadata": {
        "chunk_index": 12,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 393
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_13",
      "content": "#### <span id=\"page-1-3\"></span>2. Diffusion Models\n\nConsistency models are heavily inspired by the theory of continuous-time diffusion models (Song et al., 2021; Karras et al., 2022). Diffusion models generate data by progressively perturbing data to noise via Gaussian perturbations, then creating samples from noise via sequential denoising steps. Let  $p_{\\rm data}(\\mathbf{x})$  denote the data distribution. Diffusion models start by diffusing  $p_{\\rm data}(\\mathbf{x})$  with a stochastic differential equation (SDE) (Song et al., 2021)\n\n<span id=\"page-1-0\"></span>\n$$d\\mathbf{x}_t = \\boldsymbol{\\mu}(\\mathbf{x}_t, t) dt + \\sigma(t) d\\mathbf{w}_t, \\tag{1}$$",
      "metadata": {
        "chunk_index": 13,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 665
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_14",
      "content": "<span id=\"page-1-0\"></span>\n$$d\\mathbf{x}_t = \\boldsymbol{\\mu}(\\mathbf{x}_t, t) dt + \\sigma(t) d\\mathbf{w}_t, \\tag{1}$$\n\nwhere  $t \\in [0,T]$ , T>0 is a fixed constant,  $\\mu(\\cdot,\\cdot)$  and  $\\sigma(\\cdot)$  are the drift and diffusion coefficients respectively, and  $\\{\\mathbf{w}_t\\}_{t\\in[0,T]}$  denotes the standard Brownian motion. We denote the distribution of  $\\mathbf{x}_t$  as  $p_t(\\mathbf{x})$  and as a result  $p_0(\\mathbf{x}) \\equiv p_{\\text{data}}(\\mathbf{x})$ . A remarkable property of this SDE is the existence of an ordinary differential equation (ODE), dubbed the *Probability Flow (PF) ODE* by Song et al. (2021), whose solution trajectories sampled at t are distributed according to  $p_t(\\mathbf{x})$ :\n\n<span id=\"page-1-1\"></span>\n$$d\\mathbf{x}_{t} = \\left[ \\boldsymbol{\\mu}(\\mathbf{x}_{t}, t) - \\frac{1}{2} \\sigma(t)^{2} \\nabla \\log p_{t}(\\mathbf{x}_{t}) \\right] dt.$$\n (2)",
      "metadata": {
        "chunk_index": 14,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 904
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_15",
      "content": "<span id=\"page-1-1\"></span>\n$$d\\mathbf{x}_{t} = \\left[ \\boldsymbol{\\mu}(\\mathbf{x}_{t}, t) - \\frac{1}{2} \\sigma(t)^{2} \\nabla \\log p_{t}(\\mathbf{x}_{t}) \\right] dt.$$\n (2)\n\nHere  $\\nabla \\log p_t(\\mathbf{x})$  is the *score function* of  $p_t(\\mathbf{x})$ ; hence diffusion models are also known as *score-based generative models* (Song & Ermon, 2019; 2020; Song et al., 2021).",
      "metadata": {
        "chunk_index": 15,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 377
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_16",
      "content": "Typically, the SDE in Eq. (1) is designed such that  $p_T(\\mathbf{x})$  is close to a tractable Gaussian distribution  $\\pi(\\mathbf{x})$ . We hereafter adopt the settings in Karras et al. (2022), where  $\\mu(\\mathbf{x},t) = \\mathbf{0}$  and  $\\sigma(t) = \\sqrt{2t}$ . In this case, we have  $p_t(\\mathbf{x}) = p_{\\text{data}}(\\mathbf{x}) \\otimes \\mathcal{N}(\\mathbf{0}, t^2 \\mathbf{I})$ , where  $\\otimes$  denotes the convolution operation, and  $\\pi(\\mathbf{x}) = \\mathcal{N}(\\mathbf{0}, T^2 \\mathbf{I})$ . For sampling, we first train a *score model*  $s_{\\phi}(\\mathbf{x},t) \\approx \\nabla \\log p_t(\\mathbf{x})$  via *score matching* (Hyvärinen & Dayan, 2005; Vincent, 2011; Song et al., 2019; Song & Ermon, 2019; Ho et al., 2020), then plug it into Eq. (2) to obtain an empirical estimate of the PF ODE, which takes the form of\n\n<span id=\"page-1-2\"></span>\n$$\\frac{\\mathrm{d}\\mathbf{x}_t}{\\mathrm{d}t} = -t\\mathbf{s}_{\\phi}(\\mathbf{x}_t, t). \\tag{3}$$",
      "metadata": {
        "chunk_index": 16,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 956
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_17",
      "content": "<span id=\"page-1-2\"></span>\n$$\\frac{\\mathrm{d}\\mathbf{x}_t}{\\mathrm{d}t} = -t\\mathbf{s}_{\\phi}(\\mathbf{x}_t, t). \\tag{3}$$\n\nWe call Eq. (3) the *empirical PF ODE*. Next, we sample  $\\hat{\\mathbf{x}}_T \\sim \\pi = \\mathcal{N}(\\mathbf{0}, T^2 \\mathbf{I})$  to initialize the empirical PF ODE and solve it backwards in time with any numerical ODE solver, such as Euler (Song et al., 2020; 2021) and Heun solvers (Karras et al., 2022), to obtain the solution trajectory  $\\{\\hat{\\mathbf{x}}_t\\}_{t\\in[0,T]}$ . The resulting  $\\hat{\\mathbf{x}}_0$  can then be viewed as an approximate sample from the data distribution  $p_{\\text{data}}(\\mathbf{x})$ . To avoid numerical instability, one typically stops the solver at  $t=\\epsilon$ , where  $\\epsilon$  is a fixed small positive number, and accepts  $\\hat{\\mathbf{x}}_\\epsilon$  as the approximate sample. Following Karras et al. (2022), we rescale image pixel values to [-1,1], and set T=80,  $\\epsilon=0.002$ .",
      "metadata": {
        "chunk_index": 17,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 956
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_18",
      "content": "<span id=\"page-2-0\"></span>![](_page_2_Picture_1.jpeg)\n\nFigure 2: Consistency models are trained to map points on any trajectory of the PF ODE to the trajectory's origin.",
      "metadata": {
        "chunk_index": 18,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 170
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_19",
      "content": "Diffusion models are bottlenecked by their slow sampling speed. Clearly, using ODE solvers for sampling requires iterative evaluations of the score model sϕpx, tq, which is computationally costly. Existing methods for fast sampling include faster numerical ODE solvers [\\(Song et al.,](#page-11-13) [2020;](#page-11-13) [Zhang & Chen,](#page-11-8) [2022;](#page-11-8) [Lu et al.,](#page-10-10) [2022;](#page-10-10) [Dockhorn et al.,](#page-9-11) [2022\\)](#page-9-11), and distillation techniques [\\(Luhman & Luhman,](#page-10-13) [2021;](#page-10-13) [Sali](#page-11-10)[mans & Ho,](#page-11-10) [2022;](#page-11-10) [Meng et al.,](#page-10-14) [2022;](#page-10-14) [Zheng et al.,](#page-12-0) [2022\\)](#page-12-0). However, ODE solvers still need more than 10 evaluation steps to generate competitive samples",
      "metadata": {
        "chunk_index": 19,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 809
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_20",
      "content": ". However, ODE solvers still need more than 10 evaluation steps to generate competitive samples. Most distillation methods like [Luhman & Luhman](#page-10-13) [\\(2021\\)](#page-10-13) and [Zheng et al.](#page-12-0) [\\(2022\\)](#page-12-0) rely on collecting a large dataset of samples from the diffusion model prior to distillation, which itself is computationally expensive. To our best knowledge, the only distillation approach that does not suffer from this drawback is progressive distillation (PD, [Salimans & Ho](#page-11-10) [\\(2022\\)](#page-11-10)), with which we compare consistency models extensively in our experiments.",
      "metadata": {
        "chunk_index": 20,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 628
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_21",
      "content": "## <span id=\"page-2-2\"></span>3. Consistency Models\n\nWe propose consistency models, a new type of models that support single-step generation at the core of its design, while still allowing iterative generation for trade-offs between sample quality and compute, and zero-shot data editing. Consistency models can be trained in either the distillation mode or the isolation mode. In the former case, consistency models distill the knowledge of pre-trained diffusion models into a single-step sampler, significantly improving other distillation approaches in sample quality, while allowing zero-shot image editing applications. In the latter case, consistency models are trained in isolation, with no dependence on pretrained diffusion models. This makes them an independent new class of generative models.\n\nBelow we introduce the definition, parameterization, and sampling of consistency models, plus a brief discussion on their applications to zero-shot data editing.",
      "metadata": {
        "chunk_index": 21,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 966
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_22",
      "content": "Definition Given a solution trajectory txtu<sup>t</sup>Prϵ,T<sup>s</sup> of the PF ODE in Eq. [\\(2\\)](#page-1-1), we define the *consistency function* as f : pxt, tq ÞÑ xϵ. A consistency function has the property of *self-consistency*: its outputs are consistent for arbitrary pairs of pxt, tq that belong to the same PF ODE trajectory, *i.e*., fpxt, tq \" fpx<sup>t</sup> <sup>1</sup> , t<sup>1</sup> q for all t, t<sup>1</sup> P rϵ, Ts. As illustrated in Fig. [2,](#page-2-0) the goal of a *consistency model*, symbolized as fθ, is to estimate this consistency function f from data by learning to enforce the self-consistency property (details in Sections [4](#page-3-0) and [5\\)](#page-4-0). Note that a similar definition is used for neural flows [\\(Bilos et al.](#page-9-12) ˇ , [2021\\)](#page-9-12) in the context of neural ODEs [\\(Chen et al.,](#page-9-13) [2018\\)](#page-9-13). Compared to neural flows, however, we do not enforce consistency models to be invertible.",
      "metadata": {
        "chunk_index": 22,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 974
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_23",
      "content": "Parameterization For any consistency function fp¨, ¨q, we have fpxϵ, ϵq \" xϵ, *i.e*., fp¨, ϵq is an identity function. We call this constraint the *boundary condition*. All consistency models have to meet this boundary condition, as it plays a crucial role in the successful training of consistency models. This boundary condition is also the most confining architectural constraint on consistency models. For consistency models based on deep neural networks, we discuss two ways to implement this boundary condition *almost for free*. Suppose we have a free-form deep neural network Fθpx, tq whose output has the same dimensionality as x. The first way is to simply parameterize the consistency model as\n\n$$\\mathbf{f}_{\\theta}(\\mathbf{x},t) = \\begin{cases} \\mathbf{x} & t = \\epsilon \\\\ F_{\\theta}(\\mathbf{x},t) & t \\in (\\epsilon,T] \\end{cases}$$\n (4)\n\nThe second method is to parameterize the consistency model using skip connections, that is,",
      "metadata": {
        "chunk_index": 23,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 944
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_24",
      "content": "The second method is to parameterize the consistency model using skip connections, that is,\n\n<span id=\"page-2-1\"></span>\n$$f_{\\theta}(\\mathbf{x}, t) = c_{\\text{skip}}(t)\\mathbf{x} + c_{\\text{out}}(t)F_{\\theta}(\\mathbf{x}, t),$$\n (5)\n\nwhere cskipptq and coutptq are differentiable functions such that cskippϵq \" 1, and coutpϵq \" 0. This way, the consistency model is differentiable at t \" ϵ if Fθpx, tq, cskipptq, coutptq are all differentiable, which is critical for training continuous-time consistency models (Appendices [B.1](#page-17-0) and [B.2\\)](#page-21-0). The parameterization in Eq. [\\(5\\)](#page-2-1) bears strong resemblance to many successful diffusion models [\\(Karras et al.,](#page-10-12) [2022;](#page-10-12) [Balaji et al.,](#page-9-14) [2022\\)](#page-9-14), making it easier to borrow powerful diffusion model architectures for constructing consistency models. We therefore follow the second parameterization in all experiments.",
      "metadata": {
        "chunk_index": 24,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 948
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_25",
      "content": "Sampling With a well-trained consistency model fθp¨, ¨q, we can generate samples by sampling from the initial distribution xˆ<sup>T</sup> \" N p0, T<sup>2</sup>Iq and then evaluating the consistency model for xˆ<sup>ϵ</sup> \" fθpxˆ<sup>T</sup> , Tq. This involves only one forward pass through the consistency model and therefore *generates samples in a single step*. Importantly, one can also evaluate the consistency model multiple times by alternating denoising and noise injection steps for improved sample quality. Summarized in Algorithm [1,](#page-3-1) this *multistep* sampling procedure provides the flexibility to trade compute for sample quality. It also has important applications in zero-shot data editing. In practice, we find time points",
      "metadata": {
        "chunk_index": 25,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 751
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_26",
      "content": "#### <span id=\"page-3-1\"></span>Algorithm 1 Multistep Consistency Sampling\n\n```\nInput: Consistency model f_{\\theta}(\\cdot,\\cdot), sequence of time points \\tau_1 > \\tau_2 > \\cdots > \\tau_{N-1}, initial noise \\hat{\\mathbf{x}}_T \\mathbf{x} \\leftarrow f_{\\theta}(\\hat{\\mathbf{x}}_T,T) for n=1 to N-1 do Sample \\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I}) \\hat{\\mathbf{x}}_{\\tau_n} \\leftarrow \\mathbf{x} + \\sqrt{\\tau_n^2 - \\epsilon^2}\\mathbf{z} \\mathbf{x} \\leftarrow f_{\\theta}(\\hat{\\mathbf{x}}_{\\tau_n},\\tau_n) end for Output: \\mathbf{x}\n```\n\n $\\{\\tau_1, \\tau_2, \\cdots, \\tau_{N-1}\\}$  in Algorithm 1 with a greedy algorithm, where the time points are pinpointed one at a time using ternary search to optimize the FID of samples obtained from Algorithm 1. This assumes that given prior time points, the FID is a unimodal function of the next time point. We find this assumption to hold empirically in our experiments, and leave the exploration of better strategies as future work.",
      "metadata": {
        "chunk_index": 26,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 982
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_27",
      "content": "**Zero-Shot Data Editing** Similar to diffusion models, consistency models enable various data editing and manipulation applications in zero shot; they do not require explicit training to perform these tasks. For example, consistency models define a one-to-one mapping from a Gaussian noise vector to a data sample. Similar to latent variable models like GANs, VAEs, and normalizing flows, consistency models can easily interpolate between samples by traversing the latent space (Fig. 11). As consistency models are trained to recover  $\\mathbf{x}_{\\epsilon}$  from any noisy input  $\\mathbf{x}_{t}$  where  $t \\in [\\epsilon, T]$ , they can perform denoising for various noise levels (Fig. 12). Moreover, the multistep generation procedure in Algorithm 1 is useful for solving certain inverse problems in zero shot by using an iterative replacement procedure similar to that of diffusion models (Song & Ermon, 2019; Song et al., 2021; Ho et al., 2022b)",
      "metadata": {
        "chunk_index": 27,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 952
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_28",
      "content": ". This enables many applications in the context of image editing, including inpainting (Fig. 10), colorization (Fig. 8), super-resolution (Fig. 6b) and stroke-guided image editing (Fig. 13) as in SDEdit (Meng et al., 2021). In Section 6.3, we empirically demonstrate the power of consistency models on many zero-shot image editing tasks.",
      "metadata": {
        "chunk_index": 28,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 337
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_29",
      "content": "#### <span id=\"page-3-0\"></span>4. Training Consistency Models via Distillation\n\nWe present our first method for training consistency models based on distilling a pre-trained score model  $s_{\\phi}(\\mathbf{x},t)$ . Our discussion revolves around the empirical PF ODE in Eq. (3), obtained by plugging the score model  $s_{\\phi}(\\mathbf{x},t)$  into the PF ODE. Consider discretizing the time horizon  $[\\epsilon,T]$  into N-1 sub-intervals, with boundaries  $t_1=\\epsilon < t_2 < \\cdots < t_N = T$ . In practice, we follow Karras et al. (2022) to determine the boundaries with the formula  $t_i = (\\epsilon^{1/\\rho} + i^{-1}/N - 1(T^{1/\\rho} - \\epsilon^{1/\\rho}))^{\\rho}$ , where  $\\rho = 7$ . When\n\nN is sufficiently large, we can obtain an accurate estimate of  $\\mathbf{x}_{t_n}$  from  $\\mathbf{x}_{t_{n+1}}$  by running one discretization step of a numerical ODE solver. This estimate, which we denote as  $\\hat{\\mathbf{x}}_{t_n}^{\\phi}$ , is defined by",
      "metadata": {
        "chunk_index": 29,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 957
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_30",
      "content": "<span id=\"page-3-2\"></span>\n$$\\hat{\\mathbf{x}}_{t_n}^{\\phi} := \\mathbf{x}_{t_{n+1}} + (t_n - t_{n+1}) \\Phi(\\mathbf{x}_{t_{n+1}}, t_{n+1}; \\phi), \\quad (6)$$\n\nwhere  $\\Phi(\\cdots; \\phi)$  represents the update function of a one-step ODE solver applied to the empirical PF ODE. For example, when using the Euler solver, we have  $\\Phi(\\mathbf{x}, t; \\phi) = -ts_{\\phi}(\\mathbf{x}, t)$  which corresponds to the following update rule\n\n$$\\hat{\\mathbf{x}}_{t_n}^{\\phi} = \\mathbf{x}_{t_{n+1}} - (t_n - t_{n+1})t_{n+1}\\mathbf{s}_{\\phi}(\\mathbf{x}_{t_{n+1}}, t_{n+1}).$$\n\nFor simplicity, we only consider one-step ODE solvers in this work. It is straightforward to generalize our framework to multistep ODE solvers and we leave it as future work.",
      "metadata": {
        "chunk_index": 30,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 738
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_31",
      "content": "Due to the connection between the PF ODE in Eq. (2) and the SDE in Eq. (1) (see Section 2), one can sample along the distribution of ODE trajectories by first sampling  $\\mathbf{x} \\sim p_{\\text{data}}$ , then adding Gaussian noise to  $\\mathbf{x}$ . Specifically, given a data point  $\\mathbf{x}$ , we can generate a pair of adjacent data points  $(\\hat{\\mathbf{x}}_{t_n}^{\\phi}, \\mathbf{x}_{t_{n+1}})$  on the PF ODE trajectory efficiently by sampling  $\\mathbf{x}$  from the dataset, followed by sampling  $\\mathbf{x}_{t_{n+1}}$  from the transition density of the SDE  $\\mathcal{N}(\\mathbf{x}, t_{n+1}^2 \\mathbf{I})$ , and then computing  $\\hat{\\mathbf{x}}_{t_n}^{\\phi}$  using one discretization step of the numerical ODE solver according to Eq. (6). Afterwards, we train the consistency model by minimizing its output differences on the pair  $(\\hat{\\mathbf{x}}_{t_n}^{\\phi}, \\mathbf{x}_{t_{n+1}})$",
      "metadata": {
        "chunk_index": 31,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 904
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_32",
      "content": ". (6). Afterwards, we train the consistency model by minimizing its output differences on the pair  $(\\hat{\\mathbf{x}}_{t_n}^{\\phi}, \\mathbf{x}_{t_{n+1}})$ . This motivates our following *consistency distillation* loss for training consistency models.",
      "metadata": {
        "chunk_index": 32,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 251
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_33",
      "content": "<span id=\"page-3-3\"></span>**Definition 1.** The consistency distillation loss is defined as\n\n$$\\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}; \\boldsymbol{\\phi}) := \\mathbb{E}[\\lambda(t_n) d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}), \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t}^{\\boldsymbol{\\phi}}, t_n))], \\quad (7)$$",
      "metadata": {
        "chunk_index": 33,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 382
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_34",
      "content": "where the expectation is taken with respect to  $\\mathbf{x} \\sim p_{data}$ ,  $n \\sim \\mathcal{U}[\\![1,N-1]\\!]$ , and  $\\mathbf{x}_{t_{n+1}} \\sim \\mathcal{N}(\\mathbf{x};t_{n+1}^2\\mathbf{I})$ . Here  $\\mathcal{U}[\\![1,N-1]\\!]$  denotes the uniform distribution over  $\\{1,2,\\cdots,N-1\\}$ ,  $\\lambda(\\cdot) \\in \\mathbb{R}^+$  is a positive weighting function,  $\\hat{\\mathbf{x}}_{t_n}^{\\boldsymbol{\\phi}}$  is given by Eq. (6),  $\\boldsymbol{\\theta}^-$  denotes a running average of the past values of  $\\boldsymbol{\\theta}$  during the course of optimization, and  $d(\\cdot,\\cdot)$  is a metric function that satisfies  $\\forall \\mathbf{x},\\mathbf{y}:d(\\mathbf{x},\\mathbf{y})\\geqslant 0$  and  $d(\\mathbf{x},\\mathbf{y})=0$  if and only if  $\\mathbf{x}=\\mathbf{y}$ .",
      "metadata": {
        "chunk_index": 34,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 765
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_35",
      "content": "Unless otherwise stated, we adopt the notations in Definition 1 throughout this paper, and use  $\\mathbb{E}[\\cdot]$  to denote the expectation over all random variables. In our experiments, we consider the squared  $\\ell_2$  distance  $d(\\mathbf{x}, \\mathbf{y}) = \\|\\mathbf{x} - \\mathbf{y}\\|_2^2$ ,  $\\ell_1$  distance  $d(\\mathbf{x}, \\mathbf{y}) = \\|\\mathbf{x} - \\mathbf{y}\\|_1$ , and the Learned Perceptual Image Patch Similarity (LPIPS, Zhang et al. (2018)). We find  $\\lambda(t_n) \\equiv 1$  performs well across all tasks and datasets. In practice, we minimize the objective by stochastic gradient descent on the model parameters  $\\theta$ , while updating  $\\theta^-$  with exponential moving average (EMA). That is, given a decay",
      "metadata": {
        "chunk_index": 35,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 736
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_36",
      "content": "#### <span id=\"page-4-1\"></span>Algorithm 2 Consistency Distillation (CD)",
      "metadata": {
        "chunk_index": 36,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 73
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_37",
      "content": "Input: dataset \n$$\\mathcal{D}$$",
      "metadata": {
        "chunk_index": 37,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 31
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_38",
      "content": ", initial model parameter  $\\boldsymbol{\\theta}$ , learning rate  $\\eta$ , ODE solver  $\\Phi(\\cdot,\\cdot;\\boldsymbol{\\phi}),d(\\cdot,\\cdot),\\lambda(\\cdot),$  and  $\\mu$   $\\boldsymbol{\\theta}^-\\leftarrow\\boldsymbol{\\theta}$  repeat Sample  $\\mathbf{x}\\sim\\mathcal{D}$  and  $n\\sim\\mathcal{U}[\\![1,N-1]\\!]$  Sample  $\\mathbf{x}_{t_{n+1}}\\sim\\mathcal{N}(\\mathbf{x};t_{n+1}^2\\boldsymbol{I})$   $\\hat{\\mathbf{x}}_{t_n}^{\\boldsymbol{\\phi}}\\leftarrow\\mathbf{x}_{t_{n+1}}+(t_n-t_{n+1})\\Phi(\\mathbf{x}_{t_{n+1}},t_{n+1};\\boldsymbol{\\phi})$   $\\mathcal{L}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^-;\\boldsymbol{\\phi})\\leftarrow$   $\\lambda(t_n)d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\boldsymbol{f}_{\\boldsymbol{\\theta}^-}(\\hat{\\mathbf{x}}_{t_n}^{\\boldsymbol{\\phi}},t_n))$   $\\boldsymbol{\\theta}\\leftarrow\\boldsymbol{\\theta}-\\eta\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^-;\\boldsymbol{\\phi})$",
      "metadata": {
        "chunk_index": 38,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 948
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_39",
      "content": "$\\boldsymbol{\\theta}\\leftarrow\\boldsymbol{\\theta}-\\eta\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^-;\\boldsymbol{\\phi})$   $\\boldsymbol{\\theta}^-\\leftarrow\\text{stopgrad}(\\mu\\boldsymbol{\\theta}^-+(1-\\mu)\\boldsymbol{\\theta})$  until convergence",
      "metadata": {
        "chunk_index": 39,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 278
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_40",
      "content": "rate  $0 \\le \\mu < 1$ , we perform the following update after each optimization step:\n\n$$\\boldsymbol{\\theta}^- \\leftarrow \\operatorname{stopgrad}(\\mu \\boldsymbol{\\theta}^- + (1 - \\mu)\\boldsymbol{\\theta}).$$\n (8)\n\nThe overall training procedure is summarized in Algorithm 2. In alignment with the convention in deep reinforcement learning (Mnih et al., 2013; 2015; Lillicrap et al., 2015) and momentum based contrastive learning (Grill et al., 2020; He et al., 2020), we refer to  $f_{\\theta^-}$  as the \"target network\", and  $f_{\\theta}$  as the \"online network\". We find that compared to simply setting  $\\theta^- = \\theta$ , the EMA update and \"stopgrad\" operator in Eq. (8) can greatly stabilize the training process and improve the final performance of the consistency model.\n\nBelow we provide a theoretical justification for consistency distillation based on asymptotic analysis.",
      "metadata": {
        "chunk_index": 40,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 885
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_41",
      "content": "Below we provide a theoretical justification for consistency distillation based on asymptotic analysis.\n\n<span id=\"page-4-3\"></span>**Theorem 1.** Let  $\\Delta t := \\max_{n \\in [\\![ 1,N-1]\\!]} \\{|t_{n+1}-t_n|\\}$ , and  $f(\\cdot,\\cdot;\\phi)$  be the consistency function of the empirical PF ODE in Eq. (3). Assume  $f_{\\theta}$  satisfies the Lipschitz condition: there exists L > 0 such that for all  $t \\in [\\epsilon,T]$ ,  $\\mathbf{x}$ , and  $\\mathbf{y}$ , we have  $\\|f_{\\theta}(\\mathbf{x},t) - f_{\\theta}(\\mathbf{y},t)\\|_2 \\leq L \\|\\mathbf{x} - \\mathbf{y}\\|_2$ . Assume further that for all  $n \\in [\\![ 1,N-1]\\!]$ , the ODE solver called at  $t_{n+1}$  has local error uniformly bounded by  $O((t_{n+1}-t_n)^{p+1})$  with  $p \\geq 1$ . Then, if  $\\mathcal{L}_{OD}^{N}(\\theta,\\theta;\\phi) = 0$ , we have\n\n$$\\sup_{n,\\mathbf{x}} \\|\\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{x},t_n) - \\mathbf{f}(\\mathbf{x},t_n;\\boldsymbol{\\phi})\\|_2 = O((\\Delta t)^p).$$",
      "metadata": {
        "chunk_index": 41,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 954
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_42",
      "content": "$$\\sup_{n,\\mathbf{x}} \\|\\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{x},t_n) - \\mathbf{f}(\\mathbf{x},t_n;\\boldsymbol{\\phi})\\|_2 = O((\\Delta t)^p).$$\n\n*Proof.* The proof is based on induction and parallels the classic proof of global error bounds for numerical ODE solvers (Süli & Mayers, 2003). We provide the full proof in Appendix A.2. □\n\nSince  $\\theta^-$  is a running average of the history of  $\\theta$ , we have  $\\theta^- = \\theta$  when the optimization of Algorithm 2 converges. That is, the target and online consistency models will eventually match each other. If the consistency model additionally achieves zero consistency distillation loss, then Theorem 1",
      "metadata": {
        "chunk_index": 42,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 666
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_43",
      "content": "#### <span id=\"page-4-4\"></span>Algorithm 3 Consistency Training (CT)",
      "metadata": {
        "chunk_index": 43,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 69
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_44",
      "content": "Input: dataset  $\\mathcal{D}$ , initial model parameter  $\\boldsymbol{\\theta}$ , learning rate  $\\eta$ , step schedule  $N(\\cdot)$ , EMA decay rate schedule  $\\mu(\\cdot)$ ,  $d(\\cdot,\\cdot)$ , and  $\\lambda(\\cdot)$   $\\boldsymbol{\\theta}^- \\leftarrow \\boldsymbol{\\theta}$  and  $k \\leftarrow 0$  repeat Sample  $\\mathbf{z} \\sim \\mathcal{D}$ , and  $n \\sim \\mathcal{U}[\\![1,N(k)-1]\\!]$  Sample  $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0},\\boldsymbol{I})$   $\\mathcal{L}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^-) \\leftarrow \\lambda(t_n)d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1}),\\boldsymbol{f}_{\\boldsymbol{\\theta}^-}(\\mathbf{x}+t_n\\mathbf{z},t_n))$   $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\eta\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^-)$   $\\boldsymbol{\\theta}^- \\leftarrow \\operatorname{stopgrad}(\\mu(k)\\boldsymbol{\\theta}^- + (1-\\mu(k))\\boldsymbol{\\theta})$   $k \\leftarrow k+1$  until convergence",
      "metadata": {
        "chunk_index": 44,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 985
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_45",
      "content": "<span id=\"page-4-2\"></span>implies that, under some regularity conditions, the estimated consistency model can become arbitrarily accurate, as long as the step size of the ODE solver is sufficiently small. Importantly, our boundary condition  $f_{\\theta}(\\mathbf{x}, \\epsilon) \\equiv \\mathbf{x}$  precludes the trivial solution  $f_{\\theta}(\\mathbf{x}, t) \\equiv \\mathbf{0}$  from arising in consistency model training.",
      "metadata": {
        "chunk_index": 45,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 419
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_46",
      "content": "The consistency distillation loss  $\\mathcal{L}_{\\mathrm{CD}}^N(\\theta, \\theta^-; \\phi)$  can be extended to hold for infinitely many time steps  $(N \\to \\infty)$  if  $\\theta^- = \\theta$  or  $\\theta^- = \\mathrm{stopgrad}(\\theta)$ . The resulting continuous-time loss functions do not require specifying N nor the time steps  $\\{t_1, t_2, \\cdots, t_N\\}$ . Nonetheless, they involve Jacobian-vector products and require forward-mode automatic differentiation for efficient implementation, which may not be well-supported in some deep learning frameworks. We provide these continuous-time distillation loss functions in Theorems 3 to 5, and relegate details to Appendix B.1.",
      "metadata": {
        "chunk_index": 46,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 673
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_47",
      "content": "#### <span id=\"page-4-0\"></span>5. Training Consistency Models in Isolation\n\nConsistency models can be trained without relying on any pre-trained diffusion models. This differs from existing diffusion distillation techniques, making consistency models a new independent family of generative models.\n\nRecall that in consistency distillation, we rely on a pretrained score model  $s_{\\phi}(\\mathbf{x},t)$  to approximate the ground truth score function  $\\nabla \\log p_t(\\mathbf{x})$ . It turns out that we can avoid this pre-trained score model altogether by leveraging the following unbiased estimator (Lemma 1 in Appendix A):\n\n$$\\nabla \\log p_t(\\mathbf{x}_t) = -\\mathbb{E}\\left[\\frac{\\mathbf{x}_t - \\mathbf{x}}{t^2} \\mid \\mathbf{x}_t\\right],$$\n\nwhere  $\\mathbf{x} \\sim p_{\\text{data}}$  and  $\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{x}; t^2 \\mathbf{I})$ . That is, given  $\\mathbf{x}$  and  $\\mathbf{x}_t$ , we can estimate  $\\nabla \\log p_t(\\mathbf{x}_t)$  with  $-(\\mathbf{x}_t - \\mathbf{x})/t^2$ .",
      "metadata": {
        "chunk_index": 47,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 999
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_48",
      "content": "This unbiased estimate suffices to replace the pre-trained diffusion model in consistency distillation when using the Euler method as the ODE solver in the limit of  $N \\to \\infty$ , as\n\njustified by the following result.\n\n<span id=\"page-5-2\"></span>**Theorem 2.** Let  $\\Delta t := \\max_{n \\in [\\![ 1,N-1 ]\\!]} \\{|t_{n+1}-t_n|\\}$ . Assume d and  $f_{\\theta^-}$  are both twice continuously differentiable with bounded second derivatives, the weighting function  $\\lambda(\\cdot)$  is bounded, and  $\\mathbb{E}[\\|\\nabla \\log p_{t_n}(\\mathbf{x}_{t_n})\\|_2^2] < \\infty$ . Assume further that we use the Euler ODE solver, and the pre-trained score model matches the ground truth, i.e.,  $\\forall t \\in [\\epsilon, T] : s_{\\phi}(\\mathbf{x}, t) \\equiv \\nabla \\log p_t(\\mathbf{x})$ . Then,\n\n$$\\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}; \\boldsymbol{\\phi}) = \\mathcal{L}_{CT}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}) + o(\\Delta t), \\tag{9}$$",
      "metadata": {
        "chunk_index": 48,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 964
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_49",
      "content": "$$\\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}; \\boldsymbol{\\phi}) = \\mathcal{L}_{CT}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}) + o(\\Delta t), \\tag{9}$$\n\nwhere the expectation is taken with respect to  $\\mathbf{x} \\sim p_{data}$ ,  $n \\sim \\mathcal{U}[\\![1,N-1]\\!]$ , and  $\\mathbf{x}_{t_{n+1}} \\sim \\mathcal{N}(\\mathbf{x};t_{n+1}^2\\mathbf{I})$ . The consistency training objective, denoted by  $\\mathcal{L}_{CT}^N(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^-)$ , is defined as\n\n$$\\mathbb{E}[\\lambda(t_n)d(\\mathbf{f}_{\\theta}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1}),\\mathbf{f}_{\\theta^-}(\\mathbf{x}+t_n\\mathbf{z},t_n))], (10)$$\n\nwhere  $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ . Moreover,  $\\mathcal{L}_{CT}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}) \\geqslant O(\\Delta t)$  if  $\\inf_{N} \\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}; \\boldsymbol{\\phi}) > 0$ .",
      "metadata": {
        "chunk_index": 49,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 925
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_50",
      "content": "*Proof.* The proof is based on Taylor series expansion and properties of score functions (Lemma 1). A complete proof is provided in Appendix A.3.\n\nWe refer to Eq. (10) as the *consistency training* (CT) loss. Crucially,  $\\mathcal{L}(\\theta, \\theta^-)$  only depends on the online network  $f_{\\theta}$ , and the target network  $f_{\\theta^-}$ , while being completely agnostic to diffusion model parameters  $\\phi$ . The loss function  $\\mathcal{L}(\\theta, \\theta^-) \\geqslant O(\\Delta t)$  decreases at a slower rate than the remainder  $o(\\Delta t)$  and thus will dominate the loss in Eq. (9) as  $N \\to \\infty$  and  $\\Delta t \\to 0$ .",
      "metadata": {
        "chunk_index": 50,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 640
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_51",
      "content": "For improved practical performance, we propose to progressively increase N during training according to a schedule function  $N(\\cdot)$ . The intuition (cf., Fig. 3d) is that the consistency training loss has less \"variance\" but more \"bias\" with respect to the underlying consistency distillation loss (i.e., the left-hand side of Eq. (9)) when N is small  $(i.e., \\Delta t$  is large), which facilitates faster convergence at the beginning of training. On the contrary, it has more \"variance\" but less \"bias\" when N is large  $(i.e., \\Delta t$  is small), which is desirable when closer to the end of training. For best performance, we also find that  $\\mu$  should change along with N, according to a schedule function  $\\mu(\\cdot)$ . The full algorithm of consistency training is provided in Algorithm 3, and the schedule functions used in our experiments are given in Appendix C.",
      "metadata": {
        "chunk_index": 51,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 883
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_52",
      "content": "Similar to consistency distillation, the consistency training loss  $\\mathcal{L}_{\\mathrm{CT}}^N(\\theta,\\theta^-)$  can be extended to hold in continuous time  $(i.e., N \\to \\infty)$  if  $\\theta^- = \\mathrm{stopgrad}(\\theta)$ , as shown in Theorem 6. This continuous-time loss function does not require schedule functions for N or  $\\mu$ , but requires forward-mode automatic differentiation for efficient implementation. Unlike the discrete-time CT loss, there is no undesirable \"bias\" associated with the continuous-time objective, as we effectively take  $\\Delta t \\to 0$  in Theorem 2. We relegate more details to Appendix B.2.",
      "metadata": {
        "chunk_index": 52,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 632
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_53",
      "content": "### <span id=\"page-5-3\"></span>6. Experiments\n\nWe employ consistency distillation and consistency training to learn consistency models on real image datasets, including CIFAR-10 (Krizhevsky et al., 2009), ImageNet 64 × 64 (Deng et al., 2009), LSUN Bedroom 256 × 256, and LSUN Cat 256 × 256 (Yu et al., 2015). Results are compared according to Fréchet Inception Distance (FID, Heusel et al. (2017), lower is better), Inception Score (IS, Salimans et al. (2016), higher is better), Precision (Prec., Kynkäänniemi et al. (2019), higher is better), and Recall (Rec., Kynkäänniemi et al. (2019), higher is better). Additional experimental details are provided in Appendix C.",
      "metadata": {
        "chunk_index": 53,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 669
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_54",
      "content": "#### <span id=\"page-5-4\"></span><span id=\"page-5-1\"></span><span id=\"page-5-0\"></span>**6.1. Training Consistency Models**\n\nWe perform a series of experiments on CIFAR-10 to understand the effect of various hyperparameters on the performance of consistency models trained by consistency distillation (CD) and consistency training (CT). We first focus on the effect of the metric function  $d(\\cdot, \\cdot)$ , the ODE solver, and the number of discretization steps N in CD, then investigate the effect of the schedule functions  $N(\\cdot)$  and  $\\mu(\\cdot)$  in CT.",
      "metadata": {
        "chunk_index": 54,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 565
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_55",
      "content": "To set up our experiments for CD, we consider the squared  $\\ell_2$  distance  $d(\\mathbf{x}, \\mathbf{y}) = \\|\\mathbf{x} - \\mathbf{y}\\|_2^2$ ,  $\\ell_1$  distance  $d(\\mathbf{x}, \\mathbf{y}) = \\|\\mathbf{x} - \\mathbf{y}\\|_1$ , and the Learned Perceptual Image Patch Similarity (LPIPS, Zhang et al. (2018)) as the metric function. For the ODE solver, we compare Euler's forward method and Heun's second order method as detailed in Karras et al. (2022). For the number of discretization steps N, we compare  $N \\in \\{9, 12, 18, 36, 50, 60, 80, 120\\}$ . All consistency models trained by CD in our experiments are initialized with the corresponding pre-trained diffusion models, whereas models trained by CT are randomly initialized.",
      "metadata": {
        "chunk_index": 55,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 729
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_56",
      "content": "As visualized in Fig. 3a, the optimal metric for CD is LPIPS, which outperforms both  $\\ell_1$  and  $\\ell_2$  by a large margin over all training iterations. This is expected as the outputs of consistency models are images on CIFAR-10, and LPIPS is specifically designed for measuring the similarity between natural images. Next, we investigate which ODE solver and which discretization step N work the best for CD. As shown in Figs. 3b and 3c, Heun ODE solver and N=18 are the best choices. Both are in line with the recommendation of Karras et al. (2022) despite the fact that we are training consistency models, not diffusion models. Moreover, Fig. 3b shows that with the same N, Heun's second order solver uniformly outperforms Euler's first order solver. This corroborates with Theorem 1, which states that the optimal consistency models trained by higher order ODE solvers have smaller estimation errors with the same N. The results of Fig",
      "metadata": {
        "chunk_index": 56,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 946
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_57",
      "content": ". This corroborates with Theorem 1, which states that the optimal consistency models trained by higher order ODE solvers have smaller estimation errors with the same N. The results of Fig. 3c also indicate that once N is sufficiently large, the performance of CD becomes insensitive to N. Given these insights, we hereafter use LPIPS and Heun ODE solver for CD unless otherwise stated. For N in CD, we follow the",
      "metadata": {
        "chunk_index": 57,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 412
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_58",
      "content": "<span id=\"page-6-0\"></span>![](_page_6_Figure_1.jpeg)\n\nFigure 3: Various factors that affect consistency distillation (CD) and consistency training (CT) on CIFAR-10. The best configuration for CD is LPIPS, Heun ODE solver, and N \" 18. Our adaptive schedule functions for N and µ make CT converge significantly faster than fixing them to be constants during the course of optimization.\n\n<span id=\"page-6-1\"></span>![](_page_6_Figure_3.jpeg)\n\nFigure 4: Multistep image generation with consistency distillation (CD). CD outperforms progressive distillation (PD) across all datasets and sampling steps. The only exception is single-step generation on Bedroom 256 ˆ 256.\n\nsuggestions in [Karras et al.](#page-10-12) [\\(2022\\)](#page-10-12) on CIFAR-10 and ImageNet 64 ˆ 64. We tune N separately on other datasets (details in Appendix [C\\)](#page-24-0).",
      "metadata": {
        "chunk_index": 58,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 847
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_59",
      "content": "Due to the strong connection between CD and CT, we adopt LPIPS for our CT experiments throughout this paper. Unlike CD, there is no need for using Heun's second order solver in CT as the loss function does not rely on any particular numerical ODE solver. As demonstrated in Fig. [3d,](#page-6-0) the convergence of CT is highly sensitive to N—smaller N leads to faster convergence but worse samples, whereas larger N leads to slower convergence but better samples upon convergence. This matches our analysis in Section [5,](#page-4-0) and motivates our practical choice of progressively growing N and µ for CT to balance the trade-off between convergence speed and sample quality. As shown in Fig. [3d,](#page-6-0) adaptive schedules of N and µ significantly improve the convergence speed and sample quality of CT. In our experiments, we tune the schedules Np¨q and µp¨q separately for images of different resolutions, with more details in Appendix [C.](#page-24-0)",
      "metadata": {
        "chunk_index": 59,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 965
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_60",
      "content": "#### <span id=\"page-6-2\"></span>6.2. Few-Step Image Generation",
      "metadata": {
        "chunk_index": 60,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 62
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_61",
      "content": "Distillation In current literature, the most directly comparable approach to our consistency distillation (CD) is progressive distillation (PD, [Salimans & Ho](#page-11-10) [\\(2022\\)](#page-11-10)); both are thus far the only distillation approaches that *do not construct synthetic data before distillation*. In stark contrast, other distillation techniques, such as knowledge distillation [\\(Luhman](#page-10-13) [& Luhman,](#page-10-13) [2021\\)](#page-10-13) and DFNO [\\(Zheng et al.,](#page-12-0) [2022\\)](#page-12-0), have to prepare a large synthetic dataset by generating numerous samples from the diffusion model with expensive numerical ODE/SDE solvers. We perform comprehensive comparison for PD and CD on CIFAR-10, ImageNet 64ˆ64, and LSUN 256 ˆ 256, with all results reported in Fig. [4.](#page-6-1) All methods distill from an EDM [\\(Karras et al.,](#page-10-12) [2022\\)](#page-10-12) model that we pretrained in-house",
      "metadata": {
        "chunk_index": 61,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 931
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_62",
      "content": ". [4.](#page-6-1) All methods distill from an EDM [\\(Karras et al.,](#page-10-12) [2022\\)](#page-10-12) model that we pretrained in-house. We note that across all sampling iterations, *using the LPIPS metric uniformly improves PD compared to the squared* ℓ<sup>2</sup> *distance in the original paper of [Salimans](#page-11-10) [& Ho](#page-11-10) [\\(2022\\)](#page-11-10)*. Both PD and CD improve as we take more sampling steps. We find that CD uniformly outperforms PD across all datasets, sampling steps, and metric functions considered, except for single-step generation on Bedroom 256 ˆ 256, where CD with ℓ<sup>2</sup> slightly underperforms PD with ℓ2. As shown in Table [1,](#page-7-0) CD even outperforms distillation approaches that require synthetic dataset construction, such as Knowledge Distillation [\\(Luhman & Luhman,](#page-10-13) [2021\\)](#page-10-13) and DFNO [\\(Zheng et al.,](#page-12-0) [2022\\)](#page-12-0).",
      "metadata": {
        "chunk_index": 62,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 929
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_63",
      "content": "Direct Generation In Tables [1](#page-7-0) and [2,](#page-7-0) we compare the sample quality of consistency training (CT) with other generative models using one-step and two-step generation. We also include PD and CD results for reference. Both tables report PD results obtained from the ℓ<sup>2</sup> metric function, as this is the default setting used in the original paper of [Salimans](#page-11-10)\n\nsynthetic data construction for distillation.\n\n<span id=\"page-7-0\"></span>Table 1: Sample quality on CIFAR-10. \\*Methods that require Table 2: Sample quality on ImageNet 64 × 64, and LSUN Bedroom & Cat  $256 \\times 256$ . †Distillation techniques.",
      "metadata": {
        "chunk_index": 63,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 652
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_64",
      "content": "| METHOD                                          | NFE (↓) | FID (↓) | IS (↑) | METHOD                                 | NFE (↓) | FID (↓) | Prec. (†) | Rec. (†) |\n|-------------------------------------------------|---------|---------|--------|----------------------------------------|---------|---------|-----------|----------|\n| Diffusion + Samplers                            |         |         |        | ImageNet $64 \\times 64$                |         |         |           |          |\n| DDIM (Song et al., 2020)                        | 50      | 4.67    |        | PD <sup>†</sup> (Salimans & Ho, 2022)  | 1       | 15.39   | 0.59      | 0.62     |\n| DDIM (Song et al., 2020)                        | 20      | 6.84    |        | DFNO <sup>†</sup> (Zheng et al., 2022) | 1       | 8.35    |           |          |\n| DDIM (Song et al., 2020)                        | 10      | 8.23    |        | $\\mathbf{C}\\mathbf{D}^{\\dagger}$       | 1       | 6.20    | 0.68      | 0.63     |",
      "metadata": {
        "chunk_index": 64,
        "content_type": "text",
        "has_table": true,
        "has_figure": false,
        "char_count": 989
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_65",
      "content": "| DDIM (Song et al., 2020)                        | 10      | 8.23    |        | $\\mathbf{C}\\mathbf{D}^{\\dagger}$       | 1       | 6.20    | 0.68      | 0.63     |\n| DPM-solver-2 (Lu et al., 2022)                  | 10      | 5.94    |        | PD <sup>†</sup> (Salimans & Ho, 2022)  | 2       | 8.95    | 0.63      | 0.65     |\n| DPM-solver-fast (Lu et al., 2022)               | 10      | 4.70    |        | $\\mathbf{C}\\mathbf{D}^{\\dagger}$       | 2       | 4.70    | 0.69      | 0.64     |\n| 3-DEIS (Zhang & Chen, 2022)                     | 10      | 4.17    |        | ADM (Dhariwal & Nichol, 2021)          | 250     | 2.07    | 0.74      | 0.63     |\n| Diffusion + Distillation                        |         |         |        | EDM (Karras et al., 2022)              | 79      | 2.44    | 0.71      | 0.67     |\n| Knowledge Distillation* (Luhman & Luhman, 2021) | 1       | 9.36    |        | BigGAN-deep (Brock et al., 2019)       | 1       | 4.06    | 0.79      | 0.48     |",
      "metadata": {
        "chunk_index": 65,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 989
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_66",
      "content": "| Knowledge Distillation* (Luhman & Luhman, 2021) | 1       | 9.36    |        | BigGAN-deep (Brock et al., 2019)       | 1       | 4.06    | 0.79      | 0.48     |\n| DFNO* (Zheng et al., 2022)                      | 1       | 4.12    |        | CT                                     | 1       | 13.0    | 0.71      | 0.47     |\n| 1-Rectified Flow (+distill)* (Liu et al., 2022) | 1       | 6.18    | 9.08   | CT                                     | 2       | 11.1    | 0.69      | 0.56     |\n| 2-Rectified Flow (+distill)* (Liu et al., 2022) | 1       | 4.85    | 9.01   |                                        |         |         |           |          |\n| 3-Rectified Flow (+distill)* (Liu et al., 2022) | 1       | 5.21    | 8.79   | LSUN Bedroom 256 × 256                 |         |         |           |          |\n| PD (Salimans & Ho, 2022)                        | 1       | 8.34    | 8.69   | PD <sup>†</sup> (Salimans & Ho, 2022)  | 1       | 16.92   | 0.47      | 0.27     |",
      "metadata": {
        "chunk_index": 66,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 989
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_67",
      "content": "| PD (Salimans & Ho, 2022)                        | 1       | 8.34    | 8.69   | PD <sup>†</sup> (Salimans & Ho, 2022)  | 1       | 16.92   | 0.47      | 0.27     |\n| CD                                              | 1       | 3.55    | 9.48   | PD <sup>†</sup> (Salimans & Ho, 2022)  | 2       | 8.47    | 0.56      | 0.39     |\n| PD (Salimans & Ho, 2022)                        | 2       | 5.58    | 9.05   | $\\mathbf{C}\\mathbf{D}^{\\dagger}$       | 1       | 7.80    | 0.66      | 0.34     |\n| CD                                              | 2       | 2.93    | 9.75   | CD <sup>†</sup>                        | 2       | 5.22    | 0.68      | 0.39     |\n| Direct Generation                               |         |         |        | DDPM (Ho et al., 2020)                 | 1000    | 4.89    | 0.60      | 0.45     |\n| BigGAN (Brock et al., 2019)                     | 1       | 14.7    | 9.22   | ADM (Dhariwal & Nichol, 2021)          | 1000    | 1.90    | 0.66      | 0.51     |",
      "metadata": {
        "chunk_index": 67,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 989
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_68",
      "content": "| BigGAN (Brock et al., 2019)                     | 1       | 14.7    | 9.22   | ADM (Dhariwal & Nichol, 2021)          | 1000    | 1.90    | 0.66      | 0.51     |\n| Diffusion GAN (Xiao et al., 2022)               | 1       | 14.6    | 8.93   | EDM (Karras et al., 2022)              | 79      | 3.57    | 0.66      | 0.45     |\n| AutoGAN (Gong et al., 2019)                     | 1       | 12.4    | 8.55   | PGGAN (Karras et al., 2018)            | 1       | 8.34    |           |          |\n| E2GAN (Tian et al., 2020)                       | 1       | 11.3    | 8.51   | PG-SWGAN (Wu et al., 2019)             | 1       | 8.0     |           |          |\n| ViTGAN (Lee et al., 2021)                       | 1       | 6.66    | 9.30   | TDPM (GAN) (Zheng et al., 2023)        | 1       | 5.24    |           |          |\n| TransGAN (Jiang et al., 2021)                   | 1       | 9.26    | 9.05   | StyleGAN2 (Karras et al., 2020)        | 1       | 2.35    | 0.59      | 0.48     |",
      "metadata": {
        "chunk_index": 68,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 989
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_69",
      "content": "| TransGAN (Jiang et al., 2021)                   | 1       | 9.26    | 9.05   | StyleGAN2 (Karras et al., 2020)        | 1       | 2.35    | 0.59      | 0.48     |\n| StyleGAN2-ADA (Karras et al., 2020)             | 1       | 2.92    | 9.83   | CT                                     | 1       | 16.0    | 0.60      | 0.17     |\n| StyleGAN-XL (Sauer et al., 2022)                | 1       | 1.85    |        | CT                                     | 2       | 7.85    | 0.68      | 0.33     |\n| Score SDE (Song et al., 2021)                   | 2000    | 2.20    | 9.89   | <b>LSUN Cat 256</b> × <b>256</b>       |         |         |           |          |\n| DDPM (Ho et al., 2020)                          | 1000    | 3.17    | 9.46   |                                        | 1       | 20.6    | 0.51      | 0.25     |\n| LSGM (Vahdat et al., 2021)                      | 147     | 2.10    |        | PD <sup>†</sup> (Salimans & Ho, 2022)  | -       | 29.6    | 0.51      | 0.25     |",
      "metadata": {
        "chunk_index": 69,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 989
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_70",
      "content": "| LSGM (Vahdat et al., 2021)                      | 147     | 2.10    |        | PD <sup>†</sup> (Salimans & Ho, 2022)  | -       | 29.6    | 0.51      | 0.25     |\n| PFGM (Xu et al., 2022)                          | 110     | 2.35    | 9.68   | PD <sup>†</sup> (Salimans & Ho, 2022)  | 2       | 15.5    | 0.59      | 0.36     |\n| EDM (Karras et al., 2022)                       | 35      | 2.04    | 9.84   | CD <sup>†</sup>                        | 1       | 11.0    | 0.65      | 0.36     |\n| 1-Rectified Flow (Liu et al., 2022)             | 1       | 378     | 1.13   | $\\mathbf{C}\\mathbf{D}^{\\dagger}$       | 2       | 8.84    | 0.66      | 0.40     |\n| Glow (Kingma & Dhariwal, 2018)                  | 1       | 48.9    | 3.92   | DDPM (Ho et al., 2020)                 | 1000    | 17.1    | 0.53      | 0.48     |\n| Residual Flow (Chen et al., 2019)               | 1       | 46.4    |        | ADM (Dhariwal & Nichol, 2021)          | 1000    | 5.57    | 0.63      | 0.52     |",
      "metadata": {
        "chunk_index": 70,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 989
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_71",
      "content": "| Residual Flow (Chen et al., 2019)               | 1       | 46.4    |        | ADM (Dhariwal & Nichol, 2021)          | 1000    | 5.57    | 0.63      | 0.52     |\n| GLFlow (Xiao et al., 2019)                      | 1       | 44.6    |        | EDM (Karras et al., 2022)              | 79      | 6.69    | 0.70      | 0.43     |\n| DenseFlow (Grcić et al., 2021)                  | 1       | 34.9    |        | PGGAN (Karras et al., 2018)            | 1       | 37.5    |           |          |\n| DC-VAE (Parmar et al., 2021)                    | 1       | 17.9    | 8.20   | StyleGAN2 (Karras et al., 2020)        | 1       | 7.25    | 0.58      | 0.43     |\n| CT                                              | 1       | 8.70    | 8.49   | CT                                     | 1       | 20.7    | 0.56      | 0.23     |\n| CT                                              | 2       | 5.83    | 8.85   | CT                                     | 2       | 11.7    | 0.63      | 0.36     |",
      "metadata": {
        "chunk_index": 71,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 989
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_72",
      "content": "<span id=\"page-7-1\"></span>![](_page_7_Figure_4.jpeg)\n\nFigure 5: Samples generated by EDM (top), CT + single-step generation (middle), and CT + 2-step generation (Bottom). All corresponding images are generated from the same initial noise.\n\n<span id=\"page-8-0\"></span>![](_page_8_Figure_1.jpeg)\n\n(a) *Left*: The gray-scale image. *Middle*: Colorized images. *Right*: The ground-truth image.\n\n![](_page_8_Figure_3.jpeg)\n\n(b) *Left*: The downsampled image (32 ˆ 32). *Middle*: Full resolution images (256 ˆ 256). *Right*: The ground-truth image (256 ˆ 256).\n\n![](_page_8_Figure_5.jpeg)\n\n(c) *Left*: A stroke input provided by users. *Right*: Stroke-guided image generation.\n\nFigure 6: Zero-shot image editing with a consistency model trained by consistency distillation on LSUN Bedroom 256ˆ256.",
      "metadata": {
        "chunk_index": 72,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 792
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_73",
      "content": "[& Ho](#page-11-10) [\\(2022\\)](#page-11-10). For fair comparison, we ensure PD and CD distill the same EDM models. In Tables [1](#page-7-0) and [2,](#page-7-0) we observe that CT outperforms existing single-step, non-adversarial generative models, *i.e*., VAEs and normalizing flows, by a significant margin on CIFAR-10. Moreover, *CT achieves comparable quality to one-step samples from PD without relying on distillation*. In Fig. [5,](#page-7-1) we provide EDM samples (top), single-step CT samples (middle), and two-step CT samples (bottom). In Appendix [E,](#page-27-0) we show additional samples for both CD and CT in Figs. [14](#page-34-0) to [21.](#page-41-0) Importantly, *all samples obtained from the same initial noise vector share significant structural similarity*, even though CT and EDM models are trained independently from one another. This indicates that CT is less likely to suffer from mode collapse, as EDMs do not.",
      "metadata": {
        "chunk_index": 73,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 937
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_74",
      "content": "#### <span id=\"page-8-1\"></span>6.3. Zero-Shot Image Editing",
      "metadata": {
        "chunk_index": 74,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 60
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_75",
      "content": "Similar to diffusion models, consistency models allow zeroshot image editing by modifying the multistep sampling process in Algorithm [1.](#page-3-1) We demonstrate this capability with a consistency model trained on the LSUN bedroom dataset using consistency distillation. In Fig. [6a,](#page-8-0) we show such a consistency model can colorize gray-scale bedroom images at test time, even though it has never been trained on colorization tasks. In Fig. [6b,](#page-8-0) we show the same consistency model can generate high-resolution images from low-resolution inputs. In Fig. [6c,](#page-8-0) we additionally demonstrate that it can generate images based on stroke inputs created by humans, as in SDEdit for diffusion models [\\(Meng](#page-10-6) [et al.,](#page-10-6) [2021\\)](#page-10-6). Again, this editing capability is zero-shot, as the model has not been trained on stroke inputs. In Appendix [D,](#page-25-0) we additionally demonstrate the zero-shot",
      "metadata": {
        "chunk_index": 75,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 959
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_76",
      "content": "capability of consistency models on inpainting (Fig. [10\\)](#page-30-0), interpolation (Fig. [11\\)](#page-31-0) and denoising (Fig. [12\\)](#page-32-0), with more examples on colorization (Fig. [8\\)](#page-28-0), super-resolution (Fig. [9\\)](#page-29-0) and stroke-guided image generation (Fig. [13\\)](#page-33-0).",
      "metadata": {
        "chunk_index": 76,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 313
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_77",
      "content": "## <span id=\"page-8-2\"></span>7. Conclusion\n\nWe have introduced consistency models, a type of generative models that are specifically designed to support one-step and few-step generation. We have empirically demonstrated that our consistency distillation method outshines the existing distillation techniques for diffusion models on multiple image benchmarks and small sampling iterations. Furthermore, as a standalone generative model, consistency models generate better samples than existing single-step generation models except for GANs. Similar to diffusion models, they also allow zero-shot image editing applications such as inpainting, colorization, super-resolution, denoising, interpolation, and stroke-guided image generation.",
      "metadata": {
        "chunk_index": 77,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 736
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_78",
      "content": "In addition, consistency models share striking similarities with techniques employed in other fields, including deep Q-learning [\\(Mnih et al.,](#page-10-16) [2015\\)](#page-10-16) and momentum-based contrastive learning [\\(Grill et al.,](#page-9-15) [2020;](#page-9-15) [He et al.,](#page-9-16) [2020\\)](#page-9-16). This offers exciting prospects for cross-pollination of ideas and methods among these diverse fields.",
      "metadata": {
        "chunk_index": 78,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 418
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_79",
      "content": "## Acknowledgements\n\nWe thank Alex Nichol for reviewing the manuscript and providing valuable feedback, Chenlin Meng for providing stroke inputs needed in our stroke-guided image generation experiments, and the OpenAI Algorithms team.",
      "metadata": {
        "chunk_index": 79,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 234
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_80",
      "content": "## References",
      "metadata": {
        "chunk_index": 80,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 13
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_81",
      "content": "- <span id=\"page-9-14\"></span>Balaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Kreis, K., Aittala, M., Aila, T., Laine, S., Catanzaro, B., Karras, T., and Liu, M.-Y. ediff-i: Text-to-image diffusion models with ensemble of expert denoisers. *arXiv preprint arXiv:2211.01324*, 2022.\n- <span id=\"page-9-12\"></span>Bilos, M., Sommer, J., Rangapuram, S. S., Januschowski, T., ˇ and Gunnemann, S. Neural flows: Efficient alternative to ¨ neural odes. *Advances in Neural Information Processing Systems*, 34:21325–21337, 2021.\n- <span id=\"page-9-18\"></span>Brock, A., Donahue, J., and Simonyan, K. Large scale GAN training for high fidelity natural image synthesis. In *International Conference on Learning Representations*, 2019. URL [https://openreview.net/forum?](https://openreview.net/forum?id=B1xsqj09Fm) [id=B1xsqj09Fm](https://openreview.net/forum?id=B1xsqj09Fm).",
      "metadata": {
        "chunk_index": 81,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 870
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_82",
      "content": "- <span id=\"page-9-2\"></span>Chen, N., Zhang, Y., Zen, H., Weiss, R. J., Norouzi, M., and Chan, W. Wavegrad: Estimating gradients for waveform generation. In *International Conference on Learning Representations (ICLR)*, 2021.\n- <span id=\"page-9-13\"></span>Chen, R. T., Rubanova, Y., Bettencourt, J., and Duvenaud, D. K. Neural Ordinary Differential Equations. In *Advances in neural information processing systems*, pp. 6571–6583, 2018.\n- <span id=\"page-9-20\"></span>Chen, R. T., Behrmann, J., Duvenaud, D. K., and Jacobsen, J.-H. Residual flows for invertible generative modeling. In *Advances in Neural Information Processing Systems*, pp. 9916–9926, 2019.",
      "metadata": {
        "chunk_index": 82,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 659
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_83",
      "content": "- <span id=\"page-9-5\"></span>Chung, H., Kim, J., Mccann, M. T., Klasky, M. L., and Ye, J. C. Diffusion posterior sampling for general noisy inverse problems. In *International Conference on Learning Representations*, 2023. URL [https://openreview.](https://openreview.net/forum?id=OnD9zGAGT0k) [net/forum?id=OnD9zGAGT0k](https://openreview.net/forum?id=OnD9zGAGT0k).\n- <span id=\"page-9-9\"></span>Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In *2009 IEEE conference on computer vision and pattern recognition*, pp. 248–255. Ieee, 2009.\n- <span id=\"page-9-1\"></span>Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. *Advances in Neural Information Processing Systems (NeurIPS)*, 2021.\n- <span id=\"page-9-7\"></span>Dinh, L., Krueger, D., and Bengio, Y. NICE: Non-linear independent components estimation. *International Conference in Learning Representations Workshop Track*, 2015.",
      "metadata": {
        "chunk_index": 83,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 979
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_84",
      "content": "- <span id=\"page-9-7\"></span>Dinh, L., Krueger, D., and Bengio, Y. NICE: Non-linear independent components estimation. *International Conference in Learning Representations Workshop Track*, 2015.\n- <span id=\"page-9-8\"></span>Dinh, L., Sohl-Dickstein, J., and Bengio, S. Density estimation using real NVP. In *5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings*.",
      "metadata": {
        "chunk_index": 84,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 443
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_85",
      "content": "- OpenReview.net, 2017. URL [https://openreview.](https://openreview.net/forum?id=HkpbnH9lx) [net/forum?id=HkpbnH9lx](https://openreview.net/forum?id=HkpbnH9lx).\n- <span id=\"page-9-11\"></span>Dockhorn, T., Vahdat, A., and Kreis, K. Genie: Higherorder denoising diffusion solvers. *arXiv preprint arXiv:2210.05475*, 2022.\n- <span id=\"page-9-19\"></span>Gong, X., Chang, S., Jiang, Y., and Wang, Z. Autogan: Neural architecture search for generative adversarial networks. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pp. 3224–3234, 2019.\n- <span id=\"page-9-6\"></span>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. In *Advances in neural information processing systems*, pp. 2672–2680, 2014.",
      "metadata": {
        "chunk_index": 85,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 815
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_86",
      "content": "- <span id=\"page-9-21\"></span>Grcic, M., Grubi ´ siˇ c, I., and ´ Segvi ˇ c, S. Densely connected ´ normalizing flows. *Advances in Neural Information Processing Systems*, 34:23968–23982, 2021.\n- <span id=\"page-9-15\"></span>Grill, J.-B., Strub, F., Altche, F., Tallec, C., Richemond, P., ´ Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., et al. Bootstrap your own latent-a new approach to self-supervised learning. *Advances in neural information processing systems*, 33:21271–21284, 2020.\n- <span id=\"page-9-16\"></span>He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Momentum contrast for unsupervised visual representation learning. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pp. 9729–9738, 2020.",
      "metadata": {
        "chunk_index": 86,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 775
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_87",
      "content": "- <span id=\"page-9-17\"></span>Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In *Advances in Neural Information Processing Systems*, pp. 6626–6637, 2017.\n- <span id=\"page-9-0\"></span>Ho, J., Jain, A., and Abbeel, P. Denoising Diffusion Probabilistic Models. *Advances in Neural Information Processing Systems*, 33, 2020.\n- <span id=\"page-9-4\"></span>Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J., et al. Imagen video: High definition video generation with diffusion models. *arXiv preprint arXiv:2210.02303*, 2022a.",
      "metadata": {
        "chunk_index": 87,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 698
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_88",
      "content": "- <span id=\"page-9-3\"></span>Ho, J., Salimans, T., Gritsenko, A. A., Chan, W., Norouzi, M., and Fleet, D. J. Video diffusion models. In *ICLR Workshop on Deep Generative Models for Highly Structured Data*, 2022b. URL [https://openreview.](https://openreview.net/forum?id=BBelR2NdDZ5) [net/forum?id=BBelR2NdDZ5](https://openreview.net/forum?id=BBelR2NdDZ5).\n- <span id=\"page-9-10\"></span>Hyvarinen, A. and Dayan, P. Estimation of non-normalized ¨ statistical models by score matching. *Journal of Machine Learning Research (JMLR)*, 6(4), 2005.",
      "metadata": {
        "chunk_index": 88,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 542
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_89",
      "content": "- <span id=\"page-10-21\"></span>Jiang, Y., Chang, S., and Wang, Z. Transgan: Two pure transformers can make one strong gan, and that can scale up. *Advances in Neural Information Processing Systems*, 34:14745–14758, 2021.\n- <span id=\"page-10-24\"></span>Karras, T., Aila, T., Laine, S., and Lehtinen, J. Progressive growing of GANs for improved quality, stability, and variation. In *International Conference on Learning Representations*, 2018. URL [https://openreview.](https://openreview.net/forum?id=Hk99zCeAb) [net/forum?id=Hk99zCeAb](https://openreview.net/forum?id=Hk99zCeAb).\n- <span id=\"page-10-22\"></span>Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., and Aila, T. Analyzing and improving the image quality of stylegan. 2020.\n- <span id=\"page-10-12\"></span>Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based generative models. In *Proc. NeurIPS*, 2022.",
      "metadata": {
        "chunk_index": 89,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 922
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_90",
      "content": "- <span id=\"page-10-12\"></span>Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based generative models. In *Proc. NeurIPS*, 2022.\n- <span id=\"page-10-4\"></span>Kawar, B., Vaksman, G., and Elad, M. Snips: Solving noisy inverse problems stochastically. *arXiv preprint arXiv:2105.14951*, 2021.\n- <span id=\"page-10-5\"></span>Kawar, B., Elad, M., Ermon, S., and Song, J. Denoising diffusion restoration models. In *Advances in Neural Information Processing Systems*, 2022.\n- <span id=\"page-10-9\"></span>Kingma, D. P. and Dhariwal, P. Glow: Generative flow with invertible 1x1 convolutions. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), *Advances in Neural Information Processing Systems 31*, pp. 10215–10224. 2018.\n- <span id=\"page-10-7\"></span>Kingma, D. P. and Welling, M. Auto-encoding variational bayes. In *International Conference on Learning Representations*, 2014.",
      "metadata": {
        "chunk_index": 90,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 964
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_91",
      "content": "- <span id=\"page-10-7\"></span>Kingma, D. P. and Welling, M. Auto-encoding variational bayes. In *International Conference on Learning Representations*, 2014.\n- <span id=\"page-10-2\"></span>Kong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro, B. DiffWave: A Versatile Diffusion Model for Audio Synthesis. *arXiv preprint arXiv:2009.09761*, 2020.\n- <span id=\"page-10-11\"></span>Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009.\n- <span id=\"page-10-18\"></span>Kynka¨anniemi, T., Karras, T., Laine, S., Lehtinen, J., and ¨ Aila, T. Improved precision and recall metric for assessing generative models. *Advances in Neural Information Processing Systems*, 32, 2019.\n- <span id=\"page-10-20\"></span>Lee, K., Chang, H., Jiang, L., Zhang, H., Tu, Z., and Liu, C. Vitgan: Training gans with vision transformers. *arXiv preprint arXiv:2107.04589*, 2021.",
      "metadata": {
        "chunk_index": 91,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 890
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_92",
      "content": "- <span id=\"page-10-20\"></span>Lee, K., Chang, H., Jiang, L., Zhang, H., Tu, Z., and Liu, C. Vitgan: Training gans with vision transformers. *arXiv preprint arXiv:2107.04589*, 2021.\n- <span id=\"page-10-17\"></span>Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. *arXiv preprint arXiv:1509.02971*, 2015.",
      "metadata": {
        "chunk_index": 92,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 411
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_93",
      "content": "- <span id=\"page-10-25\"></span>Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and Han, J. On the variance of the adaptive learning rate and beyond. *arXiv preprint arXiv:1908.03265*, 2019.\n- <span id=\"page-10-19\"></span>Liu, X., Gong, C., and Liu, Q. Flow straight and fast: Learning to generate and transfer data with rectified flow. *arXiv preprint arXiv:2209.03003*, 2022.\n- <span id=\"page-10-10\"></span>Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. *arXiv preprint arXiv:2206.00927*, 2022.\n- <span id=\"page-10-13\"></span>Luhman, E. and Luhman, T. Knowledge distillation in iterative generative models for improved sampling speed. *arXiv preprint arXiv:2101.02388*, 2021.\n- <span id=\"page-10-6\"></span>Meng, C., Song, Y., Song, J., Wu, J., Zhu, J.-Y., and Ermon, S. Sdedit: Image synthesis and editing with stochastic differential equations. *arXiv preprint arXiv:2108.01073*, 2021.",
      "metadata": {
        "chunk_index": 93,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 998
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_94",
      "content": "- <span id=\"page-10-14\"></span>Meng, C., Gao, R., Kingma, D. P., Ermon, S., Ho, J., and Salimans, T. On distillation of guided diffusion models. *arXiv preprint arXiv:2210.03142*, 2022.\n- <span id=\"page-10-15\"></span>Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing atari with deep reinforcement learning. *arXiv preprint arXiv:1312.5602*, 2013.\n- <span id=\"page-10-16\"></span>Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement learning. *nature*, 518(7540): 529–533, 2015.\n- <span id=\"page-10-0\"></span>Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., and Chen, M. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. *arXiv preprint arXiv:2112.10741*, 2021.",
      "metadata": {
        "chunk_index": 94,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 945
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_95",
      "content": "- <span id=\"page-10-23\"></span>Parmar, G., Li, D., Lee, K., and Tu, Z. Dual contradistinctive generative autoencoder. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 823–832, 2021.\n- <span id=\"page-10-3\"></span>Popov, V., Vovk, I., Gogoryan, V., Sadekova, T., and Kudinov, M. Grad-TTS: A diffusion probabilistic model for text-to-speech. *arXiv preprint arXiv:2105.06337*, 2021.\n- <span id=\"page-10-1\"></span>Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with clip latents. *arXiv preprint arXiv:2204.06125*, 2022.\n- <span id=\"page-10-8\"></span>Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic backpropagation and approximate inference in deep generative models. In *Proceedings of the 31st International Conference on Machine Learning*, pp. 1278–1286, 2014.",
      "metadata": {
        "chunk_index": 95,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 871
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_96",
      "content": "- <span id=\"page-11-5\"></span>Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 10684–10695, 2022.\n- <span id=\"page-11-4\"></span>Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S., Lopes, R. G., et al. Photorealistic text-to-image diffusion models with deep language understanding. *arXiv preprint arXiv:2205.11487*, 2022.\n- <span id=\"page-11-10\"></span>Salimans, T. and Ho, J. Progressive distillation for fast sampling of diffusion models. In *International Conference on Learning Representations*, 2022. URL [https:](https://openreview.net/forum?id=TIdIXIpzhoI) [//openreview.net/forum?id=TIdIXIpzhoI](https://openreview.net/forum?id=TIdIXIpzhoI).",
      "metadata": {
        "chunk_index": 96,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 885
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_97",
      "content": "- <span id=\"page-11-15\"></span>Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen, X. Improved techniques for training gans. In *Advances in neural information processing systems*, pp. 2234–2242, 2016.\n- <span id=\"page-11-18\"></span>Sauer, A., Schwarz, K., and Geiger, A. Stylegan-xl: Scaling stylegan to large diverse datasets. In *ACM SIGGRAPH 2022 conference proceedings*, pp. 1–10, 2022.\n- <span id=\"page-11-0\"></span>Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep Unsupervised Learning Using Nonequilibrium Thermodynamics. In *International Conference on Machine Learning*, pp. 2256–2265, 2015.\n- <span id=\"page-11-13\"></span>Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. *arXiv preprint arXiv:2010.02502*, 2020.",
      "metadata": {
        "chunk_index": 97,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 794
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_98",
      "content": "- <span id=\"page-11-13\"></span>Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. *arXiv preprint arXiv:2010.02502*, 2020.\n- <span id=\"page-11-7\"></span>Song, J., Vahdat, A., Mardani, M., and Kautz, J. Pseudoinverse-guided diffusion models for inverse problems. In *International Conference on Learning Representations*, 2023. URL [https://openreview.net/](https://openreview.net/forum?id=9_gsMA8MRKQ) [forum?id=9\\\\_gsMA8MRKQ](https://openreview.net/forum?id=9_gsMA8MRKQ).\n- <span id=\"page-11-1\"></span>Song, Y. and Ermon, S. Generative Modeling by Estimating Gradients of the Data Distribution. In *Advances in Neural Information Processing Systems*, pp. 11918–11930, 2019.\n- <span id=\"page-11-2\"></span>Song, Y. and Ermon, S. Improved Techniques for Training Score-Based Generative Models. *Advances in Neural Information Processing Systems*, 33, 2020.",
      "metadata": {
        "chunk_index": 98,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 874
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_99",
      "content": "- <span id=\"page-11-2\"></span>Song, Y. and Ermon, S. Improved Techniques for Training Score-Based Generative Models. *Advances in Neural Information Processing Systems*, 33, 2020.\n- <span id=\"page-11-12\"></span>Song, Y., Garg, S., Shi, J., and Ermon, S. Sliced score matching: A scalable approach to density and score estimation. In *Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 2019, Tel Aviv, Israel, July 22-25, 2019*, pp. 204, 2019.\n- <span id=\"page-11-3\"></span>Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. In *International Conference on Learning Representations*,",
      "metadata": {
        "chunk_index": 99,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 726
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_100",
      "content": "- 2021. URL [https://openreview.net/forum?](https://openreview.net/forum?id=PxTIG12RRHS) [id=PxTIG12RRHS](https://openreview.net/forum?id=PxTIG12RRHS).\n- <span id=\"page-11-6\"></span>Song, Y., Shen, L., Xing, L., and Ermon, S. Solving inverse problems in medical imaging with score-based generative models. In *International Conference on Learning Representations*, 2022. URL [https://openreview.](https://openreview.net/forum?id=vaRCHVj0uGI) [net/forum?id=vaRCHVj0uGI](https://openreview.net/forum?id=vaRCHVj0uGI).\n- <span id=\"page-11-14\"></span>Suli, E. and Mayers, D. F. ¨ *An introduction to numerical analysis*. Cambridge university press, 2003.\n- <span id=\"page-11-17\"></span>Tian, Y., Wang, Q., Huang, Z., Li, W., Dai, D., Yang, M., Wang, J., and Fink, O. Off-policy reinforcement learning for efficient and effective gan architecture search. In *Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VII 16*, pp. 175–192. Springer, 2020.",
      "metadata": {
        "chunk_index": 100,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 994
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_101",
      "content": "- <span id=\"page-11-19\"></span>Vahdat, A., Kreis, K., and Kautz, J. Score-based generative modeling in latent space. *Advances in Neural Information Processing Systems*, 34:11287–11302, 2021.\n- <span id=\"page-11-11\"></span>Vincent, P. A Connection Between Score Matching and Denoising Autoencoders. *Neural Computation*, 23(7): 1661–1674, 2011.\n- <span id=\"page-11-22\"></span>Wu, J., Huang, Z., Acharya, D., Li, W., Thoma, J., Paudel, D. P., and Gool, L. V. Sliced wasserstein generative models. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp. 3713– 3722, 2019.\n- <span id=\"page-11-21\"></span>Xiao, Z., Yan, Q., and Amit, Y. Generative latent flow. *arXiv preprint arXiv:1905.10485*, 2019.",
      "metadata": {
        "chunk_index": 101,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 733
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_102",
      "content": "- <span id=\"page-11-21\"></span>Xiao, Z., Yan, Q., and Amit, Y. Generative latent flow. *arXiv preprint arXiv:1905.10485*, 2019.\n- <span id=\"page-11-16\"></span>Xiao, Z., Kreis, K., and Vahdat, A. Tackling the generative learning trilemma with denoising diffusion GANs. In *International Conference on Learning Representations*, 2022. URL [https://openreview.net/forum?](https://openreview.net/forum?id=JprM0p-q0Co) [id=JprM0p-q0Co](https://openreview.net/forum?id=JprM0p-q0Co).\n- <span id=\"page-11-20\"></span>Xu, Y., Liu, Z., Tegmark, M., and Jaakkola, T. S. Poisson flow generative models. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), *Advances in Neural Information Processing Systems*, 2022. URL [https:](https://openreview.net/forum?id=voV_TRqcWh) [//openreview.net/forum?id=voV\\\\_TRqcWh](https://openreview.net/forum?id=voV_TRqcWh).",
      "metadata": {
        "chunk_index": 102,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 852
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_103",
      "content": "- <span id=\"page-11-9\"></span>Yu, F., Seff, A., Zhang, Y., Song, S., Funkhouser, T., and Xiao, J. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. *arXiv preprint arXiv:1506.03365*, 2015.\n- <span id=\"page-11-8\"></span>Zhang, Q. and Chen, Y. Fast sampling of diffusion models with exponential integrator. *arXiv preprint arXiv:2204.13902*, 2022.",
      "metadata": {
        "chunk_index": 103,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 390
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_104",
      "content": "- <span id=\"page-12-1\"></span>Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, O. The unreasonable effectiveness of deep features as a perceptual metric. In *CVPR*, 2018.\n- <span id=\"page-12-0\"></span>Zheng, H., Nie, W., Vahdat, A., Azizzadenesheli, K., and Anandkumar, A. Fast sampling of diffusion models via operator learning. *arXiv preprint arXiv:2211.13449*, 2022.\n- <span id=\"page-12-2\"></span>Zheng, H., He, P., Chen, W., and Zhou, M. Truncated diffusion probabilistic models and diffusion-based adversarial auto-encoders. In *The Eleventh International Conference on Learning Representations*, 2023. URL [https:](https://openreview.net/forum?id=HDxgaKk956l) [//openreview.net/forum?id=HDxgaKk956l](https://openreview.net/forum?id=HDxgaKk956l).",
      "metadata": {
        "chunk_index": 104,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 764
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_105",
      "content": "#### Consistency Models",
      "metadata": {
        "chunk_index": 105,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 23
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_106",
      "content": "## Contents",
      "metadata": {
        "chunk_index": 106,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 11
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_107",
      "content": "| 1 | Introduction                                                | 1  |\n|---|-------------------------------------------------------------|----|\n| 2 | Diffusion Models                                            | 2  |\n| 3 | Consistency Models                                          | 3  |\n| 4 | Training Consistency Models via Distillation                | 4  |\n| 5 | Training Consistency Models in Isolation                    | 5  |\n| 6 | Experiments                                                 | 6  |\n|   | 6.1<br>Training Consistency Models                          | 6  |\n|   | 6.2<br>Few-Step Image Generation<br>                        | 7  |\n|   | 6.3<br>Zero-Shot Image Editing<br>                          | 9  |\n| 7 | Conclusion                                                  | 9  |\n|   | Appendices                                                  | 15 |\n|   | Appendix A<br>Proofs                                        | 15 |",
      "metadata": {
        "chunk_index": 107,
        "content_type": "text",
        "has_table": true,
        "has_figure": false,
        "char_count": 948
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_108",
      "content": "|   | Appendices                                                  | 15 |\n|   | Appendix A<br>Proofs                                        | 15 |\n|   | A.1<br>Notations<br>                                        | 15 |\n|   | A.2<br>Consistency Distillation                             | 15 |\n|   | A.3<br>Consistency Training<br>                             | 16 |\n|   | Appendix B<br>Continuous-Time Extensions                    | 18 |\n|   | B.1<br>Consistency Distillation in Continuous Time          | 18 |\n|   | B.2<br>Consistency Training in Continuous Time<br>          | 22 |\n|   | B.3<br>Experimental Verifications<br>                       | 24 |\n|   | Appendix C<br>Additional Experimental Details               | 25 |\n|   | Model Architectures<br>                                     | 25 |\n|   | Parameterization for Consistency Models<br>                 | 25 |\n|   | Schedule Functions for Consistency Training<br>             | 26 |",
      "metadata": {
        "chunk_index": 108,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 948
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_109",
      "content": "|   | Parameterization for Consistency Models<br>                 | 25 |\n|   | Schedule Functions for Consistency Training<br>             | 26 |\n|   | Training Details                                            | 26 |\n|   | Appendix D<br>Additional Results on Zero-Shot Image Editing | 26 |\n|   | Inpainting                                                  | 27 |\n|   | Colorization<br>                                            | 27 |\n|   | Super-resolution<br>                                        | 28 |",
      "metadata": {
        "chunk_index": 109,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 510
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_110",
      "content": "| Appendix E | Additional Samples from Consistency Models | 28 |\n|------------|--------------------------------------------|----|\n|            | Interpolation                              | 28 |\n|            | Denoising                                  | 28 |\n|            | Stroke-guided image generation             | 28 |\n\n# <span id=\"page-14-2\"></span>**Appendices**",
      "metadata": {
        "chunk_index": 110,
        "content_type": "text",
        "has_table": true,
        "has_figure": false,
        "char_count": 370
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_111",
      "content": "#### <span id=\"page-14-1\"></span>A. Proofs",
      "metadata": {
        "chunk_index": 111,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 42
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_112",
      "content": "#### <span id=\"page-14-3\"></span>A.1. Notations\n\nWe use  $f_{\\theta}(\\mathbf{x},t)$  to denote a consistency model parameterized by  $\\theta$ , and  $f(\\mathbf{x},t;\\phi)$  the consistency function of the empirical PF ODE in Eq. (3). Here  $\\phi$  symbolizes its dependency on the pre-trained score model  $s_{\\phi}(\\mathbf{x},t)$ . For the consistency function of the PF ODE in Eq. (2), we denote it as  $f(\\mathbf{x},t)$ . Given a multi-variate function  $h(\\mathbf{x},\\mathbf{y})$ , we let  $\\partial_1 h(\\mathbf{x},\\mathbf{y})$  denote the Jacobian of h over  $\\mathbf{x}$ , and analogously  $\\partial_2 h(\\mathbf{x},\\mathbf{y})$  denote the Jacobian of h over h0. Unless otherwise stated, h1 is supposed to be a random variable sampled from the data distribution h2 h3, h4 is sampled uniformly at random from h5. Furthermore, recall that we define\n\n$$\\hat{\\mathbf{x}}_{t_n}^{\\phi} := \\mathbf{x}_{t_{n+1}} + (t_n - t_{n+1}) \\Phi(\\mathbf{x}_{t_{n+1}}, t_{n+1}; \\phi)$$",
      "metadata": {
        "chunk_index": 112,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 971
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_113",
      "content": "$$\\hat{\\mathbf{x}}_{t_n}^{\\phi} := \\mathbf{x}_{t_{n+1}} + (t_n - t_{n+1}) \\Phi(\\mathbf{x}_{t_{n+1}}, t_{n+1}; \\phi)$$\n\nwhere  $\\Phi(\\dots; \\phi)$  denotes the update function of a one-step ODE solver for the empirical PF ODE defined by the score model  $s_{\\phi}(\\mathbf{x}, t)$ . By default,  $\\mathbb{E}[\\cdot]$  denotes the expectation over all relevant random variables in the expression.",
      "metadata": {
        "chunk_index": 113,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 392
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_114",
      "content": "#### <span id=\"page-14-0\"></span>A.2. Consistency Distillation\n\n**Theorem 1.** Let  $\\Delta t := \\max_{n \\in [\\![1,N-1]\\!]} \\{|t_{n+1} - t_n|\\}$ , and  $f(\\cdot,\\cdot;\\phi)$  be the consistency function of the empirical PF ODE in Eq. (3). Assume  $f_{\\boldsymbol{\\theta}}$  satisfies the Lipschitz condition: there exists L > 0 such that for all  $t \\in [\\epsilon,T]$ ,  $\\mathbf{x}$ , and  $\\mathbf{y}$ , we have  $\\|f_{\\boldsymbol{\\theta}}(\\mathbf{x},t) - f_{\\boldsymbol{\\theta}}(\\mathbf{y},t)\\|_2 \\le L \\|\\mathbf{x} - \\mathbf{y}\\|_2$ . Assume further that for all  $n \\in [\\![1,N-1]\\!]$ , the ODE solver called at  $t_{n+1}$  has local error uniformly bounded by  $O((t_{n+1} - t_n)^{p+1})$  with  $p \\ge 1$ . Then, if  $\\mathcal{L}_{CD}^N(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}; \\phi) = 0$ , we have\n\n$$\\sup_{n,\\mathbf{x}} \\|\\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{x},t_n) - \\mathbf{f}(\\mathbf{x},t_n;\\boldsymbol{\\phi})\\|_2 = O((\\Delta t)^p).$$",
      "metadata": {
        "chunk_index": 114,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 951
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_115",
      "content": "$$\\sup_{n,\\mathbf{x}} \\|\\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{x},t_n) - \\mathbf{f}(\\mathbf{x},t_n;\\boldsymbol{\\phi})\\|_2 = O((\\Delta t)^p).$$\n\n*Proof.* From  $\\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}; \\boldsymbol{\\phi}) = 0$ , we have\n\n$$\\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}; \\boldsymbol{\\phi}) = \\mathbb{E}[\\lambda(t_n)d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}), \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\hat{\\mathbf{x}}_{t_n}^{\\boldsymbol{\\phi}}, t_n))] = 0.$$\n(11)\n\nAccording to the definition, we have  $p_{t_n}(\\mathbf{x}_{t_n}) = p_{\\text{data}}(\\mathbf{x}) \\otimes \\mathcal{N}(\\mathbf{0}, t_n^2 \\mathbf{I})$  where  $t_n \\ge \\epsilon > 0$ . It follows that  $p_{t_n}(\\mathbf{x}_{t_n}) > 0$  for every  $\\mathbf{x}_{t_n}$  and  $1 \\le n \\le N$ . Therefore, Eq. (11) entails\n\n$$\\lambda(t_n)d(\\mathbf{f}_{\\theta}(\\mathbf{x}_{t_{n+1}}, t_{n+1}), \\mathbf{f}_{\\theta}(\\hat{\\mathbf{x}}_{t_n}^{\\phi}, t_n)) \\equiv 0.$$\n(12)",
      "metadata": {
        "chunk_index": 115,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 994
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_116",
      "content": "$$\\lambda(t_n)d(\\mathbf{f}_{\\theta}(\\mathbf{x}_{t_{n+1}}, t_{n+1}), \\mathbf{f}_{\\theta}(\\hat{\\mathbf{x}}_{t_n}^{\\phi}, t_n)) \\equiv 0.$$\n(12)\n\nBecause  $\\lambda(\\cdot) > 0$  and  $d(\\mathbf{x}, \\mathbf{y}) = 0 \\Leftrightarrow \\mathbf{x} = \\mathbf{y}$ , this further implies that\n\n<span id=\"page-14-5\"></span><span id=\"page-14-4\"></span>\n$$\\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}) \\equiv \\mathbf{f}_{\\boldsymbol{\\theta}}(\\hat{\\mathbf{x}}_{t_n}^{\\boldsymbol{\\phi}}, t_n). \\tag{13}$$\n\nNow let  $e_n$  represent the error vector at  $t_n$ , which is defined as\n\n$$e_n := f_{\\theta}(\\mathbf{x}_{t_n}, t_n) - f(\\mathbf{x}_{t_n}, t_n; \\phi).$$\n\nWe can easily derive the following recursion relation\n\n$$e_{n+1} = f_{\\theta}(\\mathbf{x}_{t_{n+1}}, t_{n+1}) - f(\\mathbf{x}_{t_{n+1}}, t_{n+1}; \\phi)$$",
      "metadata": {
        "chunk_index": 116,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 812
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_117",
      "content": "We can easily derive the following recursion relation\n\n$$e_{n+1} = f_{\\theta}(\\mathbf{x}_{t_{n+1}}, t_{n+1}) - f(\\mathbf{x}_{t_{n+1}}, t_{n+1}; \\phi)$$\n\n$$\\stackrel{(i)}{=} \\mathbf{f}_{\\theta}(\\hat{\\mathbf{x}}_{t_{n}}^{\\phi}, t_{n}) - \\mathbf{f}(\\mathbf{x}_{t_{n}}, t_{n}; \\phi) \n= \\mathbf{f}_{\\theta}(\\hat{\\mathbf{x}}_{t_{n}}^{\\phi}, t_{n}) - \\mathbf{f}_{\\theta}(\\mathbf{x}_{t_{n}}, t_{n}) + \\mathbf{f}_{\\theta}(\\mathbf{x}_{t_{n}}, t_{n}) - \\mathbf{f}(\\mathbf{x}_{t_{n}}, t_{n}; \\phi) \n= \\mathbf{f}_{\\theta}(\\hat{\\mathbf{x}}_{t_{n}}^{\\phi}, t_{n}) - \\mathbf{f}_{\\theta}(\\mathbf{x}_{t_{n}}, t_{n}) + \\mathbf{e}_{n},$$\n(14)\n\nwhere (i) is due to Eq. (13) and  $f(\\mathbf{x}_{t_{n+1}}, t_{n+1}; \\phi) = f(\\mathbf{x}_{t_n}, t_n; \\phi)$ . Because  $f_{\\theta}(\\cdot, t_n)$  has Lipschitz constant L, we have",
      "metadata": {
        "chunk_index": 117,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 802
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_118",
      "content": "where (i) is due to Eq. (13) and  $f(\\mathbf{x}_{t_{n+1}}, t_{n+1}; \\phi) = f(\\mathbf{x}_{t_n}, t_n; \\phi)$ . Because  $f_{\\theta}(\\cdot, t_n)$  has Lipschitz constant L, we have\n\n<span id=\"page-15-2\"></span>\n$$\\begin{aligned} \\|\\boldsymbol{e}_{n+1}\\|_{2} &\\leq \\|\\boldsymbol{e}_{n}\\|_{2} + L \\|\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}} - \\mathbf{x}_{t_{n}}\\|_{2} \\\\ &\\stackrel{(i)}{=} \\|\\boldsymbol{e}_{n}\\|_{2} + L \\cdot O((t_{n+1} - t_{n})^{p+1}) \\\\ &= \\|\\boldsymbol{e}_{n}\\|_{2} + O((t_{n+1} - t_{n})^{p+1}), \\end{aligned}$$\n\nwhere (i) holds because the ODE solver has local error bounded by  $O((t_{n+1} - t_n)^{p+1})$ . In addition, we observe that  $e_1 = \\mathbf{0}$ , because",
      "metadata": {
        "chunk_index": 118,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 687
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_119",
      "content": "where (i) holds because the ODE solver has local error bounded by  $O((t_{n+1} - t_n)^{p+1})$ . In addition, we observe that  $e_1 = \\mathbf{0}$ , because\n\n$$egin{aligned} oldsymbol{e}_1 &= oldsymbol{f}_{oldsymbol{ heta}}(\\mathbf{x}_{t_1}, t_1) - oldsymbol{f}(\\mathbf{x}_{t_1}, t_1; oldsymbol{\\phi}) \\ &\\stackrel{(ii)}{=} \\mathbf{x}_{t_1} - oldsymbol{f}(\\mathbf{x}_{t_1}, t_1; oldsymbol{\\phi}) \\ &\\stackrel{(iii)}{=} \\mathbf{x}_{t_1} - \\mathbf{x}_{t_1} \\ &= oldsymbol{0} \\end{aligned}$$\n\nHere (i) is true because the consistency model is parameterized such that  $f(\\mathbf{x}_{t_1}, t_1; \\phi) = \\mathbf{x}_{t_1}$  and (ii) is entailed by the definition of  $f(\\cdot, \\cdot; \\phi)$ . This allows us to perform induction on the recursion formula Eq. (14) to obtain\n\n$$\\|\\boldsymbol{e}_{n}\\|_{2} \\leq \\|\\boldsymbol{e}_{1}\\|_{2} + \\sum_{k=1}^{n-1} O((t_{k+1} - t_{k})^{p+1})$$\n\n$$= \\sum_{k=1}^{n-1} O((t_{k+1} - t_{k})^{p+1})$$\n\n$$= \\sum_{k=1}^{n-1} (t_{k+1} - t_{k}) O((t_{k+1} - t_{k})^{p})$$",
      "metadata": {
        "chunk_index": 119,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 992
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_120",
      "content": "$$= \\sum_{k=1}^{n-1} O((t_{k+1} - t_{k})^{p+1})$$\n\n$$= \\sum_{k=1}^{n-1} (t_{k+1} - t_{k}) O((t_{k+1} - t_{k})^{p})$$\n\n$$\\leq \\sum_{k=1}^{n-1} (t_{k+1} - t_{k}) O((\\Delta t)^{p})$$\n\n$$= O((\\Delta t)^{p}) \\sum_{k=1}^{n-1} (t_{k+1} - t_{k})$$\n\n$$= O((\\Delta t)^{p}) (t_{n} - t_{1})$$\n\n$$\\leq O((\\Delta t)^{p}) (T - \\epsilon)$$\n\n$$= O((\\Delta t)^{p}),$$\n\nwhich completes the proof.",
      "metadata": {
        "chunk_index": 120,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 377
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_121",
      "content": "#### <span id=\"page-15-1\"></span>A.3. Consistency Training\n\nThe following lemma provides an unbiased estimator for the score function, which is crucial to our proof for Theorem 2.\n\n<span id=\"page-15-0\"></span>**Lemma 1.** Let \n$$\\mathbf{x} \\sim p_{data}(\\mathbf{x})$$\n,  $\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{x}; t^2 \\mathbf{I})$ , and  $p_t(\\mathbf{x}_t) = p_{data}(\\mathbf{x}) \\otimes \\mathcal{N}(\\mathbf{0}, t^2 \\mathbf{I})$ . We have  $\\nabla \\log p_t(\\mathbf{x}) = -\\mathbb{E}[\\frac{\\mathbf{x}_t - \\mathbf{x}}{t^2} \\mid \\mathbf{x}_t]$ .\n\n*Proof.* According to the definition of  $p_t(\\mathbf{x}_t)$ , we have  $\\nabla \\log p_t(\\mathbf{x}_t) = \\nabla_{\\mathbf{x}_t} \\log \\int p_{\\text{data}}(\\mathbf{x}) p(\\mathbf{x}_t \\mid \\mathbf{x}) \\, d\\mathbf{x}$ , where  $p(\\mathbf{x}_t \\mid \\mathbf{x}) = \\mathcal{N}(\\mathbf{x}_t; \\mathbf{x}, t^2 \\mathbf{I})$ . This expression can be further simplified to yield",
      "metadata": {
        "chunk_index": 121,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 908
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_122",
      "content": "$$\\nabla \\log p_t(\\mathbf{x}_t) = \\frac{\\int p_{\\text{data}}(\\mathbf{x}) \\nabla_{\\mathbf{x}_t} p(\\mathbf{x}_t \\mid \\mathbf{x}) \\, d\\mathbf{x}}{\\int p_{\\text{data}}(\\mathbf{x}) p(\\mathbf{x}_t \\mid \\mathbf{x}) \\, d\\mathbf{x}}$$",
      "metadata": {
        "chunk_index": 122,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 225
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_123",
      "content": "$$\\begin{split} &= \\frac{\\int p_{\\text{data}}(\\mathbf{x}) p(\\mathbf{x}_t \\mid \\mathbf{x}) \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t \\mid \\mathbf{x}) \\, d\\mathbf{x}}{\\int p_{\\text{data}}(\\mathbf{x}) p(\\mathbf{x}_t \\mid \\mathbf{x}) \\, d\\mathbf{x}} \\\\ &= \\frac{\\int p_{\\text{data}}(\\mathbf{x}) p(\\mathbf{x}_t \\mid \\mathbf{x}) \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t \\mid \\mathbf{x}) \\, d\\mathbf{x}}{p_t(\\mathbf{x}_t)} \\\\ &= \\int \\frac{p_{\\text{data}}(\\mathbf{x}) p(\\mathbf{x}_t \\mid \\mathbf{x})}{p_t(\\mathbf{x}_t)} \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t \\mid \\mathbf{x}) \\, d\\mathbf{x} \\\\ &\\stackrel{(i)}{=} \\int p(\\mathbf{x} \\mid \\mathbf{x}_t) \\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t \\mid \\mathbf{x}) \\, d\\mathbf{x} \\\\ &= \\mathbb{E}[\\nabla_{\\mathbf{x}_t} \\log p(\\mathbf{x}_t \\mid \\mathbf{x}) \\mid \\mathbf{x}_t] \\\\ &= -\\mathbb{E}\\left[\\frac{\\mathbf{x}_t - \\mathbf{x}}{t^2} \\mid \\mathbf{x}_t\\right], \\end{split}$$\n\n<span id=\"page-16-0\"></span>\n\nwhere (i) is due to Bayes' rule.",
      "metadata": {
        "chunk_index": 123,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 980
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_124",
      "content": "<span id=\"page-16-0\"></span>\n\nwhere (i) is due to Bayes' rule.\n\n**Theorem 2.** Let  $\\Delta t := \\max_{n \\in [\\![1,N-1]\\!]} \\{|t_{n+1} - t_n|\\}$ . Assume d and  $f_{\\theta^-}$  are both twice continuously differentiable with bounded second derivatives, the weighting function  $\\lambda(\\cdot)$  is bounded, and  $\\mathbb{E}[\\|\\nabla \\log p_{t_n}(\\mathbf{x}_{t_n})\\|_2^2] < \\infty$ . Assume further that we use the Euler ODE solver, and the pre-trained score model matches the ground truth, i.e.,  $\\forall t \\in [\\epsilon, T] : s_{\\phi}(\\mathbf{x}, t) \\equiv \\nabla \\log p_t(\\mathbf{x})$ . Then,\n\n$$\\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}; \\boldsymbol{\\phi}) = \\mathcal{L}_{CT}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}) + o(\\Delta t),$$",
      "metadata": {
        "chunk_index": 124,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 770
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_125",
      "content": "$$\\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}; \\boldsymbol{\\phi}) = \\mathcal{L}_{CT}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}) + o(\\Delta t),$$\n\nwhere the expectation is taken with respect to  $\\mathbf{x} \\sim p_{data}$ ,  $n \\sim \\mathcal{U}[1, N-1]$ , and  $\\mathbf{x}_{t_{n+1}} \\sim \\mathcal{N}(\\mathbf{x}; t_{n+1}^2 \\mathbf{I})$ . The consistency training objective, denoted by  $\\mathcal{L}_{CT}^N(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^-)$ , is defined as\n\n$$\\mathbb{E}[\\lambda(t_n)d(\\mathbf{f}_{\\theta}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1}),\\mathbf{f}_{\\theta^-}(\\mathbf{x}+t_n\\mathbf{z},t_n))],$$\n\nwhere  $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ . Moreover,  $\\mathcal{L}_{CT}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}) \\geqslant O(\\Delta t)$  if  $\\inf_{N} \\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}; \\boldsymbol{\\phi}) > 0$ .\n\n*Proof.* With Taylor expansion, we have",
      "metadata": {
        "chunk_index": 125,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 951
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_126",
      "content": "$$\\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}; \\boldsymbol{\\phi}) = \\mathbb{E}[\\lambda(t_{n})d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}), \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}}, t_{n})] \\\\\n= \\mathbb{E}[\\lambda(t_{n})d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}), \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}} + (t_{n+1} - t_{n})t_{n+1}\\nabla \\log p_{t_{n+1}}(\\mathbf{x}_{t_{n+1}}), t_{n}))] \\\\\n= \\mathbb{E}[\\lambda(t_{n})d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}), \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}) + \\partial_{1}\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}}, t_{n+1})(t_{n+1} - t_{n})t_{n+1}\\nabla \\log p_{t_{n+1}}(\\mathbf{x}_{t_{n+1}}) \\\\\n+ \\partial_{2}\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}}, t_{n+1})(t_{n} - t_{n+1}) + o(|t_{n+1} - t_{n}|)] \\\\",
      "metadata": {
        "chunk_index": 126,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 986
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_127",
      "content": "+ \\partial_{2}\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}}, t_{n+1})(t_{n} - t_{n+1}) + o(|t_{n+1} - t_{n}|)] \\\\\n= \\mathbb{E}\\{\\lambda(t_{n})d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}), \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}}, t_{n+1})) + \\lambda(t_{n})\\partial_{2}d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}), \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}))[\\\\\n\\partial_{1}\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}}, t_{n+1})(t_{n+1} - t_{n})t_{n+1}\\nabla \\log p_{t_{n+1}}(\\mathbf{x}_{t_{n+1}}) + \\partial_{2}\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}}, t_{n+1})(t_{n} - t_{n+1}) + o(|t_{n+1} - t_{n}|)]\\} \\\\\n= \\mathbb{E}[\\lambda(t_{n})d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}), \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}))] \\\\",
      "metadata": {
        "chunk_index": 127,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 936
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_128",
      "content": "= \\mathbb{E}[\\lambda(t_{n})d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}), \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}))] \\\\\n+ \\mathbb{E}\\{\\lambda(t_{n})\\partial_{2}d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}), \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}}, t_{n+1}))[\\partial_{1}\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}}, t_{n+1})(t_{n} - t_{n+1})]\\} + \\mathbb{E}[o(|t_{n+1} - t_{n}|)]. \\\\\n(15)$$",
      "metadata": {
        "chunk_index": 128,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 508
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_129",
      "content": "Then, we apply Lemma 1 to Eq. (15) and use Taylor expansion in the reverse direction to obtain",
      "metadata": {
        "chunk_index": 129,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 94
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_130",
      "content": "$$\\begin{split} &\\mathcal{L}_{\\mathrm{CD}}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{-};\\boldsymbol{\\phi}) \\\\ =& \\mathbb{E}\\big[\\lambda(t_{n})d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1}))\\big] \\\\ &+ \\mathbb{E}\\left\\{\\lambda(t_{n})\\partial_{2}d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1}))\\left[\\partial_{1}\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1})(t_{n}-t_{n+1})t_{n+1}\\mathbb{E}\\left[\\frac{\\mathbf{x}_{t_{n+1}}-\\mathbf{x}}{t_{n+1}^{2}}\\Big|\\mathbf{x}_{t_{n+1}}\\right]\\right]\\right\\} \\\\ &+ \\mathbb{E}\\{\\lambda(t_{n})\\partial_{2}d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1}))[\\partial_{2}\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1})(t_{n}-t_{n+1})]\\} +",
      "metadata": {
        "chunk_index": 130,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 992
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_131",
      "content": "+ \\mathbb{E}[o(|t_{n+1}-t_{n}|)] \\\\ \\stackrel{(i)}{=} \\mathbb{E}[\\lambda(t_{n})d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1}))] \\\\ &+ \\mathbb{E}\\left\\{\\lambda(t_{n})\\partial_{2}d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1}))\\left[\\partial_{1}\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1})(t_{n}-t_{n+1})t_{n+1}\\left(\\frac{\\mathbf{x}_{t_{n+1}}-\\mathbf{x}}{t_{n+1}^{2}}\\right)\\right]\\right\\} \\end{split}$$",
      "metadata": {
        "chunk_index": 131,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 613
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_132",
      "content": "$$+ \\mathbb{E}\\{\\lambda(t_{n})\\partial_{2}d(\\mathbf{f}_{\\theta}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\mathbf{f}_{\\theta^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1}))[\\partial_{2}\\mathbf{f}_{\\theta^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1})(t_{n}-t_{n+1})]\\} + \\mathbb{E}[o(|t_{n+1}-t_{n}|)]$$\n\n$$= \\mathbb{E}\\left[\\lambda(t_{n})d(\\mathbf{f}_{\\theta}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\mathbf{f}_{\\theta^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1}))\\right]$$\n\n$$+ \\lambda(t_{n})\\partial_{2}d(\\mathbf{f}_{\\theta}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\mathbf{f}_{\\theta^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1}))\\left[\\partial_{2}\\mathbf{f}_{\\theta^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1})(t_{n}-t_{n+1})t_{n+1}\\left(\\frac{\\mathbf{x}_{t_{n+1}}-\\mathbf{x}}{t_{n+1}^{2}}\\right)\\right]$$",
      "metadata": {
        "chunk_index": 132,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 722
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_133",
      "content": "$$+ \\lambda(t_{n})\\partial_{2}d(\\mathbf{f}_{\\theta}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\mathbf{f}_{\\theta^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1}))\\left[\\partial_{2}\\mathbf{f}_{\\theta^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1})(t_{n}-t_{n+1})t_{n+1}\\left(\\frac{\\mathbf{x}_{t_{n+1}}-\\mathbf{x}}{t_{n+1}^{2}}\\right)\\right]$$\n\n$$+ \\mathbb{E}[o(|t_{n+1}-t_{n}|)]$$\n\n$$= \\mathbb{E}\\left[\\lambda(t_{n})d\\left(\\mathbf{f}_{\\theta}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\mathbf{f}_{\\theta^{-}}\\left(\\mathbf{x}_{t_{n+1}}+(t_{n}-t_{n+1})t_{n+1}\\frac{\\mathbf{x}_{t_{n+1}}-\\mathbf{x}}{t_{n+1}^{2}},t_{n}\\right)\\right)\\right] + \\mathbb{E}[o(|t_{n+1}-t_{n}|)]$$\n\n$$= \\mathbb{E}\\left[\\lambda(t_{n})d\\left(\\mathbf{f}_{\\theta}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\mathbf{f}_{\\theta^{-}}\\left(\\mathbf{x}_{t_{n+1}}+(t_{n}-t_{n+1})\\frac{\\mathbf{x}_{t_{n+1}}-\\mathbf{x}}{t_{n+1}},t_{n}\\right)\\right)\\right] + \\mathbb{E}[o(|t_{n+1}-t_{n}|)]$$",
      "metadata": {
        "chunk_index": 133,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 885
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_134",
      "content": "$$= \\mathbb{E}\\left[\\lambda(t_{n})d\\left(\\mathbf{f}_{\\theta}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1}),\\mathbf{f}_{\\theta^{-}}\\left(\\mathbf{x}+t_{n+1}\\mathbf{z}+(t_{n}-t_{n+1})\\mathbf{z},t_{n}\\right)\\right)\\right] + \\mathbb{E}[o(|t_{n+1}-t_{n}|)]$$\n\n$$= \\mathbb{E}\\left[\\lambda(t_{n})d\\left(\\mathbf{f}_{\\theta}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1}),\\mathbf{f}_{\\theta^{-}}\\left(\\mathbf{x}+t_{n}\\mathbf{z},t_{n}\\right)\\right)\\right] + \\mathbb{E}[o(|t_{n+1}-t_{n}|)]$$\n\n$$= \\mathbb{E}\\left[\\lambda(t_{n})d\\left(\\mathbf{f}_{\\theta}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1}),\\mathbf{f}_{\\theta^{-}}\\left(\\mathbf{x}+t_{n}\\mathbf{z},t_{n}\\right)\\right)\\right] + \\mathbb{E}[o(|t_{n+1}-t_{n}|)]$$\n\n$$= \\mathbb{E}\\left[\\lambda(t_{n})d\\left(\\mathbf{f}_{\\theta}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1}),\\mathbf{f}_{\\theta^{-}}\\left(\\mathbf{x}+t_{n}\\mathbf{z},t_{n}\\right)\\right)\\right] + \\mathbb{E}[o(|t_{n+1}-t_{n}|)]$$",
      "metadata": {
        "chunk_index": 134,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 898
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_135",
      "content": "$$= \\mathbb{E}\\left[\\lambda(t_{n})d\\left(\\mathbf{f}_{\\theta}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1}),\\mathbf{f}_{\\theta^{-}}\\left(\\mathbf{x}+t_{n}\\mathbf{z},t_{n}\\right)\\right)\\right] + \\mathbb{E}[o(|t_{n+1}-t_{n}|)]$$",
      "metadata": {
        "chunk_index": 135,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 216
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_136",
      "content": "where (i) is due to the law of total expectation, and  $\\mathbf{z} := \\frac{\\mathbf{x}_{t_{n+1}} - \\mathbf{x}}{t_{n+1}} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ . This implies  $\\mathcal{L}_{\\mathrm{CD}}^N(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^-; \\boldsymbol{\\phi}) = \\mathcal{L}_{\\mathrm{CT}}^N(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^-) + o(\\Delta t)$  and thus completes the proof for Eq. (9). Moreover, we have  $\\mathcal{L}_{\\mathrm{CT}}^N(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^-) \\geqslant O(\\Delta t)$  whenever  $\\inf_N \\mathcal{L}_{\\mathrm{CD}}^N(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^-; \\boldsymbol{\\phi}) > 0$",
      "metadata": {
        "chunk_index": 136,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 631
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_137",
      "content": ". Otherwise,  $\\mathcal{L}_{\\mathrm{CT}}^N(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^-) < O(\\Delta t)$  and thus  $\\lim_{\\Delta t \\to 0} \\mathcal{L}_{\\mathrm{CD}}^N(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^-; \\boldsymbol{\\phi}) = 0$ , which is a clear contradiction to  $\\inf_N \\mathcal{L}_{\\mathrm{CD}}^N(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^-; \\boldsymbol{\\phi}) > 0$ .",
      "metadata": {
        "chunk_index": 137,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 374
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_138",
      "content": "**Remark 1.** When the condition  $\\mathcal{L}_{CT}^N(\\theta, \\theta^-) \\ge O(\\Delta t)$  is not satisfied, such as in the case where  $\\theta^- = \\text{stopgrad}(\\theta)$ , the validity of  $\\mathcal{L}_{CT}^N(\\theta, \\theta^-)$  as a training objective for consistency models can still be justified by referencing the result provided in Theorem 6.",
      "metadata": {
        "chunk_index": 138,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 349
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_139",
      "content": "#### <span id=\"page-17-2\"></span>**B.** Continuous-Time Extensions\n\nThe consistency distillation and consistency training objectives can be generalized to hold for infinite time steps  $(N \\to \\infty)$  under suitable conditions.",
      "metadata": {
        "chunk_index": 139,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 229
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_140",
      "content": "#### <span id=\"page-17-0\"></span>**B.1. Consistency Distillation in Continuous Time**\n\nDepending on whether  $\\theta^- = \\theta$  or  $\\theta^- = \\operatorname{stopgrad}(\\theta)$  (same as setting  $\\mu = 0$ ), there are two possible continuous-time extensions for the consistency distillation objective  $\\mathcal{L}_{\\mathrm{CD}}^N(\\theta, \\theta^-; \\phi)$ . Given a twice continuously differentiable metric function  $d(\\mathbf{x}, \\mathbf{y})$ , we define  $G(\\mathbf{x})$  as a matrix, whose (i, j)-th entry is given by\n\n$$[G(\\mathbf{x})]_{ij} \\coloneqq \\frac{\\partial^2 d(\\mathbf{x}, \\mathbf{y})}{\\partial y_i \\partial y_j} \\bigg|_{\\mathbf{y} = \\mathbf{x}}.$$\n\nSimilarly, we define H(x) as\n\n$$[\\boldsymbol{H}(\\mathbf{x})]_{ij} \\coloneqq \\frac{\\partial^2 d(\\mathbf{y}, \\mathbf{x})}{\\partial y_i \\partial y_j} \\bigg|_{\\mathbf{y} = \\mathbf{x}}.$$",
      "metadata": {
        "chunk_index": 140,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 849
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_141",
      "content": "Similarly, we define H(x) as\n\n$$[\\boldsymbol{H}(\\mathbf{x})]_{ij} \\coloneqq \\frac{\\partial^2 d(\\mathbf{y}, \\mathbf{x})}{\\partial y_i \\partial y_j} \\bigg|_{\\mathbf{y} = \\mathbf{x}}.$$\n\nThe matrices G and H play a crucial role in forming continuous-time objectives for consistency distillation. Additionally, we denote the Jacobian of  $f_{\\theta}(\\mathbf{x},t)$  with respect to  $\\mathbf{x}$  as  $\\frac{\\partial f_{\\theta}(\\mathbf{x},t)}{\\partial \\mathbf{x}}$ .\n\nWhen  $\\theta^- = \\theta$  (with no stopgrad operator), we have the following theoretical result.\n\n<span id=\"page-17-1\"></span>**Theorem 3.** Let  $t_n = \\tau(\\frac{n-1}{N-1})$ , where  $n \\in [1, N]$ , and  $\\tau(\\cdot)$  is a strictly monotonic function with  $\\tau(0) = \\epsilon$  and  $\\tau(1) = T$ . Assume  $\\tau$  is continuously differentiable in [0, 1], d is three times continuously differentiable with bounded third derivatives,",
      "metadata": {
        "chunk_index": 141,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 903
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_142",
      "content": "and  $f_{\\theta}$  is twice continuously differentiable with bounded first and second derivatives. Assume further that the weighting function  $\\lambda(\\cdot)$  is bounded, and  $\\sup_{\\mathbf{x},t\\in[\\epsilon,T]}\\|\\mathbf{s}_{\\phi}(\\mathbf{x},t)\\|_2 < \\infty$ . Then with the Euler solver in consistency distillation, we have\n\n<span id=\"page-18-2\"></span><span id=\"page-18-0\"></span>\n$$\\lim_{N \\to \\infty} (N-1)^2 \\mathcal{L}_{CD}^N(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}; \\boldsymbol{\\phi}) = \\mathcal{L}_{CD}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}; \\boldsymbol{\\phi}), \\tag{17}$$\n\nwhere  $\\mathcal{L}_{CD}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}; \\boldsymbol{\\phi})$  is defined as",
      "metadata": {
        "chunk_index": 142,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 707
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_143",
      "content": "where  $\\mathcal{L}_{CD}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}; \\boldsymbol{\\phi})$  is defined as\n\n$$\\frac{1}{2}\\mathbb{E}\\left[\\frac{\\lambda(t)}{[(\\tau^{-1})'(t)]^2}\\left(\\frac{\\partial \\boldsymbol{f_{\\theta}}(\\mathbf{x}_t,t)}{\\partial t} - t\\frac{\\partial \\boldsymbol{f_{\\theta}}(\\mathbf{x}_t,t)}{\\partial \\mathbf{x}_t}\\boldsymbol{s_{\\phi}}(\\mathbf{x}_t,t)\\right)^{\\mathsf{T}}\\boldsymbol{G}(\\boldsymbol{f_{\\theta}}(\\mathbf{x}_t,t))\\left(\\frac{\\partial \\boldsymbol{f_{\\theta}}(\\mathbf{x}_t,t)}{\\partial t} - t\\frac{\\partial \\boldsymbol{f_{\\theta}}(\\mathbf{x}_t,t)}{\\partial \\mathbf{x}_t}\\boldsymbol{s_{\\phi}}(\\mathbf{x}_t,t)\\right)\\right].$$\n(18)\n\nHere the expectation above is taken over  $\\mathbf{x} \\sim p_{\\text{data}}$ ,  $u \\sim \\mathcal{U}[0, 1]$ ,  $t = \\tau(u)$ , and  $\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{x}, t^2 \\mathbf{I})$ .\n\n*Proof.* Let  $\\Delta u = \\frac{1}{N-1}$  and  $u_n = \\frac{n-1}{N-1}$ . First, we can derive the following equation with Taylor expansion:",
      "metadata": {
        "chunk_index": 143,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 994
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_144",
      "content": "*Proof.* Let  $\\Delta u = \\frac{1}{N-1}$  and  $u_n = \\frac{n-1}{N-1}$ . First, we can derive the following equation with Taylor expansion:\n\n$$f_{\\theta}(\\hat{\\mathbf{x}}_{t_{n}}^{\\phi}, t_{n}) - f_{\\theta}(\\mathbf{x}_{t_{n+1}}, t_{n+1}) = f_{\\theta}(\\mathbf{x}_{t_{n+1}} + t_{n+1}s_{\\phi}(\\mathbf{x}_{t_{n+1}}, t_{n+1})\\tau'(u_{n})\\Delta u, t_{n}) - f_{\\theta}(\\mathbf{x}_{t_{n+1}}, t_{n+1})$$\n\n$$= t_{n+1} \\frac{\\partial f_{\\theta}(\\mathbf{x}_{t_{n+1}}, t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}} s_{\\phi}(\\mathbf{x}_{t_{n+1}}, t_{n+1})\\tau'(u_{n})\\Delta u - \\frac{\\partial f_{\\theta}(\\mathbf{x}_{t_{n+1}}, t_{n+1})}{\\partial t_{n+1}}\\tau'(u_{n})\\Delta u + O((\\Delta u)^{2}), \\tag{19}$$\n\nNote that  $\\tau'(u_n) = \\frac{1}{\\tau^{-1}(t_{n+1})}$ . Then, we apply Taylor expansion to the consistency distillation loss, which gives",
      "metadata": {
        "chunk_index": 144,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 826
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_145",
      "content": "Note that  $\\tau'(u_n) = \\frac{1}{\\tau^{-1}(t_{n+1})}$ . Then, we apply Taylor expansion to the consistency distillation loss, which gives\n\n$$(N-1)^{2}\\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta};\\boldsymbol{\\phi}) = \\frac{1}{(\\Delta u)^{2}}\\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta};\\boldsymbol{\\phi}) = \\frac{1}{(\\Delta u)^{2}}\\mathbb{E}[\\lambda(t_{n})d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n})]$$\n\n$$\\stackrel{(i)}{=} \\frac{1}{2(\\Delta u)^{2}} \\left( \\mathbb{E}\\{\\lambda(t_{n})\\tau'(u_{n})^{2}[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})]^{\\mathsf{T}}\\boldsymbol{G}(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})) \\right)$$",
      "metadata": {
        "chunk_index": 145,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 909
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_146",
      "content": "$$\\cdot [\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})] + \\mathbb{E}[O(|\\Delta u|^{3})] \\right)$$\n\n$$\\stackrel{(ii)}{=} \\frac{1}{2}\\mathbb{E}\\left[\\lambda(t_{n})\\tau'(u_{n})^{2}\\left(\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial t_{n+1}} - t_{n+1}\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}}\\boldsymbol{s}_{\\boldsymbol{\\phi}}(\\mathbf{x}_{t_{n+1}},t_{n+1})\\right)^{\\mathsf{T}}\\boldsymbol{G}(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}))\\right]$$",
      "metadata": {
        "chunk_index": 146,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 693
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_147",
      "content": "$$\\cdot \\left(\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial t_{n+1}} - t_{n+1}\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}}\\boldsymbol{s}_{\\boldsymbol{\\phi}}(\\mathbf{x}_{t_{n+1}},t_{n+1})\\right)^{\\mathsf{T}}\\boldsymbol{G}(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}))\\right]$$\n\n$$\\cdot \\left(\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial t_{n+1}} - t_{n+1}\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}}\\boldsymbol{s}_{\\boldsymbol{\\phi}}(\\mathbf{x}_{t_{n+1}},t_{n+1})\\right)^{\\mathsf{T}}\\boldsymbol{G}(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})\\right)$$",
      "metadata": {
        "chunk_index": 147,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 825
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_148",
      "content": "$$\\cdot \\left(\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial t_{n+1}} - t_{n+1}\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}}\\boldsymbol{s}_{\\boldsymbol{\\phi}}(\\mathbf{x}_{t_{n+1}},t_{n+1})\\right)^{\\mathsf{T}}\\boldsymbol{G}(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})\\right)$$\n\n$$\\cdot \\left(\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial t_{n+1}} - t_{n+1}\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}}\\boldsymbol{s}_{\\boldsymbol{\\phi}}(\\mathbf{x}_{t_{n+1}},t_{n+1})\\right)^{\\mathsf{T}}\\boldsymbol{G}(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})\\right)$$",
      "metadata": {
        "chunk_index": 148,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 824
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_149",
      "content": "where we obtain (i) by expanding  $d(\\mathbf{f}_{\\theta}(\\mathbf{x}_{t_{n+1}}, t_{n+1}), \\cdot)$  to second order and observing  $d(\\mathbf{x}, \\mathbf{x}) \\equiv 0$  and  $\\nabla_{\\mathbf{y}} d(\\mathbf{x}, \\mathbf{y})|_{\\mathbf{y} = \\mathbf{x}} \\equiv \\mathbf{0}$ . We obtain (ii) using Eq. (19). By taking the limit for both sides of Eq. (20) as  $\\Delta u \\to 0$  or equivalently  $N \\to \\infty$ , we arrive at Eq. (17), which completes the proof.\n\n**Remark 2.** Although Theorem 3 assumes the Euler ODE solver for technical simplicity, we believe an analogous result can be derived for more general solvers, since all ODE solvers should perform similarly as  $N \\to \\infty$ . We leave a more general version of Theorem 3 as future work.",
      "metadata": {
        "chunk_index": 149,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 740
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_150",
      "content": "**Remark 3.** Theorem 3 implies that consistency models can be trained by minimizing  $\\mathcal{L}_{CD}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}; \\boldsymbol{\\phi})$ . In particular, when  $d(\\mathbf{x}, \\mathbf{y}) = \\|\\mathbf{x} - \\mathbf{y}\\|_2^2$ , we have\n\n<span id=\"page-18-1\"></span>\n$$\\mathcal{L}_{CD}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}; \\boldsymbol{\\phi}) = \\mathbb{E}\\left[\\frac{\\lambda(t)}{[(\\tau^{-1})'(t)]^2} \\left\\| \\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_t, t)}{\\partial t} - t \\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_t, t)}{\\partial \\mathbf{x}_t} \\boldsymbol{s}_{\\boldsymbol{\\phi}}(\\mathbf{x}_t, t) \\right\\|_2^2\\right]. \\tag{21}$$",
      "metadata": {
        "chunk_index": 150,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 716
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_151",
      "content": "<span id=\"page-18-3\"></span>However, this continuous-time objective requires computing Jacobian-vector products as a subroutine to evaluate the loss function, which can be slow and laborious to implement in deep learning frameworks that do not support forward-mode automatic differentiation.\n\n**Remark 4.** If  $f_{\\theta}(\\mathbf{x}, t)$  matches the ground truth consistency function for the empirical PF ODE of  $s_{\\phi}(\\mathbf{x}, t)$ , then\n\n$$\\frac{\\partial \\mathbf{f}_{\\theta}(\\mathbf{x}, t)}{\\partial t} - t \\frac{\\partial \\mathbf{f}_{\\theta}(\\mathbf{x}, t)}{\\partial \\mathbf{x}} \\mathbf{s}_{\\phi}(\\mathbf{x}, t) \\equiv 0$$\n\nand therefore  $\\mathcal{L}_{CD}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}; \\boldsymbol{\\phi}) = 0$ . This can be proved by noting that  $f_{\\boldsymbol{\\theta}}(\\mathbf{x}_t, t) \\equiv \\mathbf{x}_{\\epsilon}$  for all  $t \\in [\\epsilon, T]$ , and then taking the time-derivative of this identity:",
      "metadata": {
        "chunk_index": 151,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 939
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_152",
      "content": "$$f_{\\theta}(\\mathbf{x}_{t}, t) \\equiv \\mathbf{x}_{\\epsilon}$$\n\n$$\\iff \\frac{\\partial f_{\\theta}(\\mathbf{x}_{t}, t)}{\\partial \\mathbf{x}_{t}} \\frac{\\partial \\mathbf{x}_{t}}{\\partial t} + \\frac{\\partial f_{\\theta}(\\mathbf{x}_{t}, t)}{\\partial t} \\equiv 0$$\n\n$$\\iff \\frac{\\partial f_{\\theta}(\\mathbf{x}_{t}, t)}{\\partial \\mathbf{x}_{t}} [-ts_{\\phi}(\\mathbf{x}_{t}, t)] + \\frac{\\partial f_{\\theta}(\\mathbf{x}_{t}, t)}{\\partial t} \\equiv 0$$\n\n$$\\iff \\frac{\\partial f_{\\theta}(\\mathbf{x}_{t}, t)}{\\partial t} - t \\frac{\\partial f_{\\theta}(\\mathbf{x}_{t}, t)}{\\partial \\mathbf{x}_{t}} s_{\\phi}(\\mathbf{x}_{t}, t) \\equiv 0.$$\n\nThe above observation provides another motivation for  $\\mathcal{L}^{\\infty}_{CD}(\\theta, \\theta; \\phi)$ , as it is minimized if and only if the consistency model matches the ground truth consistency function.",
      "metadata": {
        "chunk_index": 152,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 829
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_153",
      "content": "For some metric functions, such as the  $\\ell_1$  norm, the Hessian  $G(\\mathbf{x})$  is zero so Theorem 3 is vacuous. Below we show that a non-vacuous statement holds for the  $\\ell_1$  norm with just a small modification of the proof for Theorem 3.\n\n<span id=\"page-19-3\"></span>**Theorem 4.** Let  $t_n = \\tau(\\frac{n-1}{N-1})$ , where  $n \\in [\\![1,N]\\!]$ , and  $\\tau(\\cdot)$  is a strictly monotonic function with  $\\tau(0) = \\epsilon$  and  $\\tau(1) = T$ . Assume  $\\tau$  is continuously differentiable in  $[\\![0,1]\\!]$ , and  $f_{\\theta}$  is twice continuously differentiable with bounded first and second derivatives. Assume further that the weighting function  $\\lambda(\\cdot)$  is bounded, and  $\\sup_{\\mathbf{x},t\\in[\\epsilon,T]}\\|\\mathbf{s}_{\\phi}(\\mathbf{x},t)\\|_2 < \\infty$ . Suppose we use the Euler ODE solver, and set  $d(\\mathbf{x},\\mathbf{y}) = \\|\\mathbf{x} - \\mathbf{y}\\|_1$  in consistency distillation. Then we have",
      "metadata": {
        "chunk_index": 153,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 940
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_154",
      "content": "<span id=\"page-19-2\"></span><span id=\"page-19-1\"></span>\n$$\\lim_{N \\to \\infty} (N-1) \\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}; \\boldsymbol{\\phi}) = \\mathcal{L}_{CD, \\ell_{1}}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}; \\boldsymbol{\\phi}), \\tag{22}$$\n\nwhere\n\n$$\\mathcal{L}_{CD, \\ \\ell_1}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}; \\boldsymbol{\\phi}) \\coloneqq \\mathbb{E}\\left[\\frac{\\lambda(t)}{(\\tau^{-1})'(t)} \\left\\| t \\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_t, t)}{\\partial \\mathbf{x}_t} \\boldsymbol{s}_{\\boldsymbol{\\phi}}(\\mathbf{x}_t, t) - \\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_t, t)}{\\partial t} \\right\\|_{1}\\right]$$\n\nwhere the expectation above is taken over  $\\mathbf{x} \\sim p_{data}$ ,  $u \\sim \\mathcal{U}[0,1]$ ,  $t = \\tau(u)$ , and  $\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{x}, t^2 \\mathbf{I})$ .\n\n*Proof.* Let  $\\Delta u = \\frac{1}{N-1}$  and  $u_n = \\frac{n-1}{N-1}$ . We have",
      "metadata": {
        "chunk_index": 154,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 974
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_155",
      "content": "*Proof.* Let  $\\Delta u = \\frac{1}{N-1}$  and  $u_n = \\frac{n-1}{N-1}$ . We have\n\n$$(N-1)\\mathcal{L}_{\\text{CD}}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta};\\boldsymbol{\\phi}) = \\frac{1}{\\Delta u}\\mathcal{L}_{\\text{CD}}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta};\\boldsymbol{\\phi}) = \\frac{1}{\\Delta u}\\mathbb{E}[\\lambda(t_{n})\\|\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n})\\|_{1}]$$\n\n$$\\stackrel{(i)}{=} \\frac{1}{\\Delta u}\\mathbb{E}\\left[\\lambda(t_{n})\\|t_{n+1}\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}}\\boldsymbol{s}_{\\boldsymbol{\\phi}}(\\mathbf{x}_{t_{n+1}},t_{n+1})\\boldsymbol{\\tau}'(u_{n}) - \\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial t_{n+1}}\\boldsymbol{\\tau}'(u_{n}) + O((\\Delta u)^{2})\\|_{1}\\right]$$",
      "metadata": {
        "chunk_index": 155,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 940
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_156",
      "content": "$$=\\mathbb{E}\\left[\\lambda(t_{n})\\boldsymbol{\\tau}'(u_{n})\\|t_{n+1}\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}}\\boldsymbol{s}_{\\boldsymbol{\\phi}}(\\mathbf{x}_{t_{n+1}},t_{n+1}) - \\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial t_{n+1}} + O(\\Delta u)\\|_{1}\\right]$$\n\n$$=\\mathbb{E}\\left[\\frac{\\lambda(t_{n})}{(\\boldsymbol{\\tau}^{-1})'(t_{n})}\\|t_{n+1}\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}}\\boldsymbol{s}_{\\boldsymbol{\\phi}}(\\mathbf{x}_{t_{n+1}},t_{n+1}) - \\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial t_{n+1}} + O(\\Delta u)\\|_{1}\\right]$$\n\n$$(23)$$\n\nwhere (i) is obtained by plugging Eq. (19) into the previous equation. Taking the limit for both sides of Eq. (23) as  $\\Delta u \\to 0$  or equivalently  $N \\to \\infty$  leads to Eq. (22), which completes the proof.",
      "metadata": {
        "chunk_index": 156,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 997
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_157",
      "content": "**Remark 5.** According to Theorem 4, consistency models can be trained by minimizing  $\\mathcal{L}_{CD, \\ell_1}^{\\infty}(\\theta, \\theta; \\phi)$ . Moreover, the same reasoning in Remark 4 can be applied to show that  $\\mathcal{L}_{CD, \\ell_1}^{\\infty}(\\theta, \\theta; \\phi) = 0$  if and only if  $f_{\\theta}(\\mathbf{x}_t, t) = \\mathbf{x}_{\\epsilon}$  for all  $\\mathbf{x}_t \\in \\mathbb{R}^d$  and  $t \\in [\\epsilon, T]$ .\n\n<span id=\"page-19-0\"></span>In the second case where  $\\theta^- = \\operatorname{stopgrad}(\\theta)$ , we can derive a so-called \"pseudo-objective\" whose gradient matches the gradient of  $\\mathcal{L}_{\\operatorname{CD}}^N(\\theta, \\theta^-; \\phi)$  in the limit of  $N \\to \\infty$ . Minimizing this pseudo-objective with gradient descent gives another way to train consistency models via distillation. This pseudo-objective is provided by the theorem below.",
      "metadata": {
        "chunk_index": 157,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 878
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_158",
      "content": "**Theorem 5.** Let  $t_n = \\tau(\\frac{n-1}{N-1})$ , where  $n \\in [\\![1,N]\\!]$ , and  $\\tau(\\cdot)$  is a strictly monotonic function with  $\\tau(0) = \\epsilon$  and  $\\tau(1) = T$ . Assume  $\\tau$  is continuously differentiable in  $[\\![0,1]\\!]$ , d is three times continuously differentiable with bounded third derivatives, and  $f_{\\theta}$  is twice continuously differentiable with bounded first and second derivatives. Assume further that the weighting function  $\\lambda(\\cdot)$  is bounded,  $\\sup_{\\mathbf{x},t\\in[\\epsilon,T]}\\|\\mathbf{s}_{\\phi}(\\mathbf{x},t)\\|_2 < \\infty$ , and  $\\sup_{\\mathbf{x},t\\in[\\epsilon,T]}\\|\\nabla_{\\theta}f_{\\theta}(\\mathbf{x},t)\\|_2 < \\infty$ . Suppose we use the Euler ODE solver, and  $\\theta^- = \\operatorname{stopgrad}(\\theta)$  in consistency distillation. Then,",
      "metadata": {
        "chunk_index": 158,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 806
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_159",
      "content": "<span id=\"page-20-1\"></span><span id=\"page-20-0\"></span>\n$$\\lim_{N \\to \\infty} (N-1) \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}; \\boldsymbol{\\phi}) = \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{CD}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}; \\boldsymbol{\\phi}), \\tag{24}$$\n\nwhere\n\n$$\\mathcal{L}_{CD}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}; \\boldsymbol{\\phi}) := \\mathbb{E}\\left[\\frac{\\lambda(t)}{(\\tau^{-1})'(t)} \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t}, t)^{\\mathsf{T}} \\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t}, t)) \\left(\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t}, t)}{\\partial t} - t \\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t}, t)}{\\partial \\mathbf{x}_{t}} \\boldsymbol{s}_{\\boldsymbol{\\phi}}(\\mathbf{x}_{t}, t)\\right)\\right]. \\tag{25}$$",
      "metadata": {
        "chunk_index": 159,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 912
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_160",
      "content": "Here the expectation above is taken over  $\\mathbf{x} \\sim p_{data}$ ,  $u \\sim \\mathcal{U}[0, 1]$ ,  $t = \\tau(u)$ , and  $\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{x}, t^2 \\mathbf{I})$ .\n\n*Proof.* We denote  $\\Delta u = \\frac{1}{N-1}$  and  $u_n = \\frac{n-1}{N-1}$ . First, we leverage Taylor series expansion to obtain\n\n$$(N-1)\\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{-};\\boldsymbol{\\phi}) = \\frac{1}{\\Delta u}\\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{-};\\boldsymbol{\\phi}) = \\frac{1}{\\Delta u}\\mathbb{E}[\\lambda(t_{n})d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n})]$$",
      "metadata": {
        "chunk_index": 160,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 723
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_161",
      "content": "$$\\stackrel{(i)}{=} \\frac{1}{2\\Delta u}\\left(\\mathbb{E}\\{\\lambda(t_{n})[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))\\right)$$",
      "metadata": {
        "chunk_index": 161,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 364
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_162",
      "content": "$$\\cdot [\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n})]\\} + \\mathbb{E}[O(|\\Delta u|^{2})]$$\n\n$$(26)$$",
      "metadata": {
        "chunk_index": 162,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 785
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_163",
      "content": "$$(26)$$\n\nwhere (i) is derived by expanding  $d(\\cdot, f_{\\theta^-}(\\hat{\\mathbf{x}}_{t_n}^{\\phi}, t_n))$  to second order and leveraging  $d(\\mathbf{x}, \\mathbf{x}) \\equiv 0$  and  $\\nabla_{\\mathbf{y}} d(\\mathbf{y}, \\mathbf{x})|_{\\mathbf{y} = \\mathbf{x}} \\equiv \\mathbf{0}$ . Next, we compute the gradient of Eq. (26) with respect to  $\\boldsymbol{\\theta}$  and simplify the result to obtain\n\n$$(N-1)\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{-};\\boldsymbol{\\phi}) = \\frac{1}{\\Delta u}\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{-};\\boldsymbol{\\phi})$$",
      "metadata": {
        "chunk_index": 163,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 646
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_164",
      "content": "$$= \\frac{1}{2\\Delta u}\\nabla_{\\boldsymbol{\\theta}}\\mathbb{E}\\{\\lambda(t_{n})[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n})]\\} + \\mathbb{E}[O(|\\Delta u|^{2})]$$",
      "metadata": {
        "chunk_index": 164,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 560
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_165",
      "content": "$$\\stackrel{(ii)}{=} \\frac{1}{\\Delta u}\\mathbb{E}\\{\\lambda(t_{n})[\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n})]\\} + \\mathbb{E}[O(|\\Delta u|^{2})]$$\n\n$$\\stackrel{(ii)}{=} \\frac{1}{\\Delta u}\\mathbb{E}\\{\\lambda(t_{n})[\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))[\\boldsymbol{t}_{n+1}\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}}\\boldsymbol{\\tau}'(u_{n})\\Delta u]\\} + \\mathbb{E}[O(|\\Delta u|)]$$",
      "metadata": {
        "chunk_index": 165,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 969
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_166",
      "content": "$$=\\mathbb{E}\\{\\lambda(t_{n})[\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))[\\boldsymbol{t}_{n+1}\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}}\\boldsymbol{\\tau}'(u_{n})\\Delta u]\\} + \\mathbb{E}[O(|\\Delta u|)]$$\n\n$$=\\nabla_{\\boldsymbol{\\theta}}\\mathbb{E}\\{\\lambda(t_{n})[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))[\\boldsymbol{t}_{n+1}\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}}\\boldsymbol{\\tau}'(u_{n})]\\} + \\mathbb{E}[O(|\\Delta u|)]$$",
      "metadata": {
        "chunk_index": 166,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 894
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_167",
      "content": "$$=\\nabla_{\\boldsymbol{\\theta}}\\mathbb{E}\\{\\lambda(t_{n})[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))[\\boldsymbol{t}_{n+1}\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}}\\boldsymbol{\\tau}'(u_{n})]\\} + \\mathbb{E}[O(|\\Delta u|)]$$\n\n$$=\\nabla_{\\boldsymbol{\\theta}}\\mathbb{E}\\{\\lambda(t_{n})[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))[\\boldsymbol{t}_{n+1}\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}}\\boldsymbol{\\tau}'(u_{n})]\\} + \\mathbb{E}[O(|\\Delta u|)]$$",
      "metadata": {
        "chunk_index": 167,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 886
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_168",
      "content": "$$=\\nabla_{\\boldsymbol{\\theta}}\\mathbb{E}\\{\\lambda(t_{n})[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))[\\boldsymbol{t}_{n+1}\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1})}{\\partial \\mathbf{x}_{t_{n+1}}}\\boldsymbol{\\tau}'(u_{n})]\\} + \\mathbb{E}[O(|\\Delta u|)]$$\n\n$$=\\nabla_{\\boldsymbol{\\theta}}\\mathbb{E}\\{\\lambda(t_{n})[\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{$$\n\nHere (i) results from the chain rule, and (ii) follows from Eq. (19) and  $f_{\\theta}(\\mathbf{x}, t) \\equiv f_{\\theta^{-}}(\\mathbf{x}, t)$ , since  $\\theta^{-} = \\text{stopgrad}(\\theta)$ . Taking the limit for both sides of Eq. (28) as  $\\Delta u \\to 0$  (or  $N \\to \\infty$ ) yields Eq. (24), which completes the proof.",
      "metadata": {
        "chunk_index": 168,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 872
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_169",
      "content": "**Remark 6.** When  $d(\\mathbf{x}, \\mathbf{y}) = \\|\\mathbf{x} - \\mathbf{y}\\|_2^2$ , the pseudo-objective  $\\mathcal{L}_{CD}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^-; \\boldsymbol{\\phi})$  can be simplified to\n\n<span id=\"page-21-2\"></span>\n$$\\mathcal{L}_{CD}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}; \\boldsymbol{\\phi}) = 2\\mathbb{E}\\left[\\frac{\\lambda(t)}{(\\tau^{-1})'(t)} \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t}, t)^{\\mathsf{T}} \\left(\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t}, t)}{\\partial t} - t \\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t}, t)}{\\partial \\mathbf{x}_{t}} \\boldsymbol{s}_{\\boldsymbol{\\phi}}(\\mathbf{x}_{t}, t)\\right)\\right]. \\tag{28}$$",
      "metadata": {
        "chunk_index": 169,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 743
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_170",
      "content": "**Remark 7.** The objective  $\\mathcal{L}^{\\infty}_{CD}(\\theta, \\theta^-; \\phi)$  defined in Theorem 5 is only meaningful in terms of its gradient—one cannot measure the progress of training by tracking the value of  $\\mathcal{L}^{\\infty}_{CD}(\\theta, \\theta^-; \\phi)$ , but can still apply gradient descent to this objective to distill consistency models from pre-trained diffusion models. Because this objective is not a typical loss function, we refer to it as the \"pseudo-objective\" for consistency distillation.",
      "metadata": {
        "chunk_index": 170,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 516
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_171",
      "content": "<span id=\"page-21-5\"></span>**Remark 8.** Following the same reasoning in Remark 4, we can easily derive that  $\\mathcal{L}_{CD}^{\\infty}(\\theta, \\theta^-; \\phi) = 0$  and  $\\nabla_{\\theta} \\mathcal{L}_{CD}^{\\infty}(\\theta, \\theta^-; \\phi) = 0$  if  $f_{\\theta}(\\mathbf{x}, t)$  matches the ground truth consistency function for the empirical PF ODE that involves  $s_{\\phi}(\\mathbf{x}, t)$ . However, the converse does not hold true in general. This distinguishes  $\\mathcal{L}_{CD}^{\\infty}(\\theta, \\theta^-; \\phi)$  from  $\\mathcal{L}_{CD}^{\\infty}(\\theta, \\theta; \\phi)$ , the latter of which is a true loss function.",
      "metadata": {
        "chunk_index": 171,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 621
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_172",
      "content": "#### <span id=\"page-21-0\"></span>**B.2.** Consistency Training in Continuous Time\n\nA remarkable observation is that the pseudo-objective in Theorem 5 can be estimated without any pre-trained diffusion models, which enables direct consistency training of consistency models. More precisely, we have the following result.",
      "metadata": {
        "chunk_index": 172,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 319
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_173",
      "content": "<span id=\"page-21-1\"></span>**Theorem 6.** Let  $t_n = \\tau(\\frac{n-1}{N-1})$ , where  $n \\in [\\![1,N]\\!]$ , and  $\\tau(\\cdot)$  is a strictly monotonic function with  $\\tau(0) = \\epsilon$  and  $\\tau(1) = T$ . Assume  $\\tau$  is continuously differentiable in  $[\\![0,1]\\!]$ , d is three times continuously differentiable with bounded third derivatives, and  $f_{\\theta}$  is twice continuously differentiable with bounded first and second derivatives. Assume further that the weighting function  $\\lambda(\\cdot)$  is bounded,  $\\mathbb{E}[\\|\\nabla \\log p_{t_n}(\\mathbf{x}_{t_n})\\|_2^2] < \\infty$ ,  $\\sup_{\\mathbf{x},t \\in [\\epsilon,T]} \\|\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\mathbf{x},t)\\|_2 < \\infty$ , and  $\\phi$  represents diffusion model parameters that satisfy  $s_{\\boldsymbol{\\phi}}(\\mathbf{x},t) \\equiv \\nabla \\log p_t(\\mathbf{x})$ . Then if  $\\boldsymbol{\\theta}^- = \\operatorname{stopgrad}(\\boldsymbol{\\theta})$ , we have",
      "metadata": {
        "chunk_index": 173,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 953
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_174",
      "content": "<span id=\"page-21-4\"></span><span id=\"page-21-3\"></span>\n$$\\lim_{N \\to \\infty} (N-1) \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{CD}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}; \\boldsymbol{\\phi}) = \\lim_{N \\to \\infty} (N-1) \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{CT}^{N}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}) = \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}_{CT}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}), \\tag{29}$$\n\nwhere  $\\mathcal{L}_{CD}^{N}$  uses the Euler ODE solver, and",
      "metadata": {
        "chunk_index": 174,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 501
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_175",
      "content": "where  $\\mathcal{L}_{CD}^{N}$  uses the Euler ODE solver, and\n\n$$\\mathcal{L}_{CT}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}) := \\mathbb{E}\\left[\\frac{\\lambda(t)}{(\\tau^{-1})'(t)} \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t}, t)^{\\mathsf{T}} \\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t}, t)) \\left(\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t}, t)}{\\partial t} + \\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t}, t)}{\\partial \\mathbf{x}_{t}} \\cdot \\frac{\\mathbf{x}_{t} - \\mathbf{x}}{t}\\right)\\right].$$\n(30)\n\nHere the expectation above is taken over  $\\mathbf{x} \\sim p_{data}$ ,  $u \\sim \\mathcal{U}[0,1]$ ,  $t = \\tau(u)$ , and  $\\mathbf{x}_t \\sim \\mathcal{N}(\\mathbf{x}, t^2 \\mathbf{I})$ .\n\n*Proof.* The proof mostly follows that of Theorem 5. First, we leverage Taylor series expansion to obtain",
      "metadata": {
        "chunk_index": 175,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 890
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_176",
      "content": "*Proof.* The proof mostly follows that of Theorem 5. First, we leverage Taylor series expansion to obtain\n\n$$(N-1)\\mathcal{L}_{\\mathrm{CT}}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{-}) = \\frac{1}{\\Delta u}\\mathcal{L}_{\\mathrm{CT}}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{-}) = \\frac{1}{\\Delta u}\\mathbb{E}[\\lambda(t_{n})d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1}),\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}+t_{n}\\mathbf{z},t_{n}))]$$\n\n$$\\stackrel{(i)}{=} \\frac{1}{2\\Delta u}\\left(\\mathbb{E}\\{\\lambda(t_{n})[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1})-\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}+t_{n}\\mathbf{z},t_{n})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}+t_{n}\\mathbf{z},t_{n}))\\right)$$",
      "metadata": {
        "chunk_index": 176,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 820
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_177",
      "content": "$$\\cdot[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1})-\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}+t_{n}\\mathbf{z},t_{n})]\\}+\\mathbb{E}[O(|\\Delta u|^{3})]$$\n\n$$=\\frac{1}{2\\Delta u}\\mathbb{E}\\{\\lambda(t_{n})[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1})-\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}+t_{n}\\mathbf{z},t_{n})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}+t_{n}\\mathbf{z},t_{n}))$$\n\n$$\\cdot[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1})-\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}+t_{n}\\mathbf{z},t_{n})]\\}+\\mathbb{E}[O(|\\Delta u|^{2})]$$",
      "metadata": {
        "chunk_index": 177,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 692
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_178",
      "content": "$$\\cdot[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1})-\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}+t_{n}\\mathbf{z},t_{n})]\\}+\\mathbb{E}[O(|\\Delta u|^{2})]$$\n\nwhere  $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ , (i) is derived by first expanding  $d(\\cdot, \\mathbf{f}_{\\theta^-}(\\mathbf{x} + t_n \\mathbf{z}, t_n))$  to second order, and then noting that  $d(\\mathbf{x}, \\mathbf{x}) \\equiv 0$  and  $\\nabla_{\\mathbf{y}} d(\\mathbf{y}, \\mathbf{x})|_{\\mathbf{y} = \\mathbf{x}} \\equiv \\mathbf{0}$ . Next, we compute the gradient of Eq. (31) with respect to  $\\boldsymbol{\\theta}$  and simplify the result to obtain\n\n$$(N-1)\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{\\mathrm{CT}}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{-}) = \\frac{1}{\\Delta u}\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{\\mathrm{CT}}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{-})$$",
      "metadata": {
        "chunk_index": 178,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 889
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_179",
      "content": "$$= \\frac{1}{2\\Delta u}\\nabla_{\\boldsymbol{\\theta}}\\mathbb{E}\\{\\lambda(t_{n})[\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}+t_{n}\\mathbf{z},t_{n})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}+t_{n}\\mathbf{z},t_{n}))$$\n\n$$\\cdot [\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}+t_{n}\\mathbf{z},t_{n})]\\} + \\mathbb{E}[O(|\\Delta u|^{2})]$$",
      "metadata": {
        "chunk_index": 179,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 534
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_180",
      "content": "<span id=\"page-22-1\"></span>\n$$\\frac{(i)}{\\Delta u} \\mathbb{E}\\{\\lambda(t_n)[\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\mathbf{x} + t_{n+1}\\mathbf{z}, t_{n+1})]^{\\mathsf{T}} \\boldsymbol{H}(f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x} + t_n\\mathbf{z}, t_n)) \\\\\n\\cdot [f_{\\boldsymbol{\\theta}}(\\mathbf{x} + t_{n+1}\\mathbf{z}, t_{n+1}) - f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x} + t_n\\mathbf{z}, t_n)]\\} + \\mathbb{E}[O(|\\Delta u|^2)] \\\\\n\\stackrel{(ii)}{=} \\frac{1}{\\Delta u} \\mathbb{E}\\{\\lambda(t_n)[\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\mathbf{x} + t_{n+1}\\mathbf{z}, t_{n+1})]^{\\mathsf{T}} \\boldsymbol{H}(f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x} + t_n\\mathbf{z}, t_n)) \\Big[ \\tau'(u_n) \\Delta u \\partial_1 f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x} + t_n\\mathbf{z}, t_n)\\mathbf{z} \\\\\n+ \\partial_2 f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x} + t_n\\mathbf{z}, t_n) \\tau'(u_n) \\Delta u \\Big] \\Big\\} + \\mathbb{E}[O(|\\Delta u|)] \\\\",
      "metadata": {
        "chunk_index": 180,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 932
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_181",
      "content": "+ \\partial_2 f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x} + t_n\\mathbf{z}, t_n) \\tau'(u_n) \\Delta u \\Big] \\Big\\} + \\mathbb{E}[O(|\\Delta u|)] \\\\\n= \\mathbb{E}\\{\\lambda(t_n) \\tau'(u_n)[\\nabla_{\\boldsymbol{\\theta}} f_{\\boldsymbol{\\theta}}(\\mathbf{x} + t_{n+1}\\mathbf{z}, t_{n+1})]^{\\mathsf{T}} \\boldsymbol{H}(f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x} + t_n\\mathbf{z}, t_n)) \\Big[ \\partial_1 f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x} + t_n\\mathbf{z}, t_n)\\mathbf{z} \\\\\n+ \\partial_2 f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x} + t_n\\mathbf{z}, t_n) \\Big] \\Big\\} + \\mathbb{E}[O(|\\Delta u|)] \\\\\n= \\nabla_{\\boldsymbol{\\theta}} \\mathbb{E}\\{\\lambda(t_n) \\tau'(u_n)[f_{\\boldsymbol{\\theta}}(\\mathbf{x} + t_{n+1}\\mathbf{z}, t_{n+1})]^{\\mathsf{T}} \\boldsymbol{H}(f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x} + t_n\\mathbf{z}, t_n)) \\Big[ \\partial_1 f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x} + t_n\\mathbf{z}, t_n)\\mathbf{z} \\\\",
      "metadata": {
        "chunk_index": 181,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 883
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_182",
      "content": "+ \\partial_2 f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x} + t_n\\mathbf{z}, t_n) \\Big] \\Big\\} + \\mathbb{E}[O(|\\Delta u|)] \\\\\n= \\nabla_{\\boldsymbol{\\theta}} \\mathbb{E}\\{\\lambda(t_n) \\tau'(u_n)[f_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}}, t_{n+1})]^{\\mathsf{T}} \\boldsymbol{H}(f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n)) \\Big[ \\partial_1 f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n) \\frac{\\mathbf{x}_{t_n} - \\mathbf{x}}{t_n} + \\partial_2 f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n) \\Big] \\Big\\} + \\mathbb{E}[O(|\\Delta u|)] \\\\\n= \\nabla_{\\boldsymbol{\\theta}} \\mathbb{E}\\{\\lambda(t_n) \\tau'(t_n)[f_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}}, t_{n+1})]^{\\mathsf{T}} \\boldsymbol{H}(f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n)) \\Big[ \\partial_1 f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n) \\frac{\\mathbf{x}_{t_n} - \\mathbf{x}}{t_n} + \\partial_2 f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n) \\Big] \\Big\\} + \\mathbb{E}[O(|\\Delta u|)] \\\\",
      "metadata": {
        "chunk_index": 182,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 959
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_183",
      "content": "= \\nabla_{\\boldsymbol{\\theta}} \\mathbb{E}\\{\\lambda(t_n) \\tau'(t_n)[f_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}}, t_{n+1})]^{\\mathsf{T}} \\boldsymbol{H}(f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n)) \\Big[ \\partial_1 f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n) \\frac{\\mathbf{x}_{t_n} - \\mathbf{x}}{t_n} + \\partial_2 f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n) \\Big] \\Big\\} + \\mathbb{E}[O(|\\Delta u|)] \\\\\n= \\nabla_{\\boldsymbol{\\theta}} \\mathbb{E}\\{\\lambda(t_n) \\tau'(t_n)[f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n)] \\Big\\} + \\mathbb{E}[O(|\\Delta u|)] \\\\\n= \\nabla_{\\boldsymbol{\\theta}} \\mathbb{E}\\{\\lambda(t_n) \\tau'(t_n)[f_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n)] \\Big\\} + \\mathbb{E}[O(|\\Delta u|)] \\\\\n= \\nabla_{\\boldsymbol{\\theta}^{-}} \\mathbb{E}[h_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n)] \\Big\\} + \\mathbb{E}[h_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n) \\Big\\} \\\\",
      "metadata": {
        "chunk_index": 183,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 913
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_184",
      "content": "= \\nabla_{\\boldsymbol{\\theta}^{-}} \\mathbb{E}[h_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n)] \\Big\\} + \\mathbb{E}[h_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n) \\Big\\} \\\\\n= \\nabla_{\\boldsymbol{\\theta}^{-}} \\mathbb{E}[h_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n)] \\Big\\} + \\mathbb{E}[h_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}, t_n)] \\\\\n= \\nabla_{\\boldsymbol{\\theta}^{-}} \\mathbb{E}[h_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_n}$$",
      "metadata": {
        "chunk_index": 184,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 444
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_185",
      "content": "Here (i) results from the chain rule, and (ii) follows from Taylor expansion. Taking the limit for both sides of Eq. (33) as  $\\Delta u \\to 0$  or  $N \\to \\infty$  yields the second equality in Eq. (29).\n\nNow we prove the first equality. Applying Taylor expansion again, we obtain\n\n<span id=\"page-22-0\"></span>\n$$(N-1)\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{\\text{CD}}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{-};\\boldsymbol{\\phi}) = \\frac{1}{\\Delta u}\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{\\text{CD}}^{N}(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^{-};\\boldsymbol{\\phi}) = \\frac{1}{\\Delta u}\\nabla_{\\boldsymbol{\\theta}}\\mathbb{E}[\\lambda(t_{n})d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))]$$",
      "metadata": {
        "chunk_index": 185,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 814
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_186",
      "content": "$$= \\frac{1}{\\Delta u}\\mathbb{E}[\\lambda(t_{n})\\nabla_{\\boldsymbol{\\theta}}d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))]$$\n\n$$= \\frac{1}{\\Delta u}\\mathbb{E}[\\lambda(t_{n})\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})^{\\mathsf{T}}\\partial_{1}d(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}),\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))]$$\n\n$$= \\frac{1}{\\Delta u}\\mathbb{E}\\left\\{\\lambda(t_{n})\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})^{\\mathsf{T}}\\left[\\partial_{1}d(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}),\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))\\right.\\right.$$",
      "metadata": {
        "chunk_index": 186,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 957
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_187",
      "content": "$$\\left. + \\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n})) + O(|\\Delta u|^{2})\\right]\\right\\}$$\n\n$$= \\frac{1}{\\Delta u}\\mathbb{E}\\{\\lambda(t_{n})\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})^{\\mathsf{T}}[\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))(\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))] + O(|\\Delta u|^{2})\\}$$",
      "metadata": {
        "chunk_index": 187,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 773
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_188",
      "content": "$$= \\frac{1}{\\Delta u}\\mathbb{E}\\{\\lambda(t_{n})\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t_{n+1}},t_{n+1})^{\\mathsf{T}}[\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t_{n+1}},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\hat{\\mathbf{x}}_{t_{n}}^{\\boldsymbol{\\phi}},t_{n}))] + O(|\\Delta u|^{2})\\}$$\n\n$$= \\frac{1}{\\Delta u}\\mathbb{E}\\{\\lambda(t_{n})[\\nabla_{\\boldsymbol{\\theta}}\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1})]^{\\mathsf{T}}\\boldsymbol{H}(\\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}+t_{n}\\mathbf{z},t_{n}))$$\n\n$$\\cdot [\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}+t_{n}\\mathbf{z},t_{n})]\\} + \\mathbb{E}[O(|\\Delta u|^{2})]$$",
      "metadata": {
        "chunk_index": 188,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 914
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_189",
      "content": "$$\\cdot [\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}+t_{n+1}\\mathbf{z},t_{n+1}) - \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}+t_{n}\\mathbf{z},t_{n})]\\} + \\mathbb{E}[O(|\\Delta u|^{2})]$$\n\nwhere (i) holds because  $\\mathbf{x}_{t_{n+1}} = \\mathbf{x} + t_{n+1}\\mathbf{z}$  and  $\\hat{\\mathbf{x}}_{t_n}^{\\phi} = \\mathbf{x}_{t_{n+1}} - (t_n - t_{n+1})t_{n+1}\\frac{-(\\mathbf{x}_{t_{n+1}} - \\mathbf{x})}{t_{n+1}^2} = \\mathbf{x}_{t_{n+1}} + (t_n - t_{n+1})\\mathbf{z} = \\mathbf{x}_{t_n}\\mathbf{z}$ . Because (i) matches Eq. (32), we can use the same reasoning procedure from Eq. (32) to Eq. (33) to conclude  $\\lim_{N\\to\\infty}(N-1)\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{\\mathrm{CD}}^N(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^-;\\boldsymbol{\\phi}) = \\lim_{N\\to\\infty}(N-1)\\nabla_{\\boldsymbol{\\theta}}\\mathcal{L}_{\\mathrm{CT}}^N(\\boldsymbol{\\theta},\\boldsymbol{\\theta}^-)$ , completing the proof.",
      "metadata": {
        "chunk_index": 189,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 898
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_190",
      "content": "**Remark 9.** Note that  $\\mathcal{L}_{CT}^{\\infty}(\\theta, \\theta^{-})$  does not depend on the diffusion model parameter  $\\phi$  and hence can be optimized without any pre-trained diffusion models.\n\n<span id=\"page-23-1\"></span>![](_page_23_Figure_1.jpeg)\n\nFigure 7: Comparing discrete consistency distillation/training algorithms with continuous counterparts.\n\n**Remark 10.** When  $d(\\mathbf{x}, \\mathbf{y}) = \\|\\mathbf{x} - \\mathbf{y}\\|_2^2$ , the continuous-time consistency training objective becomes\n\n$$\\mathcal{L}_{CT}^{\\infty}(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{-}) = 2\\mathbb{E}\\left[\\frac{\\lambda(t)}{(\\tau^{-1})'(t)}\\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_{t}, t)^{\\mathsf{T}}\\left(\\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t}, t)}{\\partial t} + \\frac{\\partial \\boldsymbol{f}_{\\boldsymbol{\\theta}^{-}}(\\mathbf{x}_{t}, t)}{\\partial \\mathbf{x}_{t}} \\cdot \\frac{\\mathbf{x}_{t} - \\mathbf{x}}{t}\\right)\\right].$$\n(34)",
      "metadata": {
        "chunk_index": 190,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 966
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_191",
      "content": "**Remark 11.** Similar to  $\\mathcal{L}^{\\infty}_{CD}(\\theta, \\theta^-; \\phi)$  in Theorem 5,  $\\mathcal{L}^{\\infty}_{CT}(\\theta, \\theta^-)$  is a pseudo-objective; one cannot track training by monitoring the value of  $\\mathcal{L}^{\\infty}_{CT}(\\theta, \\theta^-)$ , but can still apply gradient descent on this loss function to train a consistency model  $f_{\\theta}(\\mathbf{x},t)$  directly from data. Moreover, the same observation in Remark 8 holds true:  $\\mathcal{L}^{\\infty}_{CT}(\\theta, \\theta^-) = 0$  and  $\\nabla_{\\theta}\\mathcal{L}^{\\infty}_{CT}(\\theta, \\theta^-) = 0$  if  $f_{\\theta}(\\mathbf{x},t)$  matches the ground truth consistency function for the PF ODE.",
      "metadata": {
        "chunk_index": 191,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 675
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_192",
      "content": "#### <span id=\"page-23-0\"></span>**B.3. Experimental Verifications**\n\nTo experimentally verify the efficacy of our continuous-time CD and CT objectives, we train consistency models with a variety of loss functions on CIFAR-10. All results are provided in Fig. 7. We set  $\\lambda(t) = (\\tau^{-1})'(t)$  for all continuous-time experiments. Other hyperparameters are the same as in Table 3. We occasionally modify some hyperparameters for improved performance. For distillation, we compare the following objectives:",
      "metadata": {
        "chunk_index": 192,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 514
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_193",
      "content": "- CD  $(\\ell_2)$ : Consistency distillation  $\\mathcal{L}_{CD}^N$  with N=18 and the  $\\ell_2$  metric.\n- CD  $(\\ell_1)$ : Consistency distillation  $\\mathcal{L}_{CD}^N$  with N=18 and the  $\\ell_1$  metric. We set the learning rate to 2e-4.\n- CD (LPIPS): Consistency distillation  $\\mathcal{L}_{\\text{CD}}^{N}$  with N=18 and the LPIPS metric.\n- $CD^{\\infty}$  ( $\\ell_2$ ): Consistency distillation  $\\mathcal{L}_{CD}^{\\infty}$  in Theorem 3 with the  $\\ell_2$  metric. We set the learning rate to 1e-3 and dropout to 0.13.\n- $CD^{\\infty}$  ( $\\ell_1$ ): Consistency distillation  $\\mathcal{L}_{CD}^{\\infty}$  in Theorem 4 with the  $\\ell_1$  metric. We set the learning rate to 1e-3 and dropout to 0.3.\n- CD $^{\\infty}$  (stopgrad,  $\\ell_2$ ): Consistency distillation  $\\mathcal{L}_{CD}^{\\infty}$  in Theorem 5 with the  $\\ell_2$  metric. We set the learning rate to 5e-6.",
      "metadata": {
        "chunk_index": 193,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 877
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_194",
      "content": "- CD $^{\\infty}$  (stopgrad,  $\\ell_2$ ): Consistency distillation  $\\mathcal{L}_{CD}^{\\infty}$  in Theorem 5 with the  $\\ell_2$  metric. We set the learning rate to 5e-6.\n- $CD^{\\infty}$  (stopgrad, LPIPS): Consistency distillation  $\\mathcal{L}_{CD}^{\\infty}$  in Theorem 5 with the LPIPS metric. We set the learning rate to 5e-6.",
      "metadata": {
        "chunk_index": 194,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 332
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_195",
      "content": "We did not investigate using the LPIPS metric in Theorem 3 because minimizing the resulting objective would require back-propagating through second order derivatives of the VGG network used in LPIPS, which is computationally expensive and prone to numerical instability. As revealed by Fig. 7a, the stopgrad version of continuous-time distillation (Theorem 5) works better than the non-stopgrad version (Theorem 3) for both the LPIPS and  $\\ell_2$  metrics, and the LPIPS metric works the best for all distillation approaches. Additionally, discrete-time consistency distillation outperforms continuous-time\n\n<span id=\"page-24-1\"></span>",
      "metadata": {
        "chunk_index": 195,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 637
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_196",
      "content": "| Hyperparameter         | CIFAR-10 |        | ImageNet $64 \\times 64$ |          | LSUN $256 \\times 256$ |          |\n|------------------------|----------|--------|-------------------------|----------|-----------------------|----------|\n|                        | CD       | CT     | CD                      | CT       | CD                    | CT       |\n| Learning rate          | 4e-4     | 4e-4   | 8e-6                    | 8e-6     | 1e-5                  | 1e-5     |\n| Batch size             | 512      | 512    | 2048                    | 2048     | 2048                  | 2048     |\n| $\\mu$                  | 0        |        | 0.95                    |          | 0.95                  |          |\n| $\\mu_0$                |          | 0.9    |                         | 0.95     |                       | 0.95     |\n| $s_0$                  |          | 2      |                         | 2        |                       | 2        |",
      "metadata": {
        "chunk_index": 196,
        "content_type": "text",
        "has_table": true,
        "has_figure": false,
        "char_count": 951
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_197",
      "content": "| $s_0$                  |          | 2      |                         | 2        |                       | 2        |\n| $s_1$                  |          | 150    |                         | 200      |                       | 150      |\n| N                      | 18       |        | 40                      |          | 40                    |          |\n| ODE solver             | Heun     |        | Heun                    |          | Heun                  |          |\n| EMA decay rate         | 0.9999   | 0.9999 | 0.999943                | 0.999943 | 0.999943              | 0.999943 |\n| Training iterations    | 800k     | 800k   | 600k                    | 800k     | 600k                  | 1000k    |\n| Mixed-Precision (FP16) | No       | No     | Yes                     | Yes      | Yes                   | Yes      |\n| Dropout probability    | 0.0      | 0.0    | 0.0                     | 0.0      | 0.0                   | 0.0      |",
      "metadata": {
        "chunk_index": 197,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 951
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_198",
      "content": "| Dropout probability    | 0.0      | 0.0    | 0.0                     | 0.0      | 0.0                   | 0.0      |\n| Number of GPUs         | 8        | 8      | 64                      | 64       | 64                    | 64       |",
      "metadata": {
        "chunk_index": 198,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 237
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_199",
      "content": "Table 3: Hyperparameters used for training CD and CT models\n\nconsistency distillation, possibly due to the larger variance in continuous-time objectives, and the fact that one can use effective higher-order ODE solvers in discrete-time objectives.\n\nFor consistency training (CT), we find it important to initialize consistency models from a pre-trained EDM model in order to stabilize training when using continuous-time objectives. We hypothesize that this is caused by the large variance in our continuous-time loss functions. For fair comparison, we thus initialize all consistency models from the same pre-trained EDM model on CIFAR-10 for both discrete-time and continuous-time CT, even though the former works well with random initialization. We leave variance reduction techniques for continuous-time CT to future research.\n\nWe empirically compare the following objectives:",
      "metadata": {
        "chunk_index": 199,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 880
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_200",
      "content": "We empirically compare the following objectives:\n\n- CT (LPIPS): Consistency training  $\\mathcal{L}_{\\text{CT}}^N$  with N=120 and the LPIPS metric. We set the learning rate to 4e-4, and the EMA decay rate for the target network to 0.99. We do not use the schedule functions for N and  $\\mu$  here because they cause slower learning when the consistency model is initialized from a pre-trained EDM model.\n- $CT^{\\infty}$  ( $\\ell_2$ ): Consistency training  $\\mathcal{L}_{CT}^{\\infty}$  with the  $\\ell_2$  metric. We set the learning rate to 5e-6.\n- $CT^{\\infty}$  (LPIPS): Consistency training  $\\mathcal{L}_{CT}^{\\infty}$  with the LPIPS metric. We set the learning rate to 5e-6.",
      "metadata": {
        "chunk_index": 200,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 681
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_201",
      "content": "As shown in Fig. 7b, the LPIPS metric leads to improved performance for continuous-time CT. We also find that continuous-time CT outperforms discrete-time CT with the same LPIPS metric. This is likely due to the bias in discrete-time CT, as  $\\Delta t > 0$  in Theorem 2 for discrete-time objectives, whereas continuous-time CT has no bias since it implicitly drives  $\\Delta t$  to 0.",
      "metadata": {
        "chunk_index": 201,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 385
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_202",
      "content": "#### <span id=\"page-24-0\"></span>C. Additional Experimental Details\n\n**Model Architectures** We follow Song et al. (2021); Dhariwal & Nichol (2021) for model architectures. Specifically, we use the NCSN++ architecture in Song et al. (2021) for all CIFAR-10 experiments, and take the corresponding network architectures from Dhariwal & Nichol (2021) when performing experiments on ImageNet  $64 \\times 64$ , LSUN Bedroom  $256 \\times 256$  and LSUN Cat  $256 \\times 256$ .\n\n**Parameterization for Consistency Models** We use the same architectures for consistency models as those used for EDMs. The only difference is we slightly modify the skip connections in EDM to ensure the boundary condition holds for consistency models. Recall that in Section 3 we propose to parameterize a consistency model in the following form:\n\n<span id=\"page-24-2\"></span>\n$$f_{\\theta}(\\mathbf{x}, t) = c_{\\text{skip}}(t)\\mathbf{x} + c_{\\text{out}}(t)F_{\\theta}(\\mathbf{x}, t).$$",
      "metadata": {
        "chunk_index": 202,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 958
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_203",
      "content": "<span id=\"page-24-2\"></span>\n$$f_{\\theta}(\\mathbf{x}, t) = c_{\\text{skip}}(t)\\mathbf{x} + c_{\\text{out}}(t)F_{\\theta}(\\mathbf{x}, t).$$\n\nIn EDM (Karras et al., 2022), authors choose\n\n$$c_{\nm skip}(t) = \frac{\\sigma_{\nm data}^2}{t^2 + \\sigma_{\nm data}^2}, \\quad c_{\nm out}(t) = \frac{\\sigma_{\nm data}t}{\\sqrt{\\sigma_{\nm data}^2 + t^2}},$$\n\nwhere  $\\sigma_{\\text{data}} = 0.5$ . However, this choice of  $c_{\\text{skip}}$  and  $c_{\\text{out}}$  does not satisfy the boundary condition when the smallest time instant  $\\epsilon \\neq 0$ . To remedy this issue, we modify them to\n\n$$c_{\nm skip}(t) = \frac{\\sigma_{\nm data}^2}{(t-\\epsilon)^2 + \\sigma_{\nm data}^2}, \\quad c_{\nm out}(t) = \frac{\\sigma_{\nm data}(t-\\epsilon)}{\\sqrt{\\sigma_{\nm data}^2 + t^2}},$$\n\nwhich clearly satisfies  $c_{\\text{skip}}(\\epsilon) = 1$  and  $c_{\\text{out}}(\\epsilon) = 0$ .",
      "metadata": {
        "chunk_index": 203,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 846
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_204",
      "content": "which clearly satisfies  $c_{\\text{skip}}(\\epsilon) = 1$  and  $c_{\\text{out}}(\\epsilon) = 0$ .\n\nSchedule Functions for Consistency Training As discussed in Section 5, consistency generation requires specifying schedule functions  $N(\\cdot)$  and  $\\mu(\\cdot)$  for best performance. Throughout our experiments, we use schedule functions that take the form below:\n\n$$N(k) = \\left[ \\sqrt{\\frac{k}{K} ((s_1 + 1)^2 - s_0^2) + s_0^2} - 1 \\right] + 1$$\n$$\\mu(k) = \\exp\\left(\\frac{s_0 \\log \\mu_0}{N(k)}\\right),$$\n\nwhere K denotes the total number of training iterations,  $s_0$  denotes the initial discretization steps,  $s_1 > s_0$  denotes the target discretization steps at the end of training, and  $\\mu_0 > 0$  denotes the EMA decay rate at the beginning of model training.",
      "metadata": {
        "chunk_index": 204,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 773
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_205",
      "content": "**Training Details** In both consistency distillation and progressive distillation, we distill EDMs (Karras et al., 2022). We trained these EDMs ourselves according to the specifications given in Karras et al. (2022). The original EDM paper did not provide hyperparameters for the LSUN Bedroom  $256 \\times 256$  and Cat  $256 \\times 256$  datasets, so we mostly used the same hyperparameters as those for the ImageNet  $64 \\times 64$  dataset. The difference is that we trained for 600k and 300k iterations for the LSUN Bedroom and Cat datasets respectively, and reduced the batch size from 4096 to 2048.",
      "metadata": {
        "chunk_index": 205,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 605
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_206",
      "content": "We used the same EMA decay rate for LSUN  $256 \\times 256$  datasets as for the ImageNet  $64 \\times 64$  dataset. For progressive distillation, we used the same training settings as those described in Salimans & Ho (2022) for CIFAR-10 and ImageNet  $64 \\times 64$ . Although the original paper did not test on LSUN  $256 \\times 256$  datasets, we used the same settings for ImageNet  $64 \\times 64$  and found them to work well.",
      "metadata": {
        "chunk_index": 206,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 429
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_207",
      "content": "In all distillation experiments, we initialized the consistency model with pre-trained EDM weights. For consistency training, we initialized the model randomly, just as we did for training the EDMs. We trained all consistency models with the Rectified Adam optimizer (Liu et al., 2019), with no learning rate decay or warm-up, and no weight decay. We also applied EMA to the weights of the online consistency models in both consistency distillation and consistency training, as well as to the weights of the training online consistency models according to Karras et al. (2022). For LSUN  $256 \\times 256$  datasets, we chose the EMA decay rate to be the same as that for ImageNet  $64 \\times 64$ , except for consistency distillation on LSUN Bedroom  $256 \\times 256$ , where we found that using zero EMA worked better.",
      "metadata": {
        "chunk_index": 207,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 819
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_208",
      "content": "When using the LPIPS metric on CIFAR-10 and ImageNet  $64 \\times 64$ , we rescale images to resolution  $224 \\times 224$  with bilinear upsampling before feeding them to the LPIPS network. For LSUN  $256 \\times 256$ , we evaluated LPIPS without rescaling inputs. In addition, we performed horizontal flips for data augmentation for all models and on all datasets. We trained all models on a cluster of Nvidia A100 GPUs. Additional hyperparameters for consistency training and distillation are listed in Table 3.",
      "metadata": {
        "chunk_index": 208,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 511
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_209",
      "content": "#### <span id=\"page-25-0\"></span>D. Additional Results on Zero-Shot Image Editing\n\nWith consistency models, we can perform a variety of zero-shot image editing tasks. As an example, we present additional results on colorization (Fig. 8), super-resolution (Fig. 9), inpainting (Fig. 10), interpolation (Fig. 11), denoising (Fig. 12), and stroke-guided image generation (SDEdit, Meng et al. (2021), Fig. 13). The consistency model used here is trained via consistency distillation on the LSUN Bedroom  $256 \\times 256$ .",
      "metadata": {
        "chunk_index": 209,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 518
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_210",
      "content": "All these image editing tasks, except for image interpolation and denoising, can be performed via a small modification to the multistep sampling algorithm in Algorithm 1. The resulting pseudocode is provided in Algorithm 4. Here y is a reference image that guides sample generation,  $\\Omega$  is a binary mask,  $\\odot$  computes element-wise products, and A is an invertible linear transformation that maps images into a latent space where the conditional information in y is infused into the iterative",
      "metadata": {
        "chunk_index": 210,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 504
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_211",
      "content": "#### <span id=\"page-26-0\"></span>**Algorithm 4** Zero-Shot Image Editing\n\n1: **Input:** Consistency model  $f_{\\theta}(\\cdot,\\cdot)$ , sequence of time points  $t_1 > t_2 > \\cdots > t_N$ , reference image y, invertible linear transformation A, and binary image mask  $\\Omega$ \n\n2: \n$$\\mathbf{y} \\leftarrow \\mathbf{A}^{-1}[(\\mathbf{A}\\mathbf{y}) \\odot (1 - \\mathbf{\\Omega}) + \\mathbf{0} \\odot \\mathbf{\\Omega}]$$\n\n3: Sample  $\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{y}, t_1^2 \\mathbf{I})$ \n\n4:  $\\mathbf{x} \\leftarrow \\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}, t_1)$ \n\n5:  $\\mathbf{x} \\leftarrow \\mathbf{A}^{-1}[(\\mathbf{A}\\mathbf{y}) \\odot (1 - \\mathbf{\\Omega}) + (\\mathbf{A}\\mathbf{x}) \\odot \\mathbf{\\Omega}]$ \n\n6: **for** n = 2 **to** N **do** \n\nSample  $\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{x}, (t_n^2 - \\epsilon^2)\\mathbf{I})$ 7:",
      "metadata": {
        "chunk_index": 211,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 829
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_212",
      "content": "6: **for** n = 2 **to** N **do** \n\nSample  $\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{x}, (t_n^2 - \\epsilon^2)\\mathbf{I})$ 7:\n\n $\\mathbf{x} \\leftarrow \\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}, t_n)$   $\\mathbf{x} \\leftarrow \\mathbf{A}^{-1}[(\\mathbf{A}\\mathbf{y}) \\odot (1 - \\mathbf{\\Omega}) + (\\mathbf{A}\\mathbf{x}) \\odot \\mathbf{\\Omega}]$ \n\n10: **end for** \n\n11: **Output: x** \n\ngeneration procedure by masking with  $\\Omega$ . Unless otherwise stated, we choose\n\n$$t_i = \\left(T^{1/\\rho} + \\frac{i-1}{N-1} (\\epsilon^{1/\\rho} - T^{1/\\rho})\\right)^{\\rho}$$\n\nin our experiments, where N=40 for LSUN Bedroom  $256\\times256$ .\n\nBelow we describe how to perform each task using Algorithm 4.\n\n**Inpainting** When using Algorithm 4 for inpainting, we let y be an image where missing pixels are masked out,  $\\Omega$  be a binary mask where 1 indicates the missing pixels, and A be the identity transformation.",
      "metadata": {
        "chunk_index": 212,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 900
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_213",
      "content": "<span id=\"page-26-1\"></span>Colorization The algorithm for image colorization is similar, as colorization becomes a special case of inpainting once we transform data into a decoupled space. Specifically, let  $\\mathbf{y} \\in \\mathbb{R}^{h \\times w \\times 3}$  be a gray-scale image that we aim to colorize, where all channels of y are assumed to be the same, i.e., y[:,:,0] = y[:,:,1] = y[:,:,2] in NumPy notation. In our experiments, each channel of this gray scale image is obtained from a colorful image by averaging the RGB channels with\n\n$$0.2989R + 0.5870G + 0.1140B$$\n.\n\nWe define  $\\Omega \\in \\{0,1\\}^{h \\times w \\times 3}$  to be a binary mask such that\n\n$$\\Omega[i, j, k] = \\begin{cases} 1, & k = 1 \\text{ or } 2 \\\\ 0, & k = 0 \\end{cases}.$$",
      "metadata": {
        "chunk_index": 213,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 751
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_214",
      "content": "We define  $\\Omega \\in \\{0,1\\}^{h \\times w \\times 3}$  to be a binary mask such that\n\n$$\\Omega[i, j, k] = \\begin{cases} 1, & k = 1 \\text{ or } 2 \\\\ 0, & k = 0 \\end{cases}.$$\n\nLet  $Q \\in \\mathbb{R}^{3\\times 3}$  be an orthogonal matrix whose first column is proportional to the vector (0.2989, 0.5870, 0.1140). This orthogonal matrix can be obtained easily via QR decomposition, and we use the following in our experiments\n\n$$\\mathbf{Q} = \\begin{pmatrix} 0.4471 & -0.8204 & 0.3563 \\\\ 0.8780 & 0.4785 & 0 \\\\ 0.1705 & -0.3129 & -0.9343 \\end{pmatrix}.$$\n\nWe then define the linear transformation  $A: \\mathbf{x} \\in \\mathbb{R}^{h \\times w \\times 3} \\mapsto \\mathbf{y} \\in \\mathbb{R}^{h \\times w \\times 3}$ , where\n\n$$\\mathbf{y}[i,j,k] = \\sum_{l=0}^{2} \\mathbf{x}[i,j,l] \\mathbf{Q}[l,k].$$\n\nBecause Q is orthogonal, the inversion  $A^{-1}: \\mathbf{y} \\in \\mathbb{R}^{h \\times w} \\mapsto \\mathbf{x} \\in \\mathbb{R}^{h \\times w \\times 3}$  is easy to compute, where",
      "metadata": {
        "chunk_index": 214,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 958
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_215",
      "content": "Because Q is orthogonal, the inversion  $A^{-1}: \\mathbf{y} \\in \\mathbb{R}^{h \\times w} \\mapsto \\mathbf{x} \\in \\mathbb{R}^{h \\times w \\times 3}$  is easy to compute, where\n\n$$\\mathbf{x}[i,j,k] = \\sum_{l=0}^{2} \\mathbf{y}[i,j,l] \\mathbf{Q}[k,l].$$\n\nWith A and  $\\Omega$  defined as above, we can now use Algorithm 4 for image colorization.\n\n**Super-resolution** With a similar strategy, we employ Algorithm 4 for image super-resolution. For simplicity, we assume that the down-sampled image is obtained by averaging non-overlapping patches of size  $p \\times p$ . Suppose the shape of full resolution images is  $h \\times w \\times 3$ . Let  $\\mathbf{y} \\in \\mathbb{R}^{h \\times w \\times 3}$  denote a low-resolution image naively up-sampled to full resolution, where pixels in each non-overlapping patch share the same value. Additionally, let  $\\mathbf{\\Omega} \\in \\{0,1\\}^{h/p \\times w/p \\times p^2 \\times 3}$  be a binary mask such that",
      "metadata": {
        "chunk_index": 215,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 938
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_216",
      "content": "$$\\mathbf{\\Omega}[i,j,k,l] = \\begin{cases} 1, & k \\geqslant 1 \\\\ 0, & k = 0 \\end{cases}.$$\n\nSimilar to image colorization, super-resolution requires an orthogonal matrix  $Q \\in \\mathbb{R}^{p^2 \\times p^2}$  whose first column is  $(1/p, 1/p, \\cdots, 1/p)$ . This orthogonal matrix can be obtained with QR decomposition. To perform super-resolution, we define the linear transformation  $A : \\mathbf{x} \\in \\mathbb{R}^{h \\times w \\times 3} \\mapsto \\mathbf{y} \\in \\mathbb{R}^{h/p \\times w/p \\times p^2 \\times 3}$ , where\n\n$$\\mathbf{y}[i,j,k,l] = \\sum_{m=0}^{p^2-1} \\mathbf{x}[i \\times p + (m-m \\bmod p)/p, j \\times p + m \\bmod p, l] \\mathbf{Q}[m,k].$$\n\nThe inverse transformation  $A^{-1}: \\mathbf{y} \\in \\mathbb{R}^{h/p \\times w/p \\times p^2 \\times 3} \\mapsto \\mathbf{x} \\in \\mathbb{R}^{h \\times w \\times 3}$  is easy to derive, with\n\n$$\\mathbf{x}[i,j,k,l] = \\sum_{m=0}^{p^2-1} \\mathbf{y}[i \\times p + (m-m \\bmod p)/p, j \\times p + m \\bmod p, l] \\mathbf{Q}[k,m].$$",
      "metadata": {
        "chunk_index": 216,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 964
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_217",
      "content": "$$\\mathbf{x}[i,j,k,l] = \\sum_{m=0}^{p^2-1} \\mathbf{y}[i \\times p + (m-m \\bmod p)/p, j \\times p + m \\bmod p, l] \\mathbf{Q}[k,m].$$\n\nAbove definitions of A and  $\\Omega$  allow us to use Algorithm 4 for image super-resolution.\n\n**Stroke-guided image generation** We can also use Algorithm 4 for stroke-guided image generation as introduced in SDEdit (Meng et al., 2021). Specifically, we let  $\\mathbf{y} \\in \\mathbb{R}^{h \\times w \\times 3}$  be a stroke painting. We set  $\\mathbf{A} = \\mathbf{I}$ , and define  $\\mathbf{\\Omega} \\in \\mathbb{R}^{h \\times w \\times 3}$  as a matrix of ones. In our experiments, we set  $t_1 = 5.38$  and  $t_2 = 2.24$ , with N = 2.",
      "metadata": {
        "chunk_index": 217,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 662
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_218",
      "content": "**Denoising** It is possible to denoise images perturbed with various scales of Gaussian noise using a single consistency model. Suppose the input image  $\\mathbf{x}$  is perturbed with  $\\mathcal{N}(\\mathbf{0}; \\sigma^2 \\mathbf{I})$ . As long as  $\\sigma \\in [\\epsilon, T]$ , we can evaluate  $\\mathbf{f}_{\\theta}(\\mathbf{x}, \\sigma)$  to produce the denoised image.\n\n**Interpolation** We can interpolate between two images generated by consistency models. Suppose the first sample  $\\mathbf{x}_1$  is produced by noise vector  $\\mathbf{z}_1$ , and the second sample  $\\mathbf{x}_2$  is produced by noise vector  $\\mathbf{z}_2$ . In other words,  $\\mathbf{x}_1 = f_{\\theta}(\\mathbf{z}_1, T)$  and  $\\mathbf{x}_2 = f_{\\theta}(\\mathbf{z}_2, T)$ . To interpolate between  $\\mathbf{x}_1$  and  $\\mathbf{x}_2$ , we first use spherical linear interpolation to get\n\n$$\\mathbf{z} = \\frac{\\sin[(1-\\alpha)\\psi]}{\\sin(\\psi)}\\mathbf{z}_1 + \\frac{\\sin(\\alpha\\psi)}{\\sin(\\psi)}\\mathbf{z}_2,$$",
      "metadata": {
        "chunk_index": 218,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 979
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_219",
      "content": "$$\\mathbf{z} = \\frac{\\sin[(1-\\alpha)\\psi]}{\\sin(\\psi)}\\mathbf{z}_1 + \\frac{\\sin(\\alpha\\psi)}{\\sin(\\psi)}\\mathbf{z}_2,$$\n\nwhere  $\\alpha \\in [0, 1]$  and  $\\psi = \\arccos(\\frac{\\mathbf{z}_1^\\mathsf{T} \\mathbf{z}_2}{\\|\\mathbf{z}_1\\|_2 \\|\\mathbf{z}_2\\|_2})$ , then evaluate  $f_{\\theta}(\\mathbf{z}, T)$  to produce the interpolated image.",
      "metadata": {
        "chunk_index": 219,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 335
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_220",
      "content": "#### <span id=\"page-27-0\"></span>E. Additional Samples from Consistency Models\n\nWe provide additional samples from consistency distillation (CD) and consistency training (CT) on CIFAR-10 (Figs. 14 and 18), ImageNet  $64 \\times 64$  (Figs. 15 and 19), LSUN Bedroom  $256 \\times 256$  (Figs. 16 and 20) and LSUN Cat  $256 \\times 256$  (Figs. 17 and 21).\n\n<span id=\"page-28-0\"></span>![](_page_28_Picture_1.jpeg)\n\nFigure 8: Gray-scale images (left), colorized images by a consistency model (middle), and ground truth (right).\n\n<span id=\"page-29-0\"></span>![](_page_29_Picture_1.jpeg)\n\nFigure 9: Downsampled images of resolution 32 ˆ 32 (left), full resolution (256 ˆ 256) images generated by a consistency model (middle), and ground truth images of resolution 256 ˆ 256 (right).\n\n<span id=\"page-30-0\"></span>![](_page_30_Picture_1.jpeg)\n\nFigure 10: Masked images (left), imputed images by a consistency model (middle), and ground truth (right).\n\n<span id=\"page-31-0\"></span>![](_page_31_Picture_1.jpeg)",
      "metadata": {
        "chunk_index": 220,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 999
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_221",
      "content": "Figure 10: Masked images (left), imputed images by a consistency model (middle), and ground truth (right).\n\n<span id=\"page-31-0\"></span>![](_page_31_Picture_1.jpeg)\n\nFigure 11: Interpolating between leftmost and rightmost images with spherical linear interpolation. All samples are generated by a consistency model trained on LSUN Bedroom 256 ˆ 256.\n\n<span id=\"page-32-0\"></span>![](_page_32_Figure_1.jpeg)\n\nFigure 12: Single-step denoising with a consistency model. The leftmost images are ground truth. For every two rows, the top row shows noisy images with different noise levels, while the bottom row gives denoised images.\n\n<span id=\"page-33-0\"></span>![](_page_33_Picture_1.jpeg)\n\nFigure 13: SDEdit with a consistency model. The leftmost images are stroke painting inputs. Images on the right side are the results of stroke-guided image generation (SDEdit).\n\n<span id=\"page-34-0\"></span>![](_page_34_Figure_1.jpeg)",
      "metadata": {
        "chunk_index": 221,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 921
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_222",
      "content": "<span id=\"page-34-0\"></span>![](_page_34_Figure_1.jpeg)\n\nFigure 14: Uncurated samples from CIFAR-10 32 ˆ 32. All corresponding samples use the same initial noise.\n\n<span id=\"page-35-0\"></span>![](_page_35_Figure_1.jpeg)\n\nFigure 15: Uncurated samples from ImageNet 64 ˆ 64. All corresponding samples use the same initial noise.\n\n<span id=\"page-36-0\"></span>![](_page_36_Picture_1.jpeg)\n\n(a) EDM (FID=3.57)\n\n![](_page_36_Picture_3.jpeg)\n\n(b) CD with single-step generation (FID=7.80)\n\n![](_page_36_Picture_5.jpeg)\n\n(c) CD with two-step generation (FID=5.22)\n\nFigure 16: Uncurated samples from LSUN Bedroom 256 ˆ 256. All corresponding samples use the same initial noise.\n\n<span id=\"page-37-0\"></span>![](_page_37_Picture_1.jpeg)\n\n(a) EDM (FID=6.69)\n\n![](_page_37_Picture_3.jpeg)\n\n(b) CD with single-step generation (FID=10.99)\n\n![](_page_37_Picture_5.jpeg)\n\n(c) CD with two-step generation (FID=8.84)",
      "metadata": {
        "chunk_index": 222,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 898
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_223",
      "content": "(a) EDM (FID=6.69)\n\n![](_page_37_Picture_3.jpeg)\n\n(b) CD with single-step generation (FID=10.99)\n\n![](_page_37_Picture_5.jpeg)\n\n(c) CD with two-step generation (FID=8.84)\n\nFigure 17: Uncurated samples from LSUN Cat 256 ˆ 256. All corresponding samples use the same initial noise.\n\n<span id=\"page-38-0\"></span>![](_page_38_Figure_1.jpeg)\n\nFigure 18: Uncurated samples from CIFAR-10 32 ˆ 32. All corresponding samples use the same initial noise.\n\n<span id=\"page-39-0\"></span>![](_page_39_Figure_1.jpeg)\n\nFigure 19: Uncurated samples from ImageNet 64 ˆ 64. All corresponding samples use the same initial noise.\n\n<span id=\"page-40-0\"></span>![](_page_40_Picture_1.jpeg)\n\n(a) EDM (FID=3.57)\n\n![](_page_40_Picture_3.jpeg)\n\n(b) CT with single-step generation (FID=16.00)\n\n![](_page_40_Picture_5.jpeg)\n\n(c) CT with two-step generation (FID=7.80)\n\nFigure 20: Uncurated samples from LSUN Bedroom 256 ˆ 256. All corresponding samples use the same initial noise.",
      "metadata": {
        "chunk_index": 223,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 950
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_224",
      "content": "![](_page_40_Picture_5.jpeg)\n\n(c) CT with two-step generation (FID=7.80)\n\nFigure 20: Uncurated samples from LSUN Bedroom 256 ˆ 256. All corresponding samples use the same initial noise.\n\n<span id=\"page-41-0\"></span>![](_page_41_Picture_1.jpeg)\n\n(a) EDM (FID=6.69)\n\n![](_page_41_Picture_3.jpeg)\n\n(b) CT with single-step generation (FID=20.70)\n\n![](_page_41_Picture_5.jpeg)\n\n(c) CT with two-step generation (FID=11.76)\n\nFigure 21: Uncurated samples from LSUN Cat 256 ˆ 256. All corresponding samples use the same initial noise.",
      "metadata": {
        "chunk_index": 224,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 525
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_table_1",
      "content": "| METHOD                                          | NFE (↓) | FID (↓) | IS (↑) | METHOD                                 | NFE (↓) | FID (↓) | Prec. (†) | Rec. (†) |\n|-------------------------------------------------|---------|---------|--------|----------------------------------------|---------|---------|-----------|----------|\n| Diffusion + Samplers                            |         |         |        | ImageNet $64 \\times 64$                |         |         |           |          |\n| DDIM (Song et al., 2020)                        | 50      | 4.67    |        | PD <sup>†</sup> (Salimans & Ho, 2022)  | 1       | 15.39   | 0.59      | 0.62     |\n| DDIM (Song et al., 2020)                        | 20      | 6.84    |        | DFNO <sup>†</sup> (Zheng et al., 2022) | 1       | 8.35    |           |          |\n| DDIM (Song et al., 2020)                        | 10      | 8.23    |        | $\\mathbf{C}\\mathbf{D}^{\\dagger}$       | 1       | 6.20    | 0.68      | 0.63     |\n| DPM-solver-2 (Lu et al., 2022)                  | 10      | 5.94    |        | PD <sup>†</sup> (Salimans & Ho, 2022)  | 2       | 8.95    | 0.63      | 0.65     |\n| DPM-solver-fast (Lu et al., 2022)               | 10      | 4.70    |        | $\\mathbf{C}\\mathbf{D}^{\\dagger}$       | 2       | 4.70    | 0.69      | 0.64     |\n| 3-DEIS (Zhang & Chen, 2022)                     | 10      | 4.17    |        | ADM (Dhariwal & Nichol, 2021)          | 250     | 2.07    | 0.74      | 0.63     |\n| Diffusion + Distillation                        |         |         |        | EDM (Karras et al., 2022)              | 79      | 2.44    | 0.71      | 0.67     |\n| Knowledge Distillation* (Luhman & Luhman, 2021) | 1       | 9.36    |        | BigGAN-deep (Brock et al., 2019)       | 1       | 4.06    | 0.79      | 0.48     |\n| DFNO* (Zheng et al., 2022)                      | 1       | 4.12    |        | CT                                     | 1       | 13.0    | 0.71      | 0.47     |\n| 1-Rectified Flow (+distill)* (Liu et al., 2022) | 1       | 6.18    | 9.08   | CT                                     | 2       | 11.1    | 0.69      | 0.56     |\n| 2-Rectified Flow (+distill)* (Liu et al., 2022) | 1       | 4.85    | 9.01   |                                        |         |         |           |          |\n| 3-Rectified Flow (+distill)* (Liu et al., 2022) | 1       | 5.21    | 8.79   | LSUN Bedroom 256 × 256                 |         |         |           |          |\n| PD (Salimans & Ho, 2022)                        | 1       | 8.34    | 8.69   | PD <sup>†</sup> (Salimans & Ho, 2022)  | 1       | 16.92   | 0.47      | 0.27     |\n| CD                                              | 1       | 3.55    | 9.48   | PD <sup>†</sup> (Salimans & Ho, 2022)  | 2       | 8.47    | 0.56      | 0.39     |\n| PD (Salimans & Ho, 2022)                        | 2       | 5.58    | 9.05   | $\\mathbf{C}\\mathbf{D}^{\\dagger}$       | 1       | 7.80    | 0.66      | 0.34     |\n| CD                                              | 2       | 2.93    | 9.75   | CD <sup>†</sup>                        | 2       | 5.22    | 0.68      | 0.39     |\n| Direct Generation                               |         |         |        | DDPM (Ho et al., 2020)                 | 1000    | 4.89    | 0.60      | 0.45     |\n| BigGAN (Brock et al., 2019)                     | 1       | 14.7    | 9.22   | ADM (Dhariwal & Nichol, 2021)          | 1000    | 1.90    | 0.66      | 0.51     |\n| Diffusion GAN (Xiao et al., 2022)               | 1       | 14.6    | 8.93   | EDM (Karras et al., 2022)              | 79      | 3.57    | 0.66      | 0.45     |\n| AutoGAN (Gong et al., 2019)                     | 1       | 12.4    | 8.55   | PGGAN (Karras et al., 2018)            | 1       | 8.34    |           |          |\n| E2GAN (Tian et al., 2020)                       | 1       | 11.3    | 8.51   | PG-SWGAN (Wu et al., 2019)             | 1       | 8.0     |           |          |\n| ViTGAN (Lee et al., 2021)                       | 1       | 6.66    | 9.30   | TDPM (GAN) (Zheng et al., 2023)        | 1       | 5.24    |           |          |\n| TransGAN (Jiang et al., 2021)                   | 1       | 9.26    | 9.05   | StyleGAN2 (Karras et al., 2020)        | 1       | 2.35    | 0.59      | 0.48     |\n| StyleGAN2-ADA (Karras et al., 2020)             | 1       | 2.92    | 9.83   | CT                                     | 1       | 16.0    | 0.60      | 0.17     |\n| StyleGAN-XL (Sauer et al., 2022)                | 1       | 1.85    |        | CT                                     | 2       | 7.85    | 0.68      | 0.33     |\n| Score SDE (Song et al., 2021)                   | 2000    | 2.20    | 9.89   | <b>LSUN Cat 256</b> × <b>256</b>       |         |         |           |          |\n| DDPM (Ho et al., 2020)                          | 1000    | 3.17    | 9.46   |                                        | 1       | 20.6    | 0.51      | 0.25     |\n| LSGM (Vahdat et al., 2021)                      | 147     | 2.10    |        | PD <sup>†</sup> (Salimans & Ho, 2022)  | -       | 29.6    | 0.51      | 0.25     |\n| PFGM (Xu et al., 2022)                          | 110     | 2.35    | 9.68   | PD <sup>†</sup> (Salimans & Ho, 2022)  | 2       | 15.5    | 0.59      | 0.36     |\n| EDM (Karras et al., 2022)                       | 35      | 2.04    | 9.84   | CD <sup>†</sup>                        | 1       | 11.0    | 0.65      | 0.36     |\n| 1-Rectified Flow (Liu et al., 2022)             | 1       | 378     | 1.13   | $\\mathbf{C}\\mathbf{D}^{\\dagger}$       | 2       | 8.84    | 0.66      | 0.40     |\n| Glow (Kingma & Dhariwal, 2018)                  | 1       | 48.9    | 3.92   | DDPM (Ho et al., 2020)                 | 1000    | 17.1    | 0.53      | 0.48     |\n| Residual Flow (Chen et al., 2019)               | 1       | 46.4    |        | ADM (Dhariwal & Nichol, 2021)          | 1000    | 5.57    | 0.63      | 0.52     |\n| GLFlow (Xiao et al., 2019)                      | 1       | 44.6    |        | EDM (Karras et al., 2022)              | 79      | 6.69    | 0.70      | 0.43     |\n| DenseFlow (Grcić et al., 2021)                  | 1       | 34.9    |        | PGGAN (Karras et al., 2018)            | 1       | 37.5    |           |          |\n| DC-VAE (Parmar et al., 2021)                    | 1       | 17.9    | 8.20   | StyleGAN2 (Karras et al., 2020)        | 1       | 7.25    | 0.58      | 0.43     |\n| CT                                              | 1       | 8.70    | 8.49   | CT                                     | 1       | 20.7    | 0.56      | 0.23     |\n| CT                                              | 2       | 5.83    | 8.85   | CT                                     | 2       | 11.7    | 0.63      | 0.36     |\n",
      "metadata": {
        "content_type": "table",
        "table_id": "table_1",
        "headers": [
          "METHOD",
          "NFE (↓)",
          "FID (↓)",
          "IS (↑)",
          "METHOD",
          "NFE (↓)",
          "FID (↓)",
          "Prec. (†)",
          "Rec. (†)"
        ],
        "row_count": 39
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_table_2",
      "content": "| 1 | Introduction                                                | 1  |\n|---|-------------------------------------------------------------|----|\n| 2 | Diffusion Models                                            | 2  |\n| 3 | Consistency Models                                          | 3  |\n| 4 | Training Consistency Models via Distillation                | 4  |\n| 5 | Training Consistency Models in Isolation                    | 5  |\n| 6 | Experiments                                                 | 6  |\n|   | 6.1<br>Training Consistency Models                          | 6  |\n|   | 6.2<br>Few-Step Image Generation<br>                        | 7  |\n|   | 6.3<br>Zero-Shot Image Editing<br>                          | 9  |\n| 7 | Conclusion                                                  | 9  |\n|   | Appendices                                                  | 15 |\n|   | Appendix A<br>Proofs                                        | 15 |\n|   | A.1<br>Notations<br>                                        | 15 |\n|   | A.2<br>Consistency Distillation                             | 15 |\n|   | A.3<br>Consistency Training<br>                             | 16 |\n|   | Appendix B<br>Continuous-Time Extensions                    | 18 |\n|   | B.1<br>Consistency Distillation in Continuous Time          | 18 |\n|   | B.2<br>Consistency Training in Continuous Time<br>          | 22 |\n|   | B.3<br>Experimental Verifications<br>                       | 24 |\n|   | Appendix C<br>Additional Experimental Details               | 25 |\n|   | Model Architectures<br>                                     | 25 |\n|   | Parameterization for Consistency Models<br>                 | 25 |\n|   | Schedule Functions for Consistency Training<br>             | 26 |\n|   | Training Details                                            | 26 |\n|   | Appendix D<br>Additional Results on Zero-Shot Image Editing | 26 |\n|   | Inpainting                                                  | 27 |\n|   | Colorization<br>                                            | 27 |\n|   | Super-resolution<br>                                        | 28 |\n",
      "metadata": {
        "content_type": "table",
        "table_id": "table_2",
        "headers": [
          "1",
          "Introduction",
          "1"
        ],
        "row_count": 27
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_table_3",
      "content": "| Appendix E | Additional Samples from Consistency Models | 28 |\n|------------|--------------------------------------------|----|\n|            | Interpolation                              | 28 |\n|            | Denoising                                  | 28 |\n|            | Stroke-guided image generation             | 28 |\n",
      "metadata": {
        "content_type": "table",
        "table_id": "table_3",
        "headers": [
          "Appendix E",
          "Additional Samples from Consistency Models",
          "28"
        ],
        "row_count": 3
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_table_4",
      "content": "| Hyperparameter         | CIFAR-10 |        | ImageNet $64 \\times 64$ |          | LSUN $256 \\times 256$ |          |\n|------------------------|----------|--------|-------------------------|----------|-----------------------|----------|\n|                        | CD       | CT     | CD                      | CT       | CD                    | CT       |\n| Learning rate          | 4e-4     | 4e-4   | 8e-6                    | 8e-6     | 1e-5                  | 1e-5     |\n| Batch size             | 512      | 512    | 2048                    | 2048     | 2048                  | 2048     |\n| $\\mu$                  | 0        |        | 0.95                    |          | 0.95                  |          |\n| $\\mu_0$                |          | 0.9    |                         | 0.95     |                       | 0.95     |\n| $s_0$                  |          | 2      |                         | 2        |                       | 2        |\n| $s_1$                  |          | 150    |                         | 200      |                       | 150      |\n| N                      | 18       |        | 40                      |          | 40                    |          |\n| ODE solver             | Heun     |        | Heun                    |          | Heun                  |          |\n| EMA decay rate         | 0.9999   | 0.9999 | 0.999943                | 0.999943 | 0.999943              | 0.999943 |\n| Training iterations    | 800k     | 800k   | 600k                    | 800k     | 600k                  | 1000k    |\n| Mixed-Precision (FP16) | No       | No     | Yes                     | Yes      | Yes                   | Yes      |\n| Dropout probability    | 0.0      | 0.0    | 0.0                     | 0.0      | 0.0                   | 0.0      |\n| Number of GPUs         | 8        | 8      | 64                      | 64       | 64                    | 64       |\n",
      "metadata": {
        "content_type": "table",
        "table_id": "table_4",
        "headers": [
          "Hyperparameter",
          "CIFAR-10",
          "",
          "ImageNet $64 \\times 64$",
          "",
          "LSUN $256 \\times 256$",
          ""
        ],
        "row_count": 14
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_1",
      "content": "<span id=\"page-0-0\"></span>\n\nFigure: Given a Probability Flow (PF) ODE that smoothly converts data to noise, we learn to map any point (*e.g*., xt, xt <sup>1</sup> , and x<sup>T</sup> ) on the ODE trajectory to its origin (*e.g*., x0) for generative modeling. Models of these mappings are called consistency models, as their outputs are trained to be consistent for points on the same trajectory.\n\nVisual Description: The figure illustrates a Probability Flow (PF) ODE framework for generative modeling, depicting the transformation from data to noise along a trajectory. The horizontal axis is labeled from \"Data\" to \"Noise,\" with points marked as \\( x_0 \\), \\( x_t \\), \\( x_T \\), indicating various stages in the progression; the vertical aspect is implied through the transformation representation. Key findings suggest that the consistency models \\( f_\\theta(x_t, t) \\), \\( f_\\theta(x_{t'}, t') \\), and \\( f_\\theta(x_T, T) \\) are designed to map these trajectory points back to their original data point \\( x_0 \\), ensuring output consistency across the same trajectories.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_1",
        "caption": "Given a Probability Flow (PF) ODE that smoothly converts data to noise, we learn to map any point (*e.g*., xt, xt <sup>1</sup> , and x<sup>T</sup> ) on the ODE trajectory to its origin (*e.g*., x0) for generative modeling. Models of these mappings are called consistency models, as their outputs are trained to be consistent for points on the same trajectory.",
        "image_key": "_page_0_Figure_9.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_2",
      "content": "<span id=\"page-2-0\"></span>\n\nFigure: Consistency models are trained to map points on any trajectory of the PF ODE to the trajectory's origin.\n\nVisual Description: The figure illustrates the mapping of points on any trajectory of the forward ODE (Ordinary Differential Equation) to their respective origins, with the vertical axis labeled \"Data\" and \"Noise\" on the horizontal axis. The green trajectories represent the noise-free data stream while the red paths indicate the transformations along the ODE trajectories. Key findings suggest that the consistency model effectively learns to represent the dynamics of the data, facilitating the reconstruction of the origin from noisy observations.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_2",
        "caption": "Consistency models are trained to map points on any trajectory of the PF ODE to the trajectory's origin.",
        "image_key": "_page_2_Picture_1.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_3",
      "content": "<span id=\"page-6-0\"></span>\n\nFigure: Various factors that affect consistency distillation (CD) and consistency training (CT) on CIFAR-10. The best configuration for CD is LPIPS, Heun ODE solver, and N \" 18. Our adaptive schedule functions for N and µ make CT converge significantly faster than fixing them to be constants during the course of optimization.\n\nVisual Description: The academic figure presents the evaluation of factors affecting consistency distillation (CD) and consistency training (CT) on the CIFAR-10 dataset. The x-axis across all subfigures represents \"Training iterations (×10,000),\" while the y-axis displays the \"FID\" (Fréchet Inception Distance), a measure of generative model performance. Key findings include that the best configuration for CD is the LPIPS metric with the Heun ODE solver and \\( N = 18 \\) (subfigure b), and that using adaptive schedules for parameters \\( N \\) and \\( \\mu \\) in CT leads to significantly faster convergence compared to fixed values (subfigure d).",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_3",
        "caption": "Various factors that affect consistency distillation (CD) and consistency training (CT) on CIFAR-10. The best configuration for CD is LPIPS, Heun ODE solver, and N \" 18. Our adaptive schedule functions for N and µ make CT converge significantly faster than fixing them to be constants during the course of optimization.",
        "image_key": "_page_6_Figure_1.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_4",
      "content": "<span id=\"page-6-1\"></span>\n\nFigure: Multistep image generation with consistency distillation (CD). CD outperforms progressive distillation (PD) across all datasets and sampling steps. The only exception is single-step generation on Bedroom 256 ˆ 256.\n\nVisual Description: The figure presents the Fréchet Inception Distance (FID) across four datasets: CIFAR-10, ImageNet 64 × 64, Bedroom 256 × 256, and Cat 256 × 256, with varying numbers of sampling steps (1 to 4). The data shows that consistency distillation (CD) consistently outperforms progressive distillation (PD) in all cases, except for the single-step generation on the Bedroom dataset. The axes are labeled with the number of sampling steps on the x-axis and FID on the y-axis, highlighting the superior performance of CD (both ℓ2 and LPIPS) as the number of sampling steps increases.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_4",
        "caption": "Multistep image generation with consistency distillation (CD). CD outperforms progressive distillation (PD) across all datasets and sampling steps. The only exception is single-step generation on Bedroom 256 ˆ 256.",
        "image_key": "_page_6_Figure_3.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_5",
      "content": "<span id=\"page-7-1\"></span>\n\nFigure: Samples generated by EDM (top), CT + single-step generation (middle), and CT + 2-step generation (Bottom). All corresponding images are generated from the same initial noise.\n\nVisual Description: The figure presents images generated through three different methodologies: EDM (top row), CT with single-step generation (middle row), and CT with 2-step generation (bottom row). The vertical axis corresponds to the generation techniques, while the horizontal axis represents the image categories (snails, flamingos, bedrooms, and cats). Key findings indicate notable variations in image quality and detail, with the CT + 2-step generation (bottom) showcasing the highest fidelity and coherence compared to the other methods, suggesting improved performance in generating realistic images from initial noise.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_5",
        "caption": "Samples generated by EDM (top), CT + single-step generation (middle), and CT + 2-step generation (Bottom). All corresponding images are generated from the same initial noise.",
        "image_key": "_page_7_Figure_4.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_6",
      "content": "<span id=\"page-8-0\"></span>\n\nFigure: (a) *Left*: The gray-scale image. *Middle*: Colorized images. *Right*: The ground-truth image.\n\nVisual Description: The figure presents a comparison of different image processing techniques applied to a bedroom scene, displayed in three groups: a gray-scale image on the left, colorized versions in the middle, and the ground-truth image on the right. The key findings indicate that while the gray-scale image lacks detail and vibrancy, the colorized images can significantly enhance the visual appeal, although they vary in accuracy when compared to the ground-truth image. This highlights the effectiveness of colorization techniques in image restoration and the importance of selecting the right method for achieving results that closely match real-life visuals.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_6",
        "caption": "(a) *Left*: The gray-scale image. *Middle*: Colorized images. *Right*: The ground-truth image.",
        "image_key": "_page_8_Figure_1.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_7",
      "content": "Figure: (b) *Left*: The downsampled image (32 ˆ 32). *Middle*: Full resolution images (256 ˆ 256). *Right*: The ground-truth image (256 ˆ 256).\n\nVisual Description: The academic figure presents three variations of a room image: on the left is a downsampled image (32x32), in the middle is a full-resolution image (256x256), and on the right is the ground-truth high-resolution image (256x256). The leftmost image showcases significant blurriness and loss of detail compared to the progressively sharper and clearer versions in the middle and right, illustrating the impact of resolution on image quality. The key finding indicates that higher resolution provides better representation and clarity, essential for various visual applications.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_7",
        "caption": "(b) *Left*: The downsampled image (32 ˆ 32). *Middle*: Full resolution images (256 ˆ 256). *Right*: The ground-truth image (256 ˆ 256).",
        "image_key": "_page_8_Figure_3.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_8",
      "content": "Figure: (c) *Left*: A stroke input provided by users. *Right*: Stroke-guided image generation.\n\nVisual Description: The figure illustrates a comparison between user-provided stroke inputs on the left and the resulting stroke-guided image generation on the right. The left image shows a simple delineation of shapes, while the right series displays increasingly detailed renderings of a bedroom environment. Key findings suggest that the stroke inputs facilitate a successful translation into coherent, contextually relevant images, demonstrating the efficacy of stroke-guided methods in visual generation tasks.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_8",
        "caption": "(c) *Left*: A stroke input provided by users. *Right*: Stroke-guided image generation.",
        "image_key": "_page_8_Figure_5.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_9",
      "content": "<span id=\"page-23-1\"></span>\n\nFigure: Comparing discrete consistency distillation/training algorithms with continuous counterparts.\n\nVisual Description: The figures compare the performance of discrete consistency distillation (panel a) and consistency training (panel b) algorithms across different training iterations, with the vertical axis representing Fréchet Inception Distance (FID), a common measure of data quality. In the first panel, discrete methods show varied trajectories, with the lowest FID achieved by the continuous \\( CT(LPIPS) \\) method, while the second panel illustrates how consistency training consistently yields lower FID values than its discrete counterpart across iterations. Key findings suggest that continuous methods outperform discrete ones in reducing FID, indicating better data quality and consistency over training iterations.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_9",
        "caption": "Comparing discrete consistency distillation/training algorithms with continuous counterparts.",
        "image_key": "_page_23_Figure_1.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_10",
      "content": "<span id=\"page-28-0\"></span>\n\nFigure: Gray-scale images (left), colorized images by a consistency model (middle), and ground truth (right).\n\nVisual Description: The figure presents a comparison of gray-scale images (left), colorized images generated by a consistency model (middle), and the ground truth images (right). Each column represents different scenes with varying levels of detail and color accuracy, indicating that the colorization model captures some aspects of the original scene well, while others may lose fidelity. Key findings include that the model effectively retains structural elements but may introduce artifacts or incorrect color representations in certain cases.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_10",
        "caption": "Gray-scale images (left), colorized images by a consistency model (middle), and ground truth (right).",
        "image_key": "_page_28_Picture_1.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_11",
      "content": "<span id=\"page-29-0\"></span>\n\nFigure: Downsampled images of resolution 32 ˆ 32 (left), full resolution (256 ˆ 256) images generated by a consistency model (middle), and ground truth images of resolution 256 ˆ 256 (right).\n\nVisual Description: The figure compares three sets of images: downsampled images of resolution 32x32 (left), images produced by a consistency model at full resolution (256x256) (middle), and ground truth images at full resolution (right). The downsampled images exhibit significant loss of detail, while the consistency model's outputs show improved quality but may still lack some fidelity compared to the ground truth images. Key findings suggest that the consistency model enhances resolution effectively, bridging the gap between low-quality and high-quality images.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_11",
        "caption": "Downsampled images of resolution 32 ˆ 32 (left), full resolution (256 ˆ 256) images generated by a consistency model (middle), and ground truth images of resolution 256 ˆ 256 (right).",
        "image_key": "_page_29_Picture_1.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_12",
      "content": "<span id=\"page-30-0\"></span>\n\nFigure: Masked images (left), imputed images by a consistency model (middle), and ground truth (right).\n\nVisual Description: The academic figure presents a comparison of three types of images: masked (left), imputed by a consistency model (middle), and ground truth (right), arranged in a grid format. The vertical axis is labeled with \"S\" indicating various categories or samples, while the horizontal axis delineates the three types of image representations. Key findings suggest that the imputed images demonstrate improvements in clarity and detail over the masked images, approaching the quality observed in ground truth representations, highlighting the effectiveness of the consistency model in image enhancement.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_12",
        "caption": "Masked images (left), imputed images by a consistency model (middle), and ground truth (right).",
        "image_key": "_page_30_Picture_1.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_13",
      "content": "<span id=\"page-31-0\"></span>\n\nFigure: Interpolating between leftmost and rightmost images with spherical linear interpolation. All samples are generated by a consistency model trained on LSUN Bedroom 256 ˆ 256.\n\nVisual Description: This academic figure displays multiple generated images of bedrooms, arranged in a grid that shows a transition from the leftmost to the rightmost image through spherical linear interpolation. The x-axis represents the interpolation parameter, marking the progression from one image style to another, while the y-axis does not have specific measurable labels but showcases various generated samples. The key finding illustrates how smoothly transitioning the visual attributes and styles of the bedrooms can be achieved, demonstrating the effectiveness of the consistency model trained on LSUN Bedroom imagery.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_13",
        "caption": "Interpolating between leftmost and rightmost images with spherical linear interpolation. All samples are generated by a consistency model trained on LSUN Bedroom 256 ˆ 256.",
        "image_key": "_page_31_Picture_1.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_14",
      "content": "<span id=\"page-32-0\"></span>\n\nFigure: Single-step denoising with a consistency model. The leftmost images are ground truth. For every two rows, the top row shows noisy images with different noise levels, while the bottom row gives denoised images.\n\nVisual Description: The figure illustrates the effectiveness of a single-step denoising process using a consistency model. The leftmost column displays the ground truth images, while the subsequent columns showcase noisy images (top rows) at varying noise levels, with corresponding denoised images displayed below. The data trend highlights a clear reduction in noise, as evidenced by improved clarity and detail in the denoised images across different noise levels, suggesting the model's robust performance in preserving image fidelity.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_14",
        "caption": "Single-step denoising with a consistency model. The leftmost images are ground truth. For every two rows, the top row shows noisy images with different noise levels, while the bottom row gives denoised images.",
        "image_key": "_page_32_Figure_1.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_15",
      "content": "<span id=\"page-33-0\"></span>\n\nFigure: SDEdit with a consistency model. The leftmost images are stroke painting inputs. Images on the right side are the results of stroke-guided image generation (SDEdit).\n\nVisual Description: The academic figure illustrates the effectiveness of SDEdit with a consistency model, showcasing a grid of images. The leftmost column displays stroke painting inputs, while the subsequent columns depict the results of stroke-guided image generation. The trend observed is a progressive refinement and enhancement of details from the initial strokes to the final generated images, demonstrating the model's capacity to convert stylized inputs into coherent, high-fidelity visuals. Key findings emphasize the model's ability to maintain visual consistency and fidelity across varied input styles.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_15",
        "caption": "SDEdit with a consistency model. The leftmost images are stroke painting inputs. Images on the right side are the results of stroke-guided image generation (SDEdit).",
        "image_key": "_page_33_Picture_1.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_16",
      "content": "<span id=\"page-34-0\"></span>\n\nFigure: Uncurated samples from CIFAR-10 32 ˆ 32. All corresponding samples use the same initial noise.\n\nVisual Description: The figure presents uncurated samples from the CIFAR-10 dataset, arranged into three categories: EDM (top row), CD with single-step generation (middle row), and CD with two-step generation (bottom row). Each category is associated with a Fréchet Inception Distance (FID) score indicating image quality, with lower scores reflecting better fidelity. Key findings suggest that EDM yields the best visual quality (FID = 2.04), followed by two-step CD generation (FID = 2.93), and single-step CD generation, which performs the worst (FID = 3.55).",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_16",
        "caption": "Uncurated samples from CIFAR-10 32 ˆ 32. All corresponding samples use the same initial noise.",
        "image_key": "_page_34_Figure_1.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_17",
      "content": "<span id=\"page-35-0\"></span>\n\nFigure: Uncurated samples from ImageNet 64 ˆ 64. All corresponding samples use the same initial noise.\n\nVisual Description: The figure showcases generated samples from different methods applied to uncurated images from the ImageNet dataset (64x64 resolution). The three sections depict various techniques: (a) EDM with a Fréchet Inception Distance (FID) of 2.44, (b) CD with single-step generation (FID 6.20), and (c) CD with two-step generation (FID 4.70). The data trends suggest that EDM yields the highest image quality as indicated by the lowest FID score, while CD techniques show relatively higher FID scores, indicating a decrease in generated image fidelity.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_17",
        "caption": "Uncurated samples from ImageNet 64 ˆ 64. All corresponding samples use the same initial noise.",
        "image_key": "_page_35_Figure_1.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_18",
      "content": "<span id=\"page-36-0\"></span>\n\nFigure: (a) EDM (FID=3.57)\n\nVisual Description: The figure appears to present a diverse array of bedroom designs, likely for analysis in a study on aesthetic preferences or functionality in interior design. The axes likely categorize different styles or characteristics of bedrooms, such as color palette, layout, and furniture type. A key finding may highlight trends in common themes among the images, for instance, a predominance of neutral colors or minimalist designs, indicating contemporary preferences in bedroom aesthetics.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_18",
        "caption": "(a) EDM (FID=3.57)",
        "image_key": "_page_36_Picture_1.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_19",
      "content": "Figure: (b) CD with single-step generation (FID=7.80)\n\nVisual Description: The figure presents a grid of images showcasing various bedroom interiors generated through a single-step generation method, indicated by a Fréchet Inception Distance (FID) of 7.80. Although the axes are not explicitly labeled, the visual quality of the generated images reflects the model's performance in recreating realistic bedroom environments. The key finding suggests that the single-step generation allows for diverse and coherent image outputs, demonstrating the effectiveness of the method in generating high-quality interior representations.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_19",
        "caption": "(b) CD with single-step generation (FID=7.80)",
        "image_key": "_page_36_Picture_3.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_20",
      "content": "Figure: (c) CD with two-step generation (FID=5.22)\n\nVisual Description: The academic figure showcases a series of images depicting various bedroom interiors generated through a two-step generation process, with a Focused Inception Distance (FID) score of 5.22. Axes labels are not explicitly provided, but the imagery emphasizes the diversity and realism of generated bedroom designs. Key findings suggest that the generation method effectively produces a wide variety of aesthetically pleasing and spatially coherent interior layouts, demonstrating significant advancements in the quality of synthetic image generation.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_20",
        "caption": "(c) CD with two-step generation (FID=5.22)",
        "image_key": "_page_36_Picture_5.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_21",
      "content": "<span id=\"page-37-0\"></span>\n\nFigure: (a) EDM (FID=6.69)\n\nVisual Description: The figure presents a collection of cat images intended for analysis in an experiment related to Emotion Detection Model (EDM) with a FID of 6.69. The axes likely represent variables related to the emotional appeal or recognition accuracy, although specific labels are not visible. Key findings may highlight trends in how different cat expressions and appearances can influence emotional responses, possibly suggesting that certain features correlate with higher engagement or emotional resonance among viewers.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_21",
        "caption": "(a) EDM (FID=6.69)",
        "image_key": "_page_37_Picture_1.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_22",
      "content": "Figure: (b) CD with single-step generation (FID=10.99)\n\nVisual Description: The figure presents a grid of various cat images generated using a single-step generation approach, corresponding to a Fréchet Inception Distance (FID) score of 10.99. The images exhibit a range of styles and qualities, highlighting the generative model's capability to produce diverse cat representations. Observations suggest that while some images maintain realistic features, others display noticeable artifacts or distortions, indicating areas for improvement in generation quality.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_22",
        "caption": "(b) CD with single-step generation (FID=10.99)",
        "image_key": "_page_37_Picture_3.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_23",
      "content": "Figure: (c) CD with two-step generation (FID=8.84)",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_23",
        "caption": "(c) CD with two-step generation (FID=8.84)",
        "image_key": "_page_37_Picture_5.jpeg",
        "has_vision_description": false
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_24",
      "content": "<span id=\"page-38-0\"></span>\n\nFigure: Uncurated samples from CIFAR-10 32 ˆ 32. All corresponding samples use the same initial noise.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_24",
        "caption": "Uncurated samples from CIFAR-10 32 ˆ 32. All corresponding samples use the same initial noise.",
        "image_key": "_page_38_Figure_1.jpeg",
        "has_vision_description": false
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_25",
      "content": "<span id=\"page-39-0\"></span>\n\nFigure: Uncurated samples from ImageNet 64 ˆ 64. All corresponding samples use the same initial noise.\n\nVisual Description: The figure presents three rows of generated images from different methods: (a) EDM, (b) CT with single-step generation, and (c) CT with two-step generation. The axes of the figure don’t depict specific numerical data but instead categorize the image generation techniques, with Fréchet Inception Distance (FID) scores indicated for each method (EDM: 2.44, CT single-step: 12.96, CT two-step: 11.12). The results demonstrate that EDM achieves the highest quality images, as indicated by the lowest FID score, while the CT methods produce more diverse but lower quality images.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_25",
        "caption": "Uncurated samples from ImageNet 64 ˆ 64. All corresponding samples use the same initial noise.",
        "image_key": "_page_39_Figure_1.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_26",
      "content": "<span id=\"page-40-0\"></span>\n\nFigure: (a) EDM (FID=3.57)\n\nVisual Description: The academic figure presents a grid of various bedroom images, with the context of \"EDM (FID=3.57)\" potentially indicating a focus on a dataset of bedroom designs evaluated through an explanatory data model (EDM) with a Fréchet Inception Distance (FID) score of 3.57. While there are no specific axes or quantitative datasets apparent, the arrangement suggests a variety of design aesthetics and spatial layouts. Key findings might indicate diversity in bedroom styles and arrangements, showcasing how different design choices influence visual appeal or functionality.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_26",
        "caption": "(a) EDM (FID=3.57)",
        "image_key": "_page_40_Picture_1.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_27",
      "content": "Figure: (b) CT with single-step generation (FID=16.00)\n\nVisual Description: The figure presents a collage of generated bedroom images, showcasing the results of a single-step generation technique. The x-axis represents diverse styles of bedroom interiors, while the y-axis could reflect various quality metrics, such as realism or aesthetic appeal. Notably, the findings indicate an FID (Fréchet Inception Distance) score of 16.00, suggesting a moderate level of visual fidelity in the generated images, with potential areas for improvement in capturing complex details and textures.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_27",
        "caption": "(b) CT with single-step generation (FID=16.00)",
        "image_key": "_page_40_Picture_3.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_28",
      "content": "Figure: (c) CT with two-step generation (FID=7.80)",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_28",
        "caption": "(c) CT with two-step generation (FID=7.80)",
        "image_key": "_page_40_Picture_5.jpeg",
        "has_vision_description": false
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_29",
      "content": "<span id=\"page-41-0\"></span>\n\nFigure: (a) EDM (FID=6.69)",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_29",
        "caption": "(a) EDM (FID=6.69)",
        "image_key": "_page_41_Picture_1.jpeg",
        "has_vision_description": false
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_30",
      "content": "Figure: (b) CT with single-step generation (FID=20.70)",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_30",
        "caption": "(b) CT with single-step generation (FID=20.70)",
        "image_key": "_page_41_Picture_3.jpeg",
        "has_vision_description": false
      }
    },
    {
      "id": "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_figure_31",
      "content": "Figure: (c) CT with two-step generation (FID=11.76)",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_31",
        "caption": "(c) CT with two-step generation (FID=11.76)",
        "image_key": "_page_41_Picture_5.jpeg",
        "has_vision_description": false
      }
    }
  ],
  "tables": [
    {
      "table_id": "table_1",
      "headers": [
        "METHOD",
        "NFE (↓)",
        "FID (↓)",
        "IS (↑)",
        "METHOD",
        "NFE (↓)",
        "FID (↓)",
        "Prec. (†)",
        "Rec. (†)"
      ],
      "rows": [
        [
          "Diffusion + Samplers",
          "",
          "",
          "",
          "ImageNet $64 \\times 64$",
          "",
          "",
          "",
          ""
        ],
        [
          "DDIM (Song et al., 2020)",
          "50",
          "4.67",
          "",
          "PD <sup>†</sup> (Salimans & Ho, 2022)",
          "1",
          "15.39",
          "0.59",
          "0.62"
        ],
        [
          "DDIM (Song et al., 2020)",
          "20",
          "6.84",
          "",
          "DFNO <sup>†</sup> (Zheng et al., 2022)",
          "1",
          "8.35",
          "",
          ""
        ],
        [
          "DDIM (Song et al., 2020)",
          "10",
          "8.23",
          "",
          "$\\mathbf{C}\\mathbf{D}^{\\dagger}$",
          "1",
          "6.20",
          "0.68",
          "0.63"
        ],
        [
          "DPM-solver-2 (Lu et al., 2022)",
          "10",
          "5.94",
          "",
          "PD <sup>†</sup> (Salimans & Ho, 2022)",
          "2",
          "8.95",
          "0.63",
          "0.65"
        ],
        [
          "DPM-solver-fast (Lu et al., 2022)",
          "10",
          "4.70",
          "",
          "$\\mathbf{C}\\mathbf{D}^{\\dagger}$",
          "2",
          "4.70",
          "0.69",
          "0.64"
        ],
        [
          "3-DEIS (Zhang & Chen, 2022)",
          "10",
          "4.17",
          "",
          "ADM (Dhariwal & Nichol, 2021)",
          "250",
          "2.07",
          "0.74",
          "0.63"
        ],
        [
          "Diffusion + Distillation",
          "",
          "",
          "",
          "EDM (Karras et al., 2022)",
          "79",
          "2.44",
          "0.71",
          "0.67"
        ],
        [
          "Knowledge Distillation* (Luhman & Luhman, 2021)",
          "1",
          "9.36",
          "",
          "BigGAN-deep (Brock et al., 2019)",
          "1",
          "4.06",
          "0.79",
          "0.48"
        ],
        [
          "DFNO* (Zheng et al., 2022)",
          "1",
          "4.12",
          "",
          "CT",
          "1",
          "13.0",
          "0.71",
          "0.47"
        ],
        [
          "1-Rectified Flow (+distill)* (Liu et al., 2022)",
          "1",
          "6.18",
          "9.08",
          "CT",
          "2",
          "11.1",
          "0.69",
          "0.56"
        ],
        [
          "2-Rectified Flow (+distill)* (Liu et al., 2022)",
          "1",
          "4.85",
          "9.01",
          "",
          "",
          "",
          "",
          ""
        ],
        [
          "3-Rectified Flow (+distill)* (Liu et al., 2022)",
          "1",
          "5.21",
          "8.79",
          "LSUN Bedroom 256 × 256",
          "",
          "",
          "",
          ""
        ],
        [
          "PD (Salimans & Ho, 2022)",
          "1",
          "8.34",
          "8.69",
          "PD <sup>†</sup> (Salimans & Ho, 2022)",
          "1",
          "16.92",
          "0.47",
          "0.27"
        ],
        [
          "CD",
          "1",
          "3.55",
          "9.48",
          "PD <sup>†</sup> (Salimans & Ho, 2022)",
          "2",
          "8.47",
          "0.56",
          "0.39"
        ],
        [
          "PD (Salimans & Ho, 2022)",
          "2",
          "5.58",
          "9.05",
          "$\\mathbf{C}\\mathbf{D}^{\\dagger}$",
          "1",
          "7.80",
          "0.66",
          "0.34"
        ],
        [
          "CD",
          "2",
          "2.93",
          "9.75",
          "CD <sup>†</sup>",
          "2",
          "5.22",
          "0.68",
          "0.39"
        ],
        [
          "Direct Generation",
          "",
          "",
          "",
          "DDPM (Ho et al., 2020)",
          "1000",
          "4.89",
          "0.60",
          "0.45"
        ],
        [
          "BigGAN (Brock et al., 2019)",
          "1",
          "14.7",
          "9.22",
          "ADM (Dhariwal & Nichol, 2021)",
          "1000",
          "1.90",
          "0.66",
          "0.51"
        ],
        [
          "Diffusion GAN (Xiao et al., 2022)",
          "1",
          "14.6",
          "8.93",
          "EDM (Karras et al., 2022)",
          "79",
          "3.57",
          "0.66",
          "0.45"
        ],
        [
          "AutoGAN (Gong et al., 2019)",
          "1",
          "12.4",
          "8.55",
          "PGGAN (Karras et al., 2018)",
          "1",
          "8.34",
          "",
          ""
        ],
        [
          "E2GAN (Tian et al., 2020)",
          "1",
          "11.3",
          "8.51",
          "PG-SWGAN (Wu et al., 2019)",
          "1",
          "8.0",
          "",
          ""
        ],
        [
          "ViTGAN (Lee et al., 2021)",
          "1",
          "6.66",
          "9.30",
          "TDPM (GAN) (Zheng et al., 2023)",
          "1",
          "5.24",
          "",
          ""
        ],
        [
          "TransGAN (Jiang et al., 2021)",
          "1",
          "9.26",
          "9.05",
          "StyleGAN2 (Karras et al., 2020)",
          "1",
          "2.35",
          "0.59",
          "0.48"
        ],
        [
          "StyleGAN2-ADA (Karras et al., 2020)",
          "1",
          "2.92",
          "9.83",
          "CT",
          "1",
          "16.0",
          "0.60",
          "0.17"
        ],
        [
          "StyleGAN-XL (Sauer et al., 2022)",
          "1",
          "1.85",
          "",
          "CT",
          "2",
          "7.85",
          "0.68",
          "0.33"
        ],
        [
          "Score SDE (Song et al., 2021)",
          "2000",
          "2.20",
          "9.89",
          "<b>LSUN Cat 256</b> × <b>256</b>",
          "",
          "",
          "",
          ""
        ],
        [
          "DDPM (Ho et al., 2020)",
          "1000",
          "3.17",
          "9.46",
          "",
          "1",
          "20.6",
          "0.51",
          "0.25"
        ],
        [
          "LSGM (Vahdat et al., 2021)",
          "147",
          "2.10",
          "",
          "PD <sup>†</sup> (Salimans & Ho, 2022)",
          "-",
          "29.6",
          "0.51",
          "0.25"
        ],
        [
          "PFGM (Xu et al., 2022)",
          "110",
          "2.35",
          "9.68",
          "PD <sup>†</sup> (Salimans & Ho, 2022)",
          "2",
          "15.5",
          "0.59",
          "0.36"
        ],
        [
          "EDM (Karras et al., 2022)",
          "35",
          "2.04",
          "9.84",
          "CD <sup>†</sup>",
          "1",
          "11.0",
          "0.65",
          "0.36"
        ],
        [
          "1-Rectified Flow (Liu et al., 2022)",
          "1",
          "378",
          "1.13",
          "$\\mathbf{C}\\mathbf{D}^{\\dagger}$",
          "2",
          "8.84",
          "0.66",
          "0.40"
        ],
        [
          "Glow (Kingma & Dhariwal, 2018)",
          "1",
          "48.9",
          "3.92",
          "DDPM (Ho et al., 2020)",
          "1000",
          "17.1",
          "0.53",
          "0.48"
        ],
        [
          "Residual Flow (Chen et al., 2019)",
          "1",
          "46.4",
          "",
          "ADM (Dhariwal & Nichol, 2021)",
          "1000",
          "5.57",
          "0.63",
          "0.52"
        ],
        [
          "GLFlow (Xiao et al., 2019)",
          "1",
          "44.6",
          "",
          "EDM (Karras et al., 2022)",
          "79",
          "6.69",
          "0.70",
          "0.43"
        ],
        [
          "DenseFlow (Grcić et al., 2021)",
          "1",
          "34.9",
          "",
          "PGGAN (Karras et al., 2018)",
          "1",
          "37.5",
          "",
          ""
        ],
        [
          "DC-VAE (Parmar et al., 2021)",
          "1",
          "17.9",
          "8.20",
          "StyleGAN2 (Karras et al., 2020)",
          "1",
          "7.25",
          "0.58",
          "0.43"
        ],
        [
          "CT",
          "1",
          "8.70",
          "8.49",
          "CT",
          "1",
          "20.7",
          "0.56",
          "0.23"
        ],
        [
          "CT",
          "2",
          "5.83",
          "8.85",
          "CT",
          "2",
          "11.7",
          "0.63",
          "0.36"
        ]
      ],
      "markdown": "| METHOD                                          | NFE (↓) | FID (↓) | IS (↑) | METHOD                                 | NFE (↓) | FID (↓) | Prec. (†) | Rec. (†) |\n|-------------------------------------------------|---------|---------|--------|----------------------------------------|---------|---------|-----------|----------|\n| Diffusion + Samplers                            |         |         |        | ImageNet $64 \\times 64$                |         |         |           |          |\n| DDIM (Song et al., 2020)                        | 50      | 4.67    |        | PD <sup>†</sup> (Salimans & Ho, 2022)  | 1       | 15.39   | 0.59      | 0.62     |\n| DDIM (Song et al., 2020)                        | 20      | 6.84    |        | DFNO <sup>†</sup> (Zheng et al., 2022) | 1       | 8.35    |           |          |\n| DDIM (Song et al., 2020)                        | 10      | 8.23    |        | $\\mathbf{C}\\mathbf{D}^{\\dagger}$       | 1       | 6.20    | 0.68      | 0.63     |\n| DPM-solver-2 (Lu et al., 2022)                  | 10      | 5.94    |        | PD <sup>†</sup> (Salimans & Ho, 2022)  | 2       | 8.95    | 0.63      | 0.65     |\n| DPM-solver-fast (Lu et al., 2022)               | 10      | 4.70    |        | $\\mathbf{C}\\mathbf{D}^{\\dagger}$       | 2       | 4.70    | 0.69      | 0.64     |\n| 3-DEIS (Zhang & Chen, 2022)                     | 10      | 4.17    |        | ADM (Dhariwal & Nichol, 2021)          | 250     | 2.07    | 0.74      | 0.63     |\n| Diffusion + Distillation                        |         |         |        | EDM (Karras et al., 2022)              | 79      | 2.44    | 0.71      | 0.67     |\n| Knowledge Distillation* (Luhman & Luhman, 2021) | 1       | 9.36    |        | BigGAN-deep (Brock et al., 2019)       | 1       | 4.06    | 0.79      | 0.48     |\n| DFNO* (Zheng et al., 2022)                      | 1       | 4.12    |        | CT                                     | 1       | 13.0    | 0.71      | 0.47     |\n| 1-Rectified Flow (+distill)* (Liu et al., 2022) | 1       | 6.18    | 9.08   | CT                                     | 2       | 11.1    | 0.69      | 0.56     |\n| 2-Rectified Flow (+distill)* (Liu et al., 2022) | 1       | 4.85    | 9.01   |                                        |         |         |           |          |\n| 3-Rectified Flow (+distill)* (Liu et al., 2022) | 1       | 5.21    | 8.79   | LSUN Bedroom 256 × 256                 |         |         |           |          |\n| PD (Salimans & Ho, 2022)                        | 1       | 8.34    | 8.69   | PD <sup>†</sup> (Salimans & Ho, 2022)  | 1       | 16.92   | 0.47      | 0.27     |\n| CD                                              | 1       | 3.55    | 9.48   | PD <sup>†</sup> (Salimans & Ho, 2022)  | 2       | 8.47    | 0.56      | 0.39     |\n| PD (Salimans & Ho, 2022)                        | 2       | 5.58    | 9.05   | $\\mathbf{C}\\mathbf{D}^{\\dagger}$       | 1       | 7.80    | 0.66      | 0.34     |\n| CD                                              | 2       | 2.93    | 9.75   | CD <sup>†</sup>                        | 2       | 5.22    | 0.68      | 0.39     |\n| Direct Generation                               |         |         |        | DDPM (Ho et al., 2020)                 | 1000    | 4.89    | 0.60      | 0.45     |\n| BigGAN (Brock et al., 2019)                     | 1       | 14.7    | 9.22   | ADM (Dhariwal & Nichol, 2021)          | 1000    | 1.90    | 0.66      | 0.51     |\n| Diffusion GAN (Xiao et al., 2022)               | 1       | 14.6    | 8.93   | EDM (Karras et al., 2022)              | 79      | 3.57    | 0.66      | 0.45     |\n| AutoGAN (Gong et al., 2019)                     | 1       | 12.4    | 8.55   | PGGAN (Karras et al., 2018)            | 1       | 8.34    |           |          |\n| E2GAN (Tian et al., 2020)                       | 1       | 11.3    | 8.51   | PG-SWGAN (Wu et al., 2019)             | 1       | 8.0     |           |          |\n| ViTGAN (Lee et al., 2021)                       | 1       | 6.66    | 9.30   | TDPM (GAN) (Zheng et al., 2023)        | 1       | 5.24    |           |          |\n| TransGAN (Jiang et al., 2021)                   | 1       | 9.26    | 9.05   | StyleGAN2 (Karras et al., 2020)        | 1       | 2.35    | 0.59      | 0.48     |\n| StyleGAN2-ADA (Karras et al., 2020)             | 1       | 2.92    | 9.83   | CT                                     | 1       | 16.0    | 0.60      | 0.17     |\n| StyleGAN-XL (Sauer et al., 2022)                | 1       | 1.85    |        | CT                                     | 2       | 7.85    | 0.68      | 0.33     |\n| Score SDE (Song et al., 2021)                   | 2000    | 2.20    | 9.89   | <b>LSUN Cat 256</b> × <b>256</b>       |         |         |           |          |\n| DDPM (Ho et al., 2020)                          | 1000    | 3.17    | 9.46   |                                        | 1       | 20.6    | 0.51      | 0.25     |\n| LSGM (Vahdat et al., 2021)                      | 147     | 2.10    |        | PD <sup>†</sup> (Salimans & Ho, 2022)  | -       | 29.6    | 0.51      | 0.25     |\n| PFGM (Xu et al., 2022)                          | 110     | 2.35    | 9.68   | PD <sup>†</sup> (Salimans & Ho, 2022)  | 2       | 15.5    | 0.59      | 0.36     |\n| EDM (Karras et al., 2022)                       | 35      | 2.04    | 9.84   | CD <sup>†</sup>                        | 1       | 11.0    | 0.65      | 0.36     |\n| 1-Rectified Flow (Liu et al., 2022)             | 1       | 378     | 1.13   | $\\mathbf{C}\\mathbf{D}^{\\dagger}$       | 2       | 8.84    | 0.66      | 0.40     |\n| Glow (Kingma & Dhariwal, 2018)                  | 1       | 48.9    | 3.92   | DDPM (Ho et al., 2020)                 | 1000    | 17.1    | 0.53      | 0.48     |\n| Residual Flow (Chen et al., 2019)               | 1       | 46.4    |        | ADM (Dhariwal & Nichol, 2021)          | 1000    | 5.57    | 0.63      | 0.52     |\n| GLFlow (Xiao et al., 2019)                      | 1       | 44.6    |        | EDM (Karras et al., 2022)              | 79      | 6.69    | 0.70      | 0.43     |\n| DenseFlow (Grcić et al., 2021)                  | 1       | 34.9    |        | PGGAN (Karras et al., 2018)            | 1       | 37.5    |           |          |\n| DC-VAE (Parmar et al., 2021)                    | 1       | 17.9    | 8.20   | StyleGAN2 (Karras et al., 2020)        | 1       | 7.25    | 0.58      | 0.43     |\n| CT                                              | 1       | 8.70    | 8.49   | CT                                     | 1       | 20.7    | 0.56      | 0.23     |\n| CT                                              | 2       | 5.83    | 8.85   | CT                                     | 2       | 11.7    | 0.63      | 0.36     |\n"
    },
    {
      "table_id": "table_2",
      "headers": [
        "1",
        "Introduction",
        "1"
      ],
      "rows": [
        [
          "2",
          "Diffusion Models",
          "2"
        ],
        [
          "3",
          "Consistency Models",
          "3"
        ],
        [
          "4",
          "Training Consistency Models via Distillation",
          "4"
        ],
        [
          "5",
          "Training Consistency Models in Isolation",
          "5"
        ],
        [
          "6",
          "Experiments",
          "6"
        ],
        [
          "",
          "6.1<br>Training Consistency Models",
          "6"
        ],
        [
          "",
          "6.2<br>Few-Step Image Generation<br>",
          "7"
        ],
        [
          "",
          "6.3<br>Zero-Shot Image Editing<br>",
          "9"
        ],
        [
          "7",
          "Conclusion",
          "9"
        ],
        [
          "",
          "Appendices",
          "15"
        ],
        [
          "",
          "Appendix A<br>Proofs",
          "15"
        ],
        [
          "",
          "A.1<br>Notations<br>",
          "15"
        ],
        [
          "",
          "A.2<br>Consistency Distillation",
          "15"
        ],
        [
          "",
          "A.3<br>Consistency Training<br>",
          "16"
        ],
        [
          "",
          "Appendix B<br>Continuous-Time Extensions",
          "18"
        ],
        [
          "",
          "B.1<br>Consistency Distillation in Continuous Time",
          "18"
        ],
        [
          "",
          "B.2<br>Consistency Training in Continuous Time<br>",
          "22"
        ],
        [
          "",
          "B.3<br>Experimental Verifications<br>",
          "24"
        ],
        [
          "",
          "Appendix C<br>Additional Experimental Details",
          "25"
        ],
        [
          "",
          "Model Architectures<br>",
          "25"
        ],
        [
          "",
          "Parameterization for Consistency Models<br>",
          "25"
        ],
        [
          "",
          "Schedule Functions for Consistency Training<br>",
          "26"
        ],
        [
          "",
          "Training Details",
          "26"
        ],
        [
          "",
          "Appendix D<br>Additional Results on Zero-Shot Image Editing",
          "26"
        ],
        [
          "",
          "Inpainting",
          "27"
        ],
        [
          "",
          "Colorization<br>",
          "27"
        ],
        [
          "",
          "Super-resolution<br>",
          "28"
        ]
      ],
      "markdown": "| 1 | Introduction                                                | 1  |\n|---|-------------------------------------------------------------|----|\n| 2 | Diffusion Models                                            | 2  |\n| 3 | Consistency Models                                          | 3  |\n| 4 | Training Consistency Models via Distillation                | 4  |\n| 5 | Training Consistency Models in Isolation                    | 5  |\n| 6 | Experiments                                                 | 6  |\n|   | 6.1<br>Training Consistency Models                          | 6  |\n|   | 6.2<br>Few-Step Image Generation<br>                        | 7  |\n|   | 6.3<br>Zero-Shot Image Editing<br>                          | 9  |\n| 7 | Conclusion                                                  | 9  |\n|   | Appendices                                                  | 15 |\n|   | Appendix A<br>Proofs                                        | 15 |\n|   | A.1<br>Notations<br>                                        | 15 |\n|   | A.2<br>Consistency Distillation                             | 15 |\n|   | A.3<br>Consistency Training<br>                             | 16 |\n|   | Appendix B<br>Continuous-Time Extensions                    | 18 |\n|   | B.1<br>Consistency Distillation in Continuous Time          | 18 |\n|   | B.2<br>Consistency Training in Continuous Time<br>          | 22 |\n|   | B.3<br>Experimental Verifications<br>                       | 24 |\n|   | Appendix C<br>Additional Experimental Details               | 25 |\n|   | Model Architectures<br>                                     | 25 |\n|   | Parameterization for Consistency Models<br>                 | 25 |\n|   | Schedule Functions for Consistency Training<br>             | 26 |\n|   | Training Details                                            | 26 |\n|   | Appendix D<br>Additional Results on Zero-Shot Image Editing | 26 |\n|   | Inpainting                                                  | 27 |\n|   | Colorization<br>                                            | 27 |\n|   | Super-resolution<br>                                        | 28 |\n"
    },
    {
      "table_id": "table_3",
      "headers": [
        "Appendix E",
        "Additional Samples from Consistency Models",
        "28"
      ],
      "rows": [
        [
          "",
          "Interpolation",
          "28"
        ],
        [
          "",
          "Denoising",
          "28"
        ],
        [
          "",
          "Stroke-guided image generation",
          "28"
        ]
      ],
      "markdown": "| Appendix E | Additional Samples from Consistency Models | 28 |\n|------------|--------------------------------------------|----|\n|            | Interpolation                              | 28 |\n|            | Denoising                                  | 28 |\n|            | Stroke-guided image generation             | 28 |\n"
    },
    {
      "table_id": "table_4",
      "headers": [
        "Hyperparameter",
        "CIFAR-10",
        "",
        "ImageNet $64 \\times 64$",
        "",
        "LSUN $256 \\times 256$",
        ""
      ],
      "rows": [
        [
          "",
          "CD",
          "CT",
          "CD",
          "CT",
          "CD",
          "CT"
        ],
        [
          "Learning rate",
          "4e-4",
          "4e-4",
          "8e-6",
          "8e-6",
          "1e-5",
          "1e-5"
        ],
        [
          "Batch size",
          "512",
          "512",
          "2048",
          "2048",
          "2048",
          "2048"
        ],
        [
          "$\\mu$",
          "0",
          "",
          "0.95",
          "",
          "0.95",
          ""
        ],
        [
          "$\\mu_0$",
          "",
          "0.9",
          "",
          "0.95",
          "",
          "0.95"
        ],
        [
          "$s_0$",
          "",
          "2",
          "",
          "2",
          "",
          "2"
        ],
        [
          "$s_1$",
          "",
          "150",
          "",
          "200",
          "",
          "150"
        ],
        [
          "N",
          "18",
          "",
          "40",
          "",
          "40",
          ""
        ],
        [
          "ODE solver",
          "Heun",
          "",
          "Heun",
          "",
          "Heun",
          ""
        ],
        [
          "EMA decay rate",
          "0.9999",
          "0.9999",
          "0.999943",
          "0.999943",
          "0.999943",
          "0.999943"
        ],
        [
          "Training iterations",
          "800k",
          "800k",
          "600k",
          "800k",
          "600k",
          "1000k"
        ],
        [
          "Mixed-Precision (FP16)",
          "No",
          "No",
          "Yes",
          "Yes",
          "Yes",
          "Yes"
        ],
        [
          "Dropout probability",
          "0.0",
          "0.0",
          "0.0",
          "0.0",
          "0.0",
          "0.0"
        ],
        [
          "Number of GPUs",
          "8",
          "8",
          "64",
          "64",
          "64",
          "64"
        ]
      ],
      "markdown": "| Hyperparameter         | CIFAR-10 |        | ImageNet $64 \\times 64$ |          | LSUN $256 \\times 256$ |          |\n|------------------------|----------|--------|-------------------------|----------|-----------------------|----------|\n|                        | CD       | CT     | CD                      | CT       | CD                    | CT       |\n| Learning rate          | 4e-4     | 4e-4   | 8e-6                    | 8e-6     | 1e-5                  | 1e-5     |\n| Batch size             | 512      | 512    | 2048                    | 2048     | 2048                  | 2048     |\n| $\\mu$                  | 0        |        | 0.95                    |          | 0.95                  |          |\n| $\\mu_0$                |          | 0.9    |                         | 0.95     |                       | 0.95     |\n| $s_0$                  |          | 2      |                         | 2        |                       | 2        |\n| $s_1$                  |          | 150    |                         | 200      |                       | 150      |\n| N                      | 18       |        | 40                      |          | 40                    |          |\n| ODE solver             | Heun     |        | Heun                    |          | Heun                  |          |\n| EMA decay rate         | 0.9999   | 0.9999 | 0.999943                | 0.999943 | 0.999943              | 0.999943 |\n| Training iterations    | 800k     | 800k   | 600k                    | 800k     | 600k                  | 1000k    |\n| Mixed-Precision (FP16) | No       | No     | Yes                     | Yes      | Yes                   | Yes      |\n| Dropout probability    | 0.0      | 0.0    | 0.0                     | 0.0      | 0.0                   | 0.0      |\n| Number of GPUs         | 8        | 8      | 64                      | 64       | 64                    | 64       |\n"
    }
  ],
  "figures": [
    {
      "figure_id": "figure_1",
      "image_key": "_page_0_Figure_9.jpeg",
      "caption": "Given a Probability Flow (PF) ODE that smoothly converts data to noise, we learn to map any point (*e.g*., xt, xt <sup>1</sup> , and x<sup>T</sup> ) on the ODE trajectory to its origin (*e.g*., x0) for generative modeling. Models of these mappings are called consistency models, as their outputs are trained to be consistent for points on the same trajectory.",
      "alt_text": "",
      "context_before": "<span id=\"page-0-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_2",
      "image_key": "_page_2_Picture_1.jpeg",
      "caption": "Consistency models are trained to map points on any trajectory of the PF ODE to the trajectory's origin.",
      "alt_text": "",
      "context_before": "<span id=\"page-2-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_3",
      "image_key": "_page_6_Figure_1.jpeg",
      "caption": "Various factors that affect consistency distillation (CD) and consistency training (CT) on CIFAR-10. The best configuration for CD is LPIPS, Heun ODE solver, and N \" 18. Our adaptive schedule functions for N and µ make CT converge significantly faster than fixing them to be constants during the course of optimization.",
      "alt_text": "",
      "context_before": "<span id=\"page-6-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_4",
      "image_key": "_page_6_Figure_3.jpeg",
      "caption": "Multistep image generation with consistency distillation (CD). CD outperforms progressive distillation (PD) across all datasets and sampling steps. The only exception is single-step generation on Bedroom 256 ˆ 256.",
      "alt_text": "",
      "context_before": "<span id=\"page-6-1\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_5",
      "image_key": "_page_7_Figure_4.jpeg",
      "caption": "Samples generated by EDM (top), CT + single-step generation (middle), and CT + 2-step generation (Bottom). All corresponding images are generated from the same initial noise.",
      "alt_text": "",
      "context_before": "<span id=\"page-7-1\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_6",
      "image_key": "_page_8_Figure_1.jpeg",
      "caption": "(a) *Left*: The gray-scale image. *Middle*: Colorized images. *Right*: The ground-truth image.",
      "alt_text": "",
      "context_before": "<span id=\"page-8-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_7",
      "image_key": "_page_8_Figure_3.jpeg",
      "caption": "(b) *Left*: The downsampled image (32 ˆ 32). *Middle*: Full resolution images (256 ˆ 256). *Right*: The ground-truth image (256 ˆ 256).",
      "alt_text": "",
      "context_before": "",
      "context_after": ""
    },
    {
      "figure_id": "figure_8",
      "image_key": "_page_8_Figure_5.jpeg",
      "caption": "(c) *Left*: A stroke input provided by users. *Right*: Stroke-guided image generation.",
      "alt_text": "",
      "context_before": "",
      "context_after": ""
    },
    {
      "figure_id": "figure_9",
      "image_key": "_page_23_Figure_1.jpeg",
      "caption": "Comparing discrete consistency distillation/training algorithms with continuous counterparts.",
      "alt_text": "",
      "context_before": "<span id=\"page-23-1\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_10",
      "image_key": "_page_28_Picture_1.jpeg",
      "caption": "Gray-scale images (left), colorized images by a consistency model (middle), and ground truth (right).",
      "alt_text": "",
      "context_before": "<span id=\"page-28-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_11",
      "image_key": "_page_29_Picture_1.jpeg",
      "caption": "Downsampled images of resolution 32 ˆ 32 (left), full resolution (256 ˆ 256) images generated by a consistency model (middle), and ground truth images of resolution 256 ˆ 256 (right).",
      "alt_text": "",
      "context_before": "<span id=\"page-29-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_12",
      "image_key": "_page_30_Picture_1.jpeg",
      "caption": "Masked images (left), imputed images by a consistency model (middle), and ground truth (right).",
      "alt_text": "",
      "context_before": "<span id=\"page-30-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_13",
      "image_key": "_page_31_Picture_1.jpeg",
      "caption": "Interpolating between leftmost and rightmost images with spherical linear interpolation. All samples are generated by a consistency model trained on LSUN Bedroom 256 ˆ 256.",
      "alt_text": "",
      "context_before": "<span id=\"page-31-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_14",
      "image_key": "_page_32_Figure_1.jpeg",
      "caption": "Single-step denoising with a consistency model. The leftmost images are ground truth. For every two rows, the top row shows noisy images with different noise levels, while the bottom row gives denoised images.",
      "alt_text": "",
      "context_before": "<span id=\"page-32-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_15",
      "image_key": "_page_33_Picture_1.jpeg",
      "caption": "SDEdit with a consistency model. The leftmost images are stroke painting inputs. Images on the right side are the results of stroke-guided image generation (SDEdit).",
      "alt_text": "",
      "context_before": "<span id=\"page-33-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_16",
      "image_key": "_page_34_Figure_1.jpeg",
      "caption": "Uncurated samples from CIFAR-10 32 ˆ 32. All corresponding samples use the same initial noise.",
      "alt_text": "",
      "context_before": "<span id=\"page-34-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_17",
      "image_key": "_page_35_Figure_1.jpeg",
      "caption": "Uncurated samples from ImageNet 64 ˆ 64. All corresponding samples use the same initial noise.",
      "alt_text": "",
      "context_before": "<span id=\"page-35-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_18",
      "image_key": "_page_36_Picture_1.jpeg",
      "caption": "(a) EDM (FID=3.57)",
      "alt_text": "",
      "context_before": "<span id=\"page-36-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_19",
      "image_key": "_page_36_Picture_3.jpeg",
      "caption": "(b) CD with single-step generation (FID=7.80)",
      "alt_text": "",
      "context_before": "",
      "context_after": ""
    },
    {
      "figure_id": "figure_20",
      "image_key": "_page_36_Picture_5.jpeg",
      "caption": "(c) CD with two-step generation (FID=5.22)",
      "alt_text": "",
      "context_before": "",
      "context_after": ""
    },
    {
      "figure_id": "figure_21",
      "image_key": "_page_37_Picture_1.jpeg",
      "caption": "(a) EDM (FID=6.69)",
      "alt_text": "",
      "context_before": "<span id=\"page-37-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_22",
      "image_key": "_page_37_Picture_3.jpeg",
      "caption": "(b) CD with single-step generation (FID=10.99)",
      "alt_text": "",
      "context_before": "",
      "context_after": ""
    },
    {
      "figure_id": "figure_23",
      "image_key": "_page_37_Picture_5.jpeg",
      "caption": "(c) CD with two-step generation (FID=8.84)",
      "alt_text": "",
      "context_before": "",
      "context_after": ""
    },
    {
      "figure_id": "figure_24",
      "image_key": "_page_38_Figure_1.jpeg",
      "caption": "Uncurated samples from CIFAR-10 32 ˆ 32. All corresponding samples use the same initial noise.",
      "alt_text": "",
      "context_before": "<span id=\"page-38-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_25",
      "image_key": "_page_39_Figure_1.jpeg",
      "caption": "Uncurated samples from ImageNet 64 ˆ 64. All corresponding samples use the same initial noise.",
      "alt_text": "",
      "context_before": "<span id=\"page-39-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_26",
      "image_key": "_page_40_Picture_1.jpeg",
      "caption": "(a) EDM (FID=3.57)",
      "alt_text": "",
      "context_before": "<span id=\"page-40-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_27",
      "image_key": "_page_40_Picture_3.jpeg",
      "caption": "(b) CT with single-step generation (FID=16.00)",
      "alt_text": "",
      "context_before": "",
      "context_after": ""
    },
    {
      "figure_id": "figure_28",
      "image_key": "_page_40_Picture_5.jpeg",
      "caption": "(c) CT with two-step generation (FID=7.80)",
      "alt_text": "",
      "context_before": "",
      "context_after": ""
    },
    {
      "figure_id": "figure_29",
      "image_key": "_page_41_Picture_1.jpeg",
      "caption": "(a) EDM (FID=6.69)",
      "alt_text": "",
      "context_before": "<span id=\"page-41-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_30",
      "image_key": "_page_41_Picture_3.jpeg",
      "caption": "(b) CT with single-step generation (FID=20.70)",
      "alt_text": "",
      "context_before": "",
      "context_after": ""
    },
    {
      "figure_id": "figure_31",
      "image_key": "_page_41_Picture_5.jpeg",
      "caption": "(c) CT with two-step generation (FID=11.76)",
      "alt_text": "",
      "context_before": "",
      "context_after": ""
    }
  ],
  "stats": {
    "total_chunks": 260,
    "text_chunks": 225,
    "tables_extracted": 4,
    "images_extracted": 31,
    "figure_chunks_created": 31,
    "figures_with_vision": 25,
    "markdown_chars": 153842,
    "processing_time_seconds": 515.56
  }
}