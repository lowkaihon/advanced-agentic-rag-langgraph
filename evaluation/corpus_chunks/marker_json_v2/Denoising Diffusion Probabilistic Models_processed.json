{
  "source": "Denoising Diffusion Probabilistic Models.pdf",
  "processor": "marker",
  "processed_date": "2025-12-14T07:34:10.146011",
  "markdown": "# Denoising Diffusion Probabilistic Models\n\nJonathan Ho UC Berkeley jonathanho@berkeley.edu\n\nAjay Jain UC Berkeley ajayj@berkeley.edu\n\nPieter Abbeel UC Berkeley pabbeel@cs.berkeley.edu\n\n# Abstract\n\nWe present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at <https://github.com/hojonathanho/diffusion>.\n\n# 1 Introduction\n\nDeep generative models of all kinds have recently exhibited high quality samples in a wide variety of data modalities. Generative adversarial networks (GANs), autoregressive models, flows, and variational autoencoders (VAEs) have synthesized striking image and audio samples [\\[14,](#page-9-0) [27,](#page-9-1) [3,](#page-8-0) [58,](#page-11-0) [38,](#page-10-0) [25,](#page-9-2) [10,](#page-9-3) [32,](#page-10-1) [44,](#page-10-2) [57,](#page-11-1) [26,](#page-9-4) [33,](#page-10-3) [45\\]](#page-10-4), and there have been remarkable advances in energy-based modeling and score matching that have produced images comparable to those of GANs [\\[11,](#page-9-5) [55\\]](#page-11-2).\n\n<span id=\"page-0-0\"></span>![](_page_0_Picture_9.jpeg)\n\nFigure 1: Generated samples on CelebA-HQ 256 × 256 (left) and unconditional CIFAR10 (right)\n\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\n\n![](_page_1_Figure_0.jpeg)\n\nFigure 2: The directed graphical model considered in this work.\n\nThis paper presents progress in diffusion probabilistic models [53]. A diffusion probabilistic model (which we will call a \"diffusion model\" for brevity) is a parameterized Markov chain trained using variational inference to produce samples matching the data after finite time. Transitions of this chain are learned to reverse a diffusion process, which is a Markov chain that gradually adds noise to the data in the opposite direction of sampling until signal is destroyed. When the diffusion consists of small amounts of Gaussian noise, it is sufficient to set the sampling chain transitions to conditional Gaussians too, allowing for a particularly simple neural network parameterization.\n\nDiffusion models are straightforward to define and efficient to train, but to the best of our knowledge, there has been no demonstration that they are capable of generating high quality samples. We show that diffusion models actually are capable of generating high quality samples, sometimes better than the published results on other types of generative models (Section 4). In addition, we show that a certain parameterization of diffusion models reveals an equivalence with denoising score matching over multiple noise levels during training and with annealed Langevin dynamics during sampling (Section 3.2) [55, 61]. We obtained our best sample quality results using this parameterization (Section 4.2), so we consider this equivalence to be one of our primary contributions.\n\nDespite their sample quality, our models do not have competitive log likelihoods compared to other likelihood-based models (our models do, however, have log likelihoods better than the large estimates annealed importance sampling has been reported to produce for energy based models and score matching [11, 55]). We find that the majority of our models' lossless codelengths are consumed to describe imperceptible image details (Section 4.3). We present a more refined analysis of this phenomenon in the language of lossy compression, and we show that the sampling procedure of diffusion models is a type of progressive decoding that resembles autoregressive decoding along a bit ordering that vastly generalizes what is normally possible with autoregressive models.\n\n### 2 Background\n\nDiffusion models [53] are latent variable models of the form  $p_{\\theta}(\\mathbf{x}_0) \\coloneqq \\int p_{\\theta}(\\mathbf{x}_{0:T}) d\\mathbf{x}_{1:T}$ , where  $\\mathbf{x}_1, \\dots, \\mathbf{x}_T$  are latents of the same dimensionality as the data  $\\mathbf{x}_0 \\sim q(\\mathbf{x}_0)$ . The joint distribution  $p_{\\theta}(\\mathbf{x}_{0:T})$  is called the *reverse process*, and it is defined as a Markov chain with learned Gaussian transitions starting at  $p(\\mathbf{x}_T) = \\mathcal{N}(\\mathbf{x}_T; \\mathbf{0}, \\mathbf{I})$ :\n\n$$p_{\\theta}(\\mathbf{x}_{0:T}) := p(\\mathbf{x}_T) \\prod_{t=1}^{T} p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t), \\qquad p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t) := \\mathcal{N}(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_{\\theta}(\\mathbf{x}_t, t), \\boldsymbol{\\Sigma}_{\\theta}(\\mathbf{x}_t, t))$$\n(1)\n\nWhat distinguishes diffusion models from other types of latent variable models is that the approximate posterior  $q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)$ , called the *forward process* or *diffusion process*, is fixed to a Markov chain that gradually adds Gaussian noise to the data according to a variance schedule  $\\beta_1, \\ldots, \\beta_T$ :\n\n$$q(\\mathbf{x}_{1:T}|\\mathbf{x}_0) := \\prod_{t=1}^{T} q(\\mathbf{x}_t|\\mathbf{x}_{t-1}), \\qquad q(\\mathbf{x}_t|\\mathbf{x}_{t-1}) := \\mathcal{N}(\\mathbf{x}_t; \\sqrt{1-\\beta_t}\\mathbf{x}_{t-1}, \\beta_t \\mathbf{I})$$\n(2)\n\nTraining is performed by optimizing the usual variational bound on negative log likelihood:\n\n$$\\mathbb{E}\\left[-\\log p_{\\theta}(\\mathbf{x}_{0})\\right] \\leq \\mathbb{E}_{q}\\left[-\\log \\frac{p_{\\theta}(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T}|\\mathbf{x}_{0})}\\right] = \\mathbb{E}_{q}\\left[-\\log p(\\mathbf{x}_{T}) - \\sum_{t>1} \\log \\frac{p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_{t})}{q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})}\\right] =: L \\quad (3)$$\n\nThe forward process variances  $\\beta_t$  can be learned by reparameterization [33] or held constant as hyperparameters, and expressiveness of the reverse process is ensured in part by the choice of Gaussian conditionals in  $p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)$ , because both processes have the same functional form when  $\\beta_t$  are small [53]. A notable property of the forward process is that it admits sampling  $\\mathbf{x}_t$  at an arbitrary timestep t in closed form: using the notation  $\\alpha_t := 1 - \\beta_t$  and  $\\bar{\\alpha}_t := \\prod_{s=1}^t \\alpha_s$ , we have\n\n<span id=\"page-1-2\"></span><span id=\"page-1-1\"></span><span id=\"page-1-0\"></span>\n$$q(\\mathbf{x}_t|\\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0, (1 - \\bar{\\alpha}_t)\\mathbf{I})$$\n(4)\n\nEfficient training is therefore possible by optimizing random terms of L with stochastic gradient descent. Further improvements come from variance reduction by rewriting L (3) as:\n\n$$\\mathbb{E}_{q}\\left[\\underbrace{D_{\\mathrm{KL}}(q(\\mathbf{x}_{T}|\\mathbf{x}_{0}) \\parallel p(\\mathbf{x}_{T}))}_{L_{t}} + \\sum_{t>1} \\underbrace{D_{\\mathrm{KL}}(q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0}) \\parallel p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_{t}))}_{L_{t}} \\underbrace{-\\log p_{\\theta}(\\mathbf{x}_{0}|\\mathbf{x}_{1})}_{L_{t}}\\right]$$\n(5)\n\n(See Appendix A for details. The labels on the terms are used in Section 3.) Equation (5) uses KL divergence to directly compare  $p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)$  against forward process posteriors, which are tractable when conditioned on  $\\mathbf{x}_0$ :\n\n<span id=\"page-2-4\"></span><span id=\"page-2-2\"></span>\n$$q(\\mathbf{x}_{t-1}|\\mathbf{x}_t, \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0), \\tilde{\\beta}_t \\mathbf{I}), \\tag{6}$$\n\nwhere \n$$\\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0) \\coloneqq \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1 - \\bar{\\alpha}_t}\\mathbf{x}_0 + \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t}\\mathbf{x}_t$$\n and  $\\tilde{\\beta}_t \\coloneqq \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t}\\beta_t$  (7)\n\nConsequently, all KL divergences in Eq. (5) are comparisons between Gaussians, so they can be calculated in a Rao-Blackwellized fashion with closed form expressions instead of high variance Monte Carlo estimates.\n\n### <span id=\"page-2-1\"></span>3 Diffusion models and denoising autoencoders\n\nDiffusion models might appear to be a restricted class of latent variable models, but they allow a large number of degrees of freedom in implementation. One must choose the variances  $\\beta_t$  of the forward process and the model architecture and Gaussian distribution parameterization of the reverse process. To guide our choices, we establish a new explicit connection between diffusion models and denoising score matching (Section 3.2) that leads to a simplified, weighted variational bound objective for diffusion models (Section 3.4). Ultimately, our model design is justified by simplicity and empirical results (Section 4). Our discussion is categorized by the terms of Eq. (5).\n\n### 3.1 Forward process and $L_T$\n\nWe ignore the fact that the forward process variances  $\\beta_t$  are learnable by reparameterization and instead fix them to constants (see Section 4 for details). Thus, in our implementation, the approximate posterior q has no learnable parameters, so  $L_T$  is a constant during training and can be ignored.\n\n### <span id=\"page-2-0\"></span>**3.2** Reverse process and $L_{1:T-1}$\n\nNow we discuss our choices in  $p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_{\\theta}(\\mathbf{x}_t, t), \\boldsymbol{\\Sigma}_{\\theta}(\\mathbf{x}_t, t))$  for  $1 < t \\leq T$ . First, we set  $\\boldsymbol{\\Sigma}_{\\theta}(\\mathbf{x}_t, t) = \\sigma_t^2 \\mathbf{I}$  to untrained time dependent constants. Experimentally, both  $\\sigma_t^2 = \\beta_t$  and  $\\sigma_t^2 = \\tilde{\\beta}_t = \\frac{1 - \\tilde{\\alpha}_{t-1}}{1 - \\tilde{\\alpha}_t} \\beta_t$  had similar results. The first choice is optimal for  $\\mathbf{x}_0 \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ , and the second is optimal for  $\\mathbf{x}_0$  deterministically set to one point. These are the two extreme choices corresponding to upper and lower bounds on reverse process entropy for data with coordinatewise unit variance [53].\n\nSecond, to represent the mean  $\\mu_{\\theta}(\\mathbf{x}_t,t)$ , we propose a specific parameterization motivated by the following analysis of  $L_t$ . With  $p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_{\\theta}(\\mathbf{x}_t,t), \\sigma_t^2 \\mathbf{I})$ , we can write:\n\n<span id=\"page-2-5\"></span><span id=\"page-2-3\"></span>\n$$L_{t-1} = \\mathbb{E}_q \\left[ \\frac{1}{2\\sigma_t^2} \\| \\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0) - \\boldsymbol{\\mu}_{\\theta}(\\mathbf{x}_t, t) \\|^2 \\right] + C$$\n (8)\n\nwhere C is a constant that does not depend on  $\\theta$ . So, we see that the most straightforward parameterization of  $\\mu_{\\theta}$  is a model that predicts  $\\tilde{\\mu}_{t}$ , the forward process posterior mean. However, we can expand Eq. (8) further by reparameterizing Eq. (4) as  $\\mathbf{x}_{t}(\\mathbf{x}_{0}, \\boldsymbol{\\epsilon}) = \\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{0} + \\sqrt{1 - \\bar{\\alpha}_{t}}\\boldsymbol{\\epsilon}$  for  $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$  and applying the forward process posterior formula (7):\n\n$$L_{t-1} - C = \\mathbb{E}_{\\mathbf{x}_0, \\epsilon} \\left[ \\frac{1}{2\\sigma_t^2} \\left\\| \\tilde{\\boldsymbol{\\mu}}_t \\left( \\mathbf{x}_t(\\mathbf{x}_0, \\epsilon), \\frac{1}{\\sqrt{\\bar{\\alpha}_t}} (\\mathbf{x}_t(\\mathbf{x}_0, \\epsilon) - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon) \\right) - \\boldsymbol{\\mu}_{\\theta}(\\mathbf{x}_t(\\mathbf{x}_0, \\epsilon), t) \\right\\|^2 \\right]$$\n(9)\n\n$$= \\mathbb{E}_{\\mathbf{x}_0, \\epsilon} \\left[ \\frac{1}{2\\sigma_t^2} \\left\\| \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t(\\mathbf{x}_0, \\epsilon) - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon \\right) - \\boldsymbol{\\mu}_{\\theta}(\\mathbf{x}_t(\\mathbf{x}_0, \\epsilon), t) \\right\\|^2 \\right]$$\n(10)\n\n# <span id=\"page-3-1\"></span>Algorithm 1 Training 1: repeat 2: $\\mathbf{x}_0 \\sim q(\\mathbf{x}_0)$ 3: $t \\sim \\text{Uniform}(\\{1, \\dots, T\\})$ 4: $\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ 5: Take gradient descent step on $\\nabla_{\\theta} \\| \\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_{\\theta}(\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}, t) \\|^2$ 6: until converged Algorithm 2 Sampling 1: $\\mathbf{x}_T \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ 2: for $t = T, \\dots, 1$ do 3: $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ if t > 1, else $\\mathbf{z} = \\mathbf{0}$ 4: $\\mathbf{x}_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_{\\theta}(\\mathbf{x}_t, t) \\right) + \\sigma_t \\mathbf{z}$ 5: end for 6: return $\\mathbf{x}_0$\n\nEquation (10) reveals that  $\\mu_{\\theta}$  must predict  $\\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon \\right)$  given  $\\mathbf{x}_t$ . Since  $\\mathbf{x}_t$  is available as input to the model, we may choose the parameterization\n\n$$\\boldsymbol{\\mu}_{\\theta}(\\mathbf{x}_{t}, t) = \\tilde{\\boldsymbol{\\mu}}_{t} \\left( \\mathbf{x}_{t}, \\frac{1}{\\sqrt{\\bar{\\alpha}_{t}}} (\\mathbf{x}_{t} - \\sqrt{1 - \\bar{\\alpha}_{t}} \\boldsymbol{\\epsilon}_{\\theta}(\\mathbf{x}_{t})) \\right) = \\frac{1}{\\sqrt{\\alpha_{t}}} \\left( \\mathbf{x}_{t} - \\frac{\\beta_{t}}{\\sqrt{1 - \\bar{\\alpha}_{t}}} \\boldsymbol{\\epsilon}_{\\theta}(\\mathbf{x}_{t}, t) \\right)$$\n(11)\n\nwhere  $\\epsilon_{\\theta}$  is a function approximator intended to predict  $\\epsilon$  from  $\\mathbf{x}_t$ . To sample  $\\mathbf{x}_{t-1} \\sim p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)$  is to compute  $\\mathbf{x}_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1-\\alpha_t}} \\epsilon_{\\theta}(\\mathbf{x}_t,t) \\right) + \\sigma_t \\mathbf{z}$ , where  $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I})$ . The complete sampling procedure, Algorithm 2, resembles Langevin dynamics with  $\\epsilon_{\\theta}$  as a learned gradient of the data density. Furthermore, with the parameterization (11), Eq. (10) simplifies to:\n\n<span id=\"page-3-3\"></span><span id=\"page-3-2\"></span>\n$$\\mathbb{E}_{\\mathbf{x}_{0},\\epsilon} \\left[ \\frac{\\beta_{t}^{2}}{2\\sigma_{t}^{2}\\alpha_{t}(1-\\bar{\\alpha}_{t})} \\left\\| \\epsilon - \\epsilon_{\\theta}(\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{0} + \\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon, t) \\right\\|^{2} \\right]$$\n(12)\n\nwhich resembles denoising score matching over multiple noise scales indexed by t [55]. As Eq. (12) is equal to (one term of) the variational bound for the Langevin-like reverse process (11), we see that optimizing an objective resembling denoising score matching is equivalent to using variational inference to fit the finite-time marginal of a sampling chain resembling Langevin dynamics.\n\nTo summarize, we can train the reverse process mean function approximator  $\\mu_{\\theta}$  to predict  $\\tilde{\\mu}_{t}$ , or by modifying its parameterization, we can train it to predict  $\\epsilon$ . (There is also the possibility of predicting  $\\mathbf{x}_{0}$ , but we found this to lead to worse sample quality early in our experiments.) We have shown that the  $\\epsilon$ -prediction parameterization both resembles Langevin dynamics and simplifies the diffusion model's variational bound to an objective that resembles denoising score matching. Nonetheless, it is just another parameterization of  $p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_{t})$ , so we verify its effectiveness in Section 4 in an ablation where we compare predicting  $\\epsilon$  against predicting  $\\tilde{\\mu}_{t}$ .\n\n### 3.3 Data scaling, reverse process decoder, and $L_0$\n\nWe assume that image data consists of integers in  $\\{0, 1, \\dots, 255\\}$  scaled linearly to [-1, 1]. This ensures that the neural network reverse process operates on consistently scaled inputs starting from the standard normal prior  $p(\\mathbf{x}_T)$ . To obtain discrete log likelihoods, we set the last term of the reverse process to an independent discrete decoder derived from the Gaussian  $\\mathcal{N}(\\mathbf{x}_0; \\boldsymbol{\\mu}_{\\theta}(\\mathbf{x}_1, 1), \\sigma_1^2 \\mathbf{I})$ :\n\n<span id=\"page-3-4\"></span>\n$$p_{\\theta}(\\mathbf{x}_{0}|\\mathbf{x}_{1}) = \\prod_{i=1}^{D} \\int_{\\delta_{-}(x_{0}^{i})}^{\\delta_{+}(x_{0}^{i})} \\mathcal{N}(x; \\mu_{\\theta}^{i}(\\mathbf{x}_{1}, 1), \\sigma_{1}^{2}) dx$$\n\n$$\\delta_{+}(x) = \\begin{cases} \\infty & \\text{if } x = 1\\\\ x + \\frac{1}{255} & \\text{if } x < 1 \\end{cases} \\quad \\delta_{-}(x) = \\begin{cases} -\\infty & \\text{if } x = -1\\\\ x - \\frac{1}{255} & \\text{if } x > -1 \\end{cases}$$\n(13)\n\nwhere D is the data dimensionality and the i superscript indicates extraction of one coordinate. (It would be straightforward to instead incorporate a more powerful decoder like a conditional autoregressive model, but we leave that to future work.) Similar to the discretized continuous distributions used in VAE decoders and autoregressive models [34, 52], our choice here ensures that the variational bound is a lossless codelength of discrete data, without need of adding noise to the data or incorporating the Jacobian of the scaling operation into the log likelihood. At the end of sampling, we display  $\\mu_{\\theta}(\\mathbf{x}_1, 1)$  noiselessly.\n\n### <span id=\"page-3-0\"></span>3.4 Simplified training objective\n\nWith the reverse process and decoder defined above, the variational bound, consisting of terms derived from Eqs. (12) and (13), is clearly differentiable with respect to  $\\theta$  and is ready to be employed for\n\n<span id=\"page-4-2\"></span>Table 1: CIFAR10 results. NLL measured in bits/dim.\n\n| Model                                                                     | IS                                                               | FID                                  | NLL Test (Train)                         | —<br>————————————————————————————————————                                                                                         | 1 0751 540                   |                           |\n|---------------------------------------------------------------------------|------------------------------------------------------------------|--------------------------------------|------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|------------------------------|---------------------------|\n| Conditional                                                               |                                                                  |                                      |                                          | -Table 2: Unconditio<br>_process parameterizati                                                                                   |                              |                           |\n| EBM [11]<br>JEM [17]<br>BigGAN [3]<br>StyleGAN2 + ADA (v1) [29]           | 8.30<br>8.76<br>9.22<br><b>10.06</b>                             | 37.9<br>38.4<br>14.73<br><b>2.67</b> |                                          | tive ablation. Blank er<br>train and generated por<br>range scores.                                                               | ntries were uns              | stable to                 |\n| Unconditional                                                             |                                                                  |                                      |                                          | Objective                                                                                                                         | IS                           | FID                       |\n| Diffusion (original) [53]                                                 |                                                                  |                                      | < 5.40                                   | $\tilde{\\mu}$ prediction (baseline)                                                                                                |                              |                           |\n| Gated PixelCNN [59]<br>Sparse Transformer [7]<br>PixelIQN [43]            | 4.60<br>5.29                                                     | 65.93<br>49.46                       | 3.03 (2.90)<br><b>2.80</b>               | $L$ , learned diagonal $\\Sigma$ $L$ , fixed isotropic $\\Sigma$ $\\ \\tilde{\\mu} - \\tilde{\\mu}_{\\theta}\\ ^2$                         | $7.28\\pm0.10$ $8.06\\pm0.09$  | 23.69<br>13.22<br>-       |\n| EBM [11]<br>NCSNv2 [56]                                                   | 6.78                                                             | $38.2 \\\\ 31.75$                      |                                          | $\\epsilon$ prediction (ours)                                                                                                      |                              |                           |\n| NCSN [55]<br>SNGAN [39]<br>SNGAN-DDLS [4]<br>StyleGAN2 + ADA (v1) [29]    | $8.87\\pm0.12$<br>$8.22\\pm0.05$<br>$9.09\\pm0.10$<br>$9.74\\pm0.05$ | 25.32<br>21.7<br>15.42<br>3.26       |                                          | $L$ , learned diagonal $\\Sigma$ $L$ , fixed isotropic $\\Sigma$ $\\ \\tilde{\\epsilon} - \\epsilon_{\\theta}\\ ^2 (L_{\\mathrm{simple}})$ | $-7.67\\pm0.13$ $9.46\\pm0.11$ | -<br>13.51<br><b>3.17</b> |\n| Ours $(L, \\text{ fixed isotropic } \\Sigma)$<br>Ours $(L_{\\text{simple}})$ | $7.67 \\pm 0.13$<br>$9.46 \\pm 0.11$                               | 13.51<br><b>3.17</b>                 | $\\leq 3.70 (3.69)$<br>$\\leq 3.75 (3.72)$ |                                                                                                                                   |                              |                           |\n\ntraining. However, we found it beneficial to sample quality (and simpler to implement) to train on the following variant of the variational bound:\n\n<span id=\"page-4-1\"></span>\n$$L_{\\text{simple}}(\\theta) := \\mathbb{E}_{t,\\mathbf{x}_0,\\epsilon} \\left[ \\left\\| \\epsilon - \\epsilon_{\\theta} (\\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, t) \\right\\|^2 \\right]$$\n (14)\n\nwhere t is uniform between 1 and T. The t=1 case corresponds to  $L_0$  with the integral in the discrete decoder definition (13) approximated by the Gaussian probability density function times the bin width, ignoring  $\\sigma_1^2$  and edge effects. The t>1 cases correspond to an unweighted version of Eq. (12), analogous to the loss weighting used by the NCSN denoising score matching model [55]. ( $L_T$  does not appear because the forward process variances  $\\beta_t$  are fixed.) Algorithm 1 displays the complete training procedure with this simplified objective.\n\nSince our simplified objective (14) discards the weighting in Eq. (12), it is a weighted variational bound that emphasizes different aspects of reconstruction compared to the standard variational bound [18, 22]. In particular, our diffusion process setup in Section 4 causes the simplified objective to down-weight loss terms corresponding to small t. These terms train the network to denoise data with very small amounts of noise, so it is beneficial to down-weight them so that the network can focus on more difficult denoising tasks at larger t terms. We will see in our experiments that this reweighting leads to better sample quality.\n\n### <span id=\"page-4-0\"></span>4 Experiments\n\nWe set T=1000 for all experiments so that the number of neural network evaluations needed during sampling matches previous work [53, 55]. We set the forward process variances to constants increasing linearly from  $\\beta_1=10^{-4}$  to  $\\beta_T=0.02$ . These constants were chosen to be small relative to data scaled to [-1,1], ensuring that reverse and forward processes have approximately the same functional form while keeping the signal-to-noise ratio at  $\\mathbf{x}_T$  as small as possible ( $L_T=D_{\\mathrm{KL}}(q(\\mathbf{x}_T|\\mathbf{x}_0) \\parallel \\mathcal{N}(\\mathbf{0},\\mathbf{I})) \\approx 10^{-5}$  bits per dimension in our experiments).\n\nTo represent the reverse process, we use a U-Net backbone similar to an unmasked PixelCNN++ [52, 48] with group normalization throughout [66]. Parameters are shared across time, which is specified to the network using the Transformer sinusoidal position embedding [60]. We use self-attention at the  $16 \\times 16$  feature map resolution [63, 60]. Details are in Appendix B.\n\n### 4.1 Sample quality\n\nTable 1 shows Inception scores, FID scores, and negative log likelihoods (lossless codelengths) on CIFAR10. With our FID score of 3.17, our unconditional model achieves better sample quality than most models in the literature, including class conditional models. Our FID score is computed with respect to the training set, as is standard practice; when we compute it with respect to the test set, the score is 5.24, which is still better than many of the training set FID scores in the literature.\n\n<span id=\"page-5-2\"></span>![](_page_5_Picture_0.jpeg)\n\n![](_page_5_Picture_1.jpeg)\n\nFigure 3: LSUN Church samples. FID=7.89\n\nFigure 4: LSUN Bedroom samples. FID=4.90\n\n<span id=\"page-5-3\"></span>\n\n| Algorithm 3 Sending $x_0$                                                                                                    | Algorithm 4 Receiving                                                         |\n|------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------|\n| 1: Send $\\mathbf{x}_T \\sim q(\\mathbf{x}_T \\mathbf{x}_0)$ using $p(\\mathbf{x}_T)$                                             | 1: Receive $\\mathbf{x}_T$ using $p(\\mathbf{x}_T)$                             |\n| 2: <b>for</b> $t = T - 1, \\dots, 2, 1$ <b>do</b>                                                                             | 2: for $t = T - 1, \\dots, 1, 0$ do                                            |\n| 3: Send $\\mathbf{x}_t \\sim q(\\mathbf{x}_t \\mathbf{x}_{t+1}, \\mathbf{x}_0)$ using $p_{\\theta}(\\mathbf{x}_t \\mathbf{x}_{t+1})$ | 3: Receive $\\mathbf{x}_t$ using $p_{\\theta}(\\mathbf{x}_t   \\mathbf{x}_{t+1})$ |\n| 4: <b>end for</b>                                                                                                            | 4: end for                                                                    |\n| 5: Send $\\mathbf{x}_0$ using $p_{\\theta}(\\mathbf{x}_0 \\mathbf{x}_1)$                                                         | 5: return $\\mathbf{x}_0$                                                      |\n\nWe find that training our models on the true variational bound yields better codelengths than training on the simplified objective, as expected, but the latter yields the best sample quality. See Fig. 1 for CIFAR10 and CelebA-HQ  $256\\times256$  samples, Fig. 3 and Fig. 4 for LSUN  $256\\times256$  samples [71], and Appendix D for more.\n\n### <span id=\"page-5-0\"></span>4.2 Reverse process parameterization and training objective ablation\n\nIn Table 2, we show the sample quality effects of reverse process parameterizations and training objectives (Section 3.2). We find that the baseline option of predicting  $\\tilde{\\mu}$  works well only when trained on the true variational bound instead of unweighted mean squared error, a simplified objective akin to Eq. (14). We also see that learning reverse process variances (by incorporating a parameterized diagonal  $\\Sigma_{\\theta}(\\mathbf{x}_t)$  into the variational bound) leads to unstable training and poorer sample quality compared to fixed variances. Predicting  $\\epsilon$ , as we proposed, performs approximately as well as predicting  $\\tilde{\\mu}$  when trained on the variational bound with fixed variances, but much better when trained with our simplified objective.\n\n### <span id=\"page-5-1\"></span>4.3 Progressive coding\n\nTable 1 also shows the codelengths of our CIFAR10 models. The gap between train and test is at most 0.03 bits per dimension, which is comparable to the gaps reported with other likelihood-based models and indicates that our diffusion model is not overfitting (see Appendix D for nearest neighbor visualizations). Still, while our lossless codelengths are better than the large estimates reported for energy based models and score matching using annealed importance sampling [11], they are not competitive with other types of likelihood-based generative models [7].\n\nSince our samples are nonetheless of high quality, we conclude that diffusion models have an inductive bias that makes them excellent lossy compressors. Treating the variational bound terms  $L_1 + \\cdots + L_T$  as rate and  $L_0$  as distortion, our CIFAR10 model with the highest quality samples has a rate of **1.78** bits/dim and a distortion of **1.97** bits/dim, which amounts to a root mean squared error of 0.95 on a scale from 0 to 255. More than half of the lossless codelength describes imperceptible distortions.\n\n**Progressive lossy compression** We can probe further into the rate-distortion behavior of our model by introducing a progressive lossy code that mirrors the form of Eq. (5): see Algorithms 3 and 4, which assume access to a procedure, such as minimal random coding [19, 20], that can transmit a sample  $\\mathbf{x} \\sim q(\\mathbf{x})$  using approximately  $D_{\\mathrm{KL}}(q(\\mathbf{x}) \\parallel p(\\mathbf{x}))$  bits on average for any distributions p and q, for which only p is available to the receiver beforehand. When applied to  $\\mathbf{x}_0 \\sim q(\\mathbf{x}_0)$ , Algorithms 3 and 4 transmit  $\\mathbf{x}_T, \\ldots, \\mathbf{x}_0$  in sequence using a total expected codelength equal to Eq. (5). The receiver,\n\nat any time t, has the partial information  $x_t$  fully available and can progressively estimate:\n\n$$\\mathbf{x}_0 \\approx \\hat{\\mathbf{x}}_0 = \\left(\\mathbf{x}_t - \\sqrt{1 - \\bar{\\alpha}_t} \\boldsymbol{\\epsilon}_{\\theta}(\\mathbf{x}_t)\\right) / \\sqrt{\\bar{\\alpha}_t}$$\n (15)\n\ndue to Eq. (4). (A stochastic reconstruction  $\\mathbf{x}_0 \\sim p_\\theta(\\mathbf{x}_0|\\mathbf{x}_t)$  is also valid, but we do not consider it here because it makes distortion more difficult to evaluate.) Figure 5 shows the resulting rate-distortion plot on the CIFAR10 test set. At each time t, the distortion is calculated as the root mean squared error  $\\sqrt{\\|\\mathbf{x}_0 - \\hat{\\mathbf{x}}_0\\|^2/D}$ , and the rate is calculated as the cumulative number of bits received so far at time t. The distortion decreases steeply in the low-rate region of the rate-distortion plot, indicating that the majority of the bits are indeed allocated to imperceptible distortions.\n\n<span id=\"page-6-0\"></span>![](_page_6_Figure_3.jpeg)\n\nFigure 5: Unconditional CIFAR10 test set rate-distortion vs. time. Distortion is measured in root mean squared error on a [0, 255] scale. See Table 4 for details.\n\n**Progressive generation** We also run a progressive unconditional generation process given by progressive decompression from random bits. In other words, we predict the result of the reverse process,  $\\hat{\\mathbf{x}}_0$ , while sampling from the reverse process using Algorithm 2. Figures 6 and 10 show the resulting sample quality of  $\\hat{\\mathbf{x}}_0$  over the course of the reverse process. Large scale image features appear first and details appear last. Figure 7 shows stochastic predictions  $\\mathbf{x}_0 \\sim p_{\\theta}(\\mathbf{x}_0|\\mathbf{x}_t)$  with  $\\mathbf{x}_t$  frozen for various t. When t is small, all but fine details are preserved, and when t is large, only large scale features are preserved. Perhaps these are hints of conceptual compression [18].\n\n<span id=\"page-6-1\"></span>![](_page_6_Figure_6.jpeg)\n\n<span id=\"page-6-2\"></span>Figure 6: Unconditional CIFAR10 progressive generation ( $\\hat{\\mathbf{x}}_0$  over time, from left to right). Extended samples and sample quality metrics over time in the appendix (Figs. 10 and 14).\n\n![](_page_6_Figure_8.jpeg)\n\nFigure 7: When conditioned on the same latent, CelebA-HQ  $256 \\times 256$  samples share high-level attributes. Bottom-right quadrants are  $\\mathbf{x}_t$ , and other quadrants are samples from  $p_{\\theta}(\\mathbf{x}_0|\\mathbf{x}_t)$ .\n\n**Connection to autoregressive decoding** Note that the variational bound (5) can be rewritten as:\n\n$$L = D_{\\mathrm{KL}}(q(\\mathbf{x}_T) \\parallel p(\\mathbf{x}_T)) + \\mathbb{E}_q \\left[ \\sum_{t>1} D_{\\mathrm{KL}}(q(\\mathbf{x}_{t-1}|\\mathbf{x}_t) \\parallel p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)) \\right] + H(\\mathbf{x}_0)$$\n (16)\n\n(See Appendix A for a derivation.) Now consider setting the diffusion process length T to the dimensionality of the data, defining the forward process so that  $q(\\mathbf{x}_t|\\mathbf{x}_0)$  places all probability mass on  $\\mathbf{x}_0$  with the first t coordinates masked out (i.e.  $q(\\mathbf{x}_t|\\mathbf{x}_{t-1})$  masks out the  $t^{th}$  coordinate), setting  $p(\\mathbf{x}_T)$  to place all mass on a blank image, and, for the sake of argument, taking  $p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)$  to\n\n<span id=\"page-7-0\"></span>![](_page_7_Figure_0.jpeg)\n\nFigure 8: Interpolations of CelebA-HQ 256x256 images with 500 timesteps of diffusion.\n\nbe a fully expressive conditional distribution. With these choices,  $D_{\\mathrm{KL}}(q(\\mathbf{x}_T) \\parallel p(\\mathbf{x}_T)) = 0$ , and minimizing  $D_{\\mathrm{KL}}(q(\\mathbf{x}_{t-1}|\\mathbf{x}_t) \\parallel p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t))$  trains  $p_{\\theta}$  to copy coordinates  $t+1,\\ldots,T$  unchanged and to predict the  $t^{\\mathrm{th}}$  coordinate given  $t+1,\\ldots,T$ . Thus, training  $p_{\\theta}$  with this particular diffusion is training an autoregressive model.\n\nWe can therefore interpret the Gaussian diffusion model (2) as a kind of autoregressive model with a generalized bit ordering that cannot be expressed by reordering data coordinates. Prior work has shown that such reorderings introduce inductive biases that have an impact on sample quality [38], so we speculate that the Gaussian diffusion serves a similar purpose, perhaps to greater effect since Gaussian noise might be more natural to add to images compared to masking noise. Moreover, the Gaussian diffusion length is not restricted to equal the data dimension; for instance, we use T=1000, which is less than the dimension of the  $32\\times32\\times3$  or  $256\\times256\\times3$  images in our experiments. Gaussian diffusions can be made shorter for fast sampling or longer for model expressiveness.\n\n### 4.4 Interpolation\n\nWe can interpolate source images  $\\mathbf{x}_0, \\mathbf{x}_0' \\sim q(\\mathbf{x}_0)$  in latent space using q as a stochastic encoder,  $\\mathbf{x}_t, \\mathbf{x}_t' \\sim q(\\mathbf{x}_t|\\mathbf{x}_0)$ , then decoding the linearly interpolated latent  $\\bar{\\mathbf{x}}_t = (1-\\lambda)\\mathbf{x}_0 + \\lambda\\mathbf{x}_0'$  into image space by the reverse process,  $\\bar{\\mathbf{x}}_0 \\sim p(\\mathbf{x}_0|\\bar{\\mathbf{x}}_t)$ . In effect, we use the reverse process to remove artifacts from linearly interpolating corrupted versions of the source images, as depicted in Fig. 8 (left). We fixed the noise for different values of  $\\lambda$  so  $\\mathbf{x}_t$  and  $\\mathbf{x}_t'$  remain the same. Fig. 8 (right) shows interpolations and reconstructions of original CelebA-HQ  $256 \\times 256$  images (t = 500). The reverse process produces high-quality reconstructions, and plausible interpolations that smoothly vary attributes such as pose, skin tone, hairstyle, expression and background, but not eyewear. Larger t results in coarser and more varied interpolations, with novel samples at t = 1000 (Appendix Fig. 9).\n\n### 5 Related Work\n\nWhile diffusion models might resemble flows [9, 46, 10, 32, 5, 16, 23] and VAEs [33, 47, 37], diffusion models are designed so that q has no parameters and the top-level latent  $\\mathbf{x}_T$  has nearly zero mutual information with the data  $\\mathbf{x}_0$ . Our  $\\epsilon$ -prediction reverse process parameterization establishes a connection between diffusion models and denoising score matching over multiple noise levels with annealed Langevin dynamics for sampling [55, 56]. Diffusion models, however, admit straightforward log likelihood evaluation, and the training procedure explicitly trains the Langevin dynamics sampler using variational inference (see Appendix C for details). The connection also has the reverse implication that a certain weighted form of denoising score matching is the same as variational inference to train a Langevin-like sampler. Other methods for learning transition operators of Markov chains include infusion training [2], variational walkback [15], generative stochastic networks [1], and others [50, 54, 36, 42, 35, 65].\n\nBy the known connection between score matching and energy-based modeling, our work could have implications for other recent work on energy-based models [67–69, 12, 70, 13, 11, 41, 17, 8]. Our rate-distortion curves are computed over time in one evaluation of the variational bound, reminiscent of how rate-distortion curves can be computed over distortion penalties in one run of annealed importance sampling [24]. Our progressive decoding argument can be seen in convolutional DRAW and related models [18, 40] and may also lead to more general designs for subscale orderings or sampling strategies for autoregressive models [38, 64].\n\n# 6 Conclusion\n\nWe have presented high quality image samples using diffusion models, and we have found connections among diffusion models and variational inference for training Markov chains, denoising score matching and annealed Langevin dynamics (and energy-based models by extension), autoregressive models, and progressive lossy compression. Since diffusion models seem to have excellent inductive biases for image data, we look forward to investigating their utility in other data modalities and as components in other types of generative models and machine learning systems.\n\n# Broader Impact\n\nOur work on diffusion models takes on a similar scope as existing work on other types of deep generative models, such as efforts to improve the sample quality of GANs, flows, autoregressive models, and so forth. Our paper represents progress in making diffusion models a generally useful tool in this family of techniques, so it may serve to amplify any impacts that generative models have had (and will have) on the broader world.\n\nUnfortunately, there are numerous well-known malicious uses of generative models. Sample generation techniques can be employed to produce fake images and videos of high profile figures for political purposes. While fake images were manually created long before software tools were available, generative models such as ours make the process easier. Fortunately, CNN-generated images currently have subtle flaws that allow detection [\\[62\\]](#page-11-18), but improvements in generative models may make this more difficult. Generative models also reflect the biases in the datasets on which they are trained. As many large datasets are collected from the internet by automated systems, it can be difficult to remove these biases, especially when the images are unlabeled. If samples from generative models trained on these datasets proliferate throughout the internet, then these biases will only be reinforced further.\n\nOn the other hand, diffusion models may be useful for data compression, which, as data becomes higher resolution and as global internet traffic increases, might be crucial to ensure accessibility of the internet to wide audiences. Our work might contribute to representation learning on unlabeled raw data for a large range of downstream tasks, from image classification to reinforcement learning, and diffusion models might also become viable for creative uses in art, photography, and music.\n\n# Acknowledgments and Disclosure of Funding\n\nThis work was supported by ONR PECASE and the NSF Graduate Research Fellowship under grant number DGE-1752814. Google's TensorFlow Research Cloud (TFRC) provided Cloud TPUs.\n\n# References\n\n- <span id=\"page-8-5\"></span>[1] Guillaume Alain, Yoshua Bengio, Li Yao, Jason Yosinski, Eric Thibodeau-Laufer, Saizheng Zhang, and Pascal Vincent. GSNs: generative stochastic networks. *Information and Inference: A Journal of the IMA*, 5(2):210–249, 2016.\n- <span id=\"page-8-4\"></span>[2] Florian Bordes, Sina Honari, and Pascal Vincent. Learning to generate samples from noise through infusion training. In *International Conference on Learning Representations*, 2017.\n- <span id=\"page-8-0\"></span>[3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In *International Conference on Learning Representations*, 2019.\n- <span id=\"page-8-2\"></span>[4] Tong Che, Ruixiang Zhang, Jascha Sohl-Dickstein, Hugo Larochelle, Liam Paull, Yuan Cao, and Yoshua Bengio. Your GAN is secretly an energy-based model and you should use discriminator driven latent sampling. *arXiv preprint arXiv:2003.06060*, 2020.\n- <span id=\"page-8-3\"></span>[5] Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. In *Advances in Neural Information Processing Systems*, pages 6571–6583, 2018.\n- <span id=\"page-8-6\"></span>[6] Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. PixelSNAIL: An improved autoregressive generative model. In *International Conference on Machine Learning*, pages 863–871, 2018.\n- <span id=\"page-8-1\"></span>[7] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. *arXiv preprint arXiv:1904.10509*, 2019.\n\n- <span id=\"page-9-17\"></span>[8] Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc'Aurelio Ranzato. Residual energy-based models for text generation. *arXiv preprint arXiv:2004.11714*, 2020.\n- <span id=\"page-9-11\"></span>[9] Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components estimation. *arXiv preprint arXiv:1410.8516*, 2014.\n- <span id=\"page-9-3\"></span>[10] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. *arXiv preprint arXiv:1605.08803*, 2016.\n- <span id=\"page-9-5\"></span>[11] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In *Advances in Neural Information Processing Systems*, pages 3603–3613, 2019.\n- <span id=\"page-9-15\"></span>[12] Ruiqi Gao, Yang Lu, Junpei Zhou, Song-Chun Zhu, and Ying Nian Wu. Learning generative ConvNets via multi-grid modeling and sampling. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 9155–9164, 2018.\n- <span id=\"page-9-16\"></span>[13] Ruiqi Gao, Erik Nijkamp, Diederik P Kingma, Zhen Xu, Andrew M Dai, and Ying Nian Wu. Flow contrastive estimation of energy-based models. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 7518–7528, 2020.\n- <span id=\"page-9-0\"></span>[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In *Advances in Neural Information Processing Systems*, pages 2672–2680, 2014.\n- <span id=\"page-9-14\"></span>[15] Anirudh Goyal, Nan Rosemary Ke, Surya Ganguli, and Yoshua Bengio. Variational walkback: Learning a transition operator as a stochastic recurrent net. In *Advances in Neural Information Processing Systems*, pages 4392–4402, 2017.\n- <span id=\"page-9-12\"></span>[16] Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, and David Duvenaud. FFJORD: Free-form continuous dynamics for scalable reversible generative models. In *International Conference on Learning Representations*, 2019.\n- <span id=\"page-9-6\"></span>[17] Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like one. In *International Conference on Learning Representations*, 2020.\n- <span id=\"page-9-7\"></span>[18] Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards conceptual compression. In *Advances In Neural Information Processing Systems*, pages 3549–3557, 2016.\n- <span id=\"page-9-9\"></span>[19] Prahladh Harsha, Rahul Jain, David McAllester, and Jaikumar Radhakrishnan. The communication complexity of correlation. In *Twenty-Second Annual IEEE Conference on Computational Complexity (CCC'07)*, pages 10–23. IEEE, 2007.\n- <span id=\"page-9-10\"></span>[20] Marton Havasi, Robert Peharz, and José Miguel Hernández-Lobato. Minimal random code learning: Getting bits back from compressed model parameters. In *International Conference on Learning Representations*, 2019.\n- <span id=\"page-9-20\"></span>[21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In *Advances in Neural Information Processing Systems*, pages 6626–6637, 2017.\n- <span id=\"page-9-8\"></span>[22] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. In *International Conference on Learning Representations*, 2017.\n- <span id=\"page-9-13\"></span>[23] Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving flow-based generative models with variational dequantization and architecture design. In *International Conference on Machine Learning*, 2019.\n- <span id=\"page-9-18\"></span>[24] Sicong Huang, Alireza Makhzani, Yanshuai Cao, and Roger Grosse. Evaluating lossy compression rates of deep generative models. In *International Conference on Machine Learning*, 2020.\n- <span id=\"page-9-2\"></span>[25] Nal Kalchbrenner, Aaron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex Graves, and Koray Kavukcuoglu. Video pixel networks. In *International Conference on Machine Learning*, pages 1771–1779, 2017.\n- <span id=\"page-9-4\"></span>[26] Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg, Aaron van den Oord, Sander Dieleman, and Koray Kavukcuoglu. Efficient neural audio synthesis. In *International Conference on Machine Learning*, pages 2410–2419, 2018.\n- <span id=\"page-9-1\"></span>[27] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In *International Conference on Learning Representations*, 2018.\n- <span id=\"page-9-19\"></span>[28] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages\n\n- 4401–4410, 2019.\n- <span id=\"page-10-6\"></span>[29] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. *arXiv preprint arXiv:2006.06676v1*, 2020.\n- <span id=\"page-10-19\"></span>[30] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of StyleGAN. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 8110–8119, 2020.\n- <span id=\"page-10-21\"></span>[31] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In *International Conference on Learning Representations*, 2015.\n- <span id=\"page-10-1\"></span>[32] Diederik P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In *Advances in Neural Information Processing Systems*, pages 10215–10224, 2018.\n- <span id=\"page-10-3\"></span>[33] Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. *arXiv preprint arXiv:1312.6114*, 2013.\n- <span id=\"page-10-5\"></span>[34] Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. In *Advances in Neural Information Processing Systems*, pages 4743–4751, 2016.\n- <span id=\"page-10-16\"></span>[35] John Lawson, George Tucker, Bo Dai, and Rajesh Ranganath. Energy-inspired models: Learning with sampler-induced distributions. In *Advances in Neural Information Processing Systems*, pages 8501–8513, 2019.\n- <span id=\"page-10-14\"></span>[36] Daniel Levy, Matt D. Hoffman, and Jascha Sohl-Dickstein. Generalizing Hamiltonian Monte Carlo with neural networks. In *International Conference on Learning Representations*, 2018.\n- <span id=\"page-10-12\"></span>[37] Lars Maaløe, Marco Fraccaro, Valentin Liévin, and Ole Winther. BIVA: A very deep hierarchy of latent variables for generative modeling. In *Advances in Neural Information Processing Systems*, pages 6548–6558, 2019.\n- <span id=\"page-10-0\"></span>[38] Jacob Menick and Nal Kalchbrenner. Generating high fidelity images with subscale pixel networks and multidimensional upscaling. In *International Conference on Learning Representations*, 2019.\n- <span id=\"page-10-8\"></span>[39] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In *International Conference on Learning Representations*, 2018.\n- <span id=\"page-10-18\"></span>[40] Alex Nichol. VQ-DRAW: A sequential discrete VAE. *arXiv preprint arXiv:2003.01599*, 2020.\n- <span id=\"page-10-17\"></span>[41] Erik Nijkamp, Mitch Hill, Tian Han, Song-Chun Zhu, and Ying Nian Wu. On the anatomy of MCMC-based maximum likelihood learning of energy-based models. *arXiv preprint arXiv:1903.12370*, 2019.\n- <span id=\"page-10-15\"></span>[42] Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying Nian Wu. Learning non-convergent non-persistent short-run MCMC toward energy-based model. In *Advances in Neural Information Processing Systems*, pages 5233–5243, 2019.\n- <span id=\"page-10-7\"></span>[43] Georg Ostrovski, Will Dabney, and Remi Munos. Autoregressive quantile networks for generative modeling. In *International Conference on Machine Learning*, pages 3936–3945, 2018.\n- <span id=\"page-10-2\"></span>[44] Ryan Prenger, Rafael Valle, and Bryan Catanzaro. WaveGlow: A flow-based generative network for speech synthesis. In *ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*, pages 3617–3621. IEEE, 2019.\n- <span id=\"page-10-4\"></span>[45] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with VQ-VAE-2. In *Advances in Neural Information Processing Systems*, pages 14837–14847, 2019.\n- <span id=\"page-10-10\"></span>[46] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In *International Conference on Machine Learning*, pages 1530–1538, 2015.\n- <span id=\"page-10-11\"></span>[47] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In *International Conference on Machine Learning*, pages 1278–1286, 2014.\n- <span id=\"page-10-9\"></span>[48] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical image segmentation. In *International Conference on Medical Image Computing and Computer-Assisted Intervention*, pages 234–241. Springer, 2015.\n- <span id=\"page-10-20\"></span>[49] Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In *Advances in Neural Information Processing Systems*, pages 901–909, 2016.\n- <span id=\"page-10-13\"></span>[50] Tim Salimans, Diederik Kingma, and Max Welling. Markov Chain Monte Carlo and variational inference: Bridging the gap. In *International Conference on Machine Learning*, pages 1218–1226, 2015.\n\n- <span id=\"page-11-20\"></span>[51] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In *Advances in Neural Information Processing Systems*, pages 2234–2242, 2016.\n- <span id=\"page-11-5\"></span>[52] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications. In *International Conference on Learning Representations*, 2017.\n- <span id=\"page-11-3\"></span>[53] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In *International Conference on Machine Learning*, pages 2256–2265, 2015.\n- <span id=\"page-11-12\"></span>[54] Jiaming Song, Shengjia Zhao, and Stefano Ermon. A-NICE-MC: Adversarial training for MCMC. In *Advances in Neural Information Processing Systems*, pages 5140–5150, 2017.\n- <span id=\"page-11-2\"></span>[55] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In *Advances in Neural Information Processing Systems*, pages 11895–11907, 2019.\n- <span id=\"page-11-7\"></span>[56] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. *arXiv preprint arXiv:2006.09011*, 2020.\n- <span id=\"page-11-1\"></span>[57] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. WaveNet: A generative model for raw audio. *arXiv preprint arXiv:1609.03499*, 2016.\n- <span id=\"page-11-0\"></span>[58] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. *International Conference on Machine Learning*, 2016.\n- <span id=\"page-11-6\"></span>[59] Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. Conditional image generation with PixelCNN decoders. In *Advances in Neural Information Processing Systems*, pages 4790–4798, 2016.\n- <span id=\"page-11-9\"></span>[60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In *Advances in Neural Information Processing Systems*, pages 5998–6008, 2017.\n- <span id=\"page-11-4\"></span>[61] Pascal Vincent. A connection between score matching and denoising autoencoders. *Neural Computation*, 23(7):1661–1674, 2011.\n- <span id=\"page-11-18\"></span>[62] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A Efros. Cnn-generated images are surprisingly easy to spot...for now. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, 2020.\n- <span id=\"page-11-10\"></span>[63] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 7794–7803, 2018.\n- <span id=\"page-11-17\"></span>[64] Auke J Wiggers and Emiel Hoogeboom. Predictive sampling with forecasting autoregressive models. *arXiv preprint arXiv:2002.09928*, 2020.\n- <span id=\"page-11-13\"></span>[65] Hao Wu, Jonas Köhler, and Frank Noé. Stochastic normalizing flows. *arXiv preprint arXiv:2002.06707*, 2020.\n- <span id=\"page-11-8\"></span>[66] Yuxin Wu and Kaiming He. Group normalization. In *Proceedings of the European Conference on Computer Vision (ECCV)*, pages 3–19, 2018.\n- <span id=\"page-11-14\"></span>[67] Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu. A theory of generative convnet. In *International Conference on Machine Learning*, pages 2635–2644, 2016.\n- [68] Jianwen Xie, Song-Chun Zhu, and Ying Nian Wu. Synthesizing dynamic patterns by spatial-temporal generative convnet. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 7093–7101, 2017.\n- <span id=\"page-11-15\"></span>[69] Jianwen Xie, Zilong Zheng, Ruiqi Gao, Wenguan Wang, Song-Chun Zhu, and Ying Nian Wu. Learning descriptor networks for 3d shape synthesis and analysis. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 8629–8638, 2018.\n- <span id=\"page-11-16\"></span>[70] Jianwen Xie, Song-Chun Zhu, and Ying Nian Wu. Learning energy-based spatial-temporal generative convnets for dynamic patterns. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 2019.\n- <span id=\"page-11-11\"></span>[71] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop. *arXiv preprint arXiv:1506.03365*, 2015.\n- <span id=\"page-11-19\"></span>[72] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. *arXiv preprint arXiv:1605.07146*, 2016.\n\n# Extra information\n\n<span id=\"page-12-2\"></span>LSUN FID scores for LSUN datasets are included in Table [3.](#page-12-2) Scores marked with <sup>∗</sup> are reported by StyleGAN2 as baselines, and other scores are reported by their respective authors.\n\n|  | Table 3: FID scores for LSUN 256 × 256 datasets |  |  |\n|--|-------------------------------------------------|--|--|\n|  |                                                 |  |  |\n\n| Model                 | LSUN Bedroom | LSUN Church | LSUN Cat |\n|-----------------------|--------------|-------------|----------|\n| ProgressiveGAN [27]   | 8.34         | 6.42        | 37.52    |\n| StyleGAN [28]         | 2.65         | 4.21∗       | 8.53∗    |\n| StyleGAN2 [30]        | -            | 3.86        | 6.93     |\n| Ours (Lsimple)        | 6.36         | 7.89        | 19.75    |\n| Ours (Lsimple, large) | 4.90         | -           | -        |\n\nProgressive compression Our lossy compression argument in Section [4.3](#page-5-1) is only a proof of concept, because Algorithms [3](#page-5-3) and [4](#page-5-3) depend on a procedure such as minimal random coding [\\[20\\]](#page-9-10), which is not tractable for high dimensional data. These algorithms serve as a compression interpretation of the variational bound [\\(5\\)](#page-2-2) of Sohl-Dickstein et al. [\\[53\\]](#page-11-3), not yet as a practical compression system.\n\n<span id=\"page-12-1\"></span>Table 4: Unconditional CIFAR10 test set rate-distortion values (accompanies Fig. [5\\)](#page-6-0)\n\n| Reverse process time (T − t + 1) | Rate (bits/dim) | Distortion (RMSE [0, 255]) |\n|----------------------------------|-----------------|----------------------------|\n| 1000                             | 1.77581         | 0.95136                    |\n| 900                              | 0.11994         | 12.02277                   |\n| 800                              | 0.05415         | 18.47482                   |\n| 700                              | 0.02866         | 24.43656                   |\n| 600                              | 0.01507         | 30.80948                   |\n| 500                              | 0.00716         | 38.03236                   |\n| 400                              | 0.00282         | 46.12765                   |\n| 300                              | 0.00081         | 54.18826                   |\n| 200                              | 0.00013         | 60.97170                   |\n| 100                              | 0.00000         | 67.60125                   |\n\n# <span id=\"page-12-0\"></span>A Extended derivations\n\nBelow is a derivation of Eq. [\\(5\\)](#page-2-2), the reduced variance variational bound for diffusion models. This material is from Sohl-Dickstein et al. [\\[53\\]](#page-11-3); we include it here only for completeness.\n\n$$L = \\mathbb{E}_q \\left[ -\\log \\frac{p_{\\theta}(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)} \\right]$$\n (17)\n\n$$= \\mathbb{E}_q \\left[ -\\log p(\\mathbf{x}_T) - \\sum_{t \\ge 1} \\log \\frac{p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)}{q(\\mathbf{x}_t|\\mathbf{x}_{t-1})} \\right]$$\n(18)\n\n$$= \\mathbb{E}_q \\left[ -\\log p(\\mathbf{x}_T) - \\sum_{t>1} \\log \\frac{p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)}{q(\\mathbf{x}_t|\\mathbf{x}_{t-1})} - \\log \\frac{p_{\\theta}(\\mathbf{x}_0|\\mathbf{x}_1)}{q(\\mathbf{x}_1|\\mathbf{x}_0)} \\right]$$\n(19)\n\n$$= \\mathbb{E}_q \\left[ -\\log p(\\mathbf{x}_T) - \\sum_{t>1} \\log \\frac{p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}|\\mathbf{x}_t, \\mathbf{x}_0)} \\cdot \\frac{q(\\mathbf{x}_{t-1}|\\mathbf{x}_0)}{q(\\mathbf{x}_t|\\mathbf{x}_0)} - \\log \\frac{p_{\\theta}(\\mathbf{x}_0|\\mathbf{x}_1)}{q(\\mathbf{x}_1|\\mathbf{x}_0)} \\right]$$\n(20)\n\n$$= \\mathbb{E}_q \\left[ -\\log \\frac{p(\\mathbf{x}_T)}{q(\\mathbf{x}_T | \\mathbf{x}_0)} - \\sum_{t>1} \\log \\frac{p_{\\theta}(\\mathbf{x}_{t-1} | \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1} | \\mathbf{x}_t, \\mathbf{x}_0)} - \\log p_{\\theta}(\\mathbf{x}_0 | \\mathbf{x}_1) \\right]$$\n(21)\n\n$$= \\mathbb{E}_{q} \\left[ D_{\\mathrm{KL}}(q(\\mathbf{x}_{T}|\\mathbf{x}_{0}) \\parallel p(\\mathbf{x}_{T})) + \\sum_{t>1} D_{\\mathrm{KL}}(q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0}) \\parallel p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_{t})) - \\log p_{\\theta}(\\mathbf{x}_{0}|\\mathbf{x}_{1}) \\right]$$\n(22)\n\nThe following is an alternate version of L. It is not tractable to estimate, but it is useful for our discussion in Section 4.3.\n\n$$L = \\mathbb{E}_q \\left[ -\\log p(\\mathbf{x}_T) - \\sum_{t \\ge 1} \\log \\frac{p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)}{q(\\mathbf{x}_t|\\mathbf{x}_{t-1})} \\right]$$\n(23)\n\n$$= \\mathbb{E}_q \\left[ -\\log p(\\mathbf{x}_T) - \\sum_{t>1} \\log \\frac{p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}|\\mathbf{x}_t)} \\cdot \\frac{q(\\mathbf{x}_{t-1})}{q(\\mathbf{x}_t)} \\right]$$\n(24)\n\n$$= \\mathbb{E}_q \\left[ -\\log \\frac{p(\\mathbf{x}_T)}{q(\\mathbf{x}_T)} - \\sum_{t \\ge 1} \\log \\frac{p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}|\\mathbf{x}_t)} - \\log q(\\mathbf{x}_0) \\right]$$\n(25)\n\n$$= D_{\\mathrm{KL}}(q(\\mathbf{x}_T) \\parallel p(\\mathbf{x}_T)) + \\mathbb{E}_q \\left[ \\sum_{t \\ge 1} D_{\\mathrm{KL}}(q(\\mathbf{x}_{t-1}|\\mathbf{x}_t) \\parallel p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)) \\right] + H(\\mathbf{x}_0)$$\n (26)\n\n### <span id=\"page-13-0\"></span>**B** Experimental details\n\nOur neural network architecture follows the backbone of PixelCNN++ [52], which is a U-Net [48] based on a Wide ResNet [72]. We replaced weight normalization [49] with group normalization [66] to make the implementation simpler. Our  $32 \\times 32$  models use four feature map resolutions ( $32 \\times 32$  to  $4 \\times 4$ ), and our  $256 \\times 256$  models use six. All models have two convolutional residual blocks per resolution level and self-attention blocks at the  $16 \\times 16$  resolution between the convolutional blocks [6]. Diffusion time t is specified by adding the Transformer sinusoidal position embedding [60] into each residual block. Our CIFAR10 model has 35.7 million parameters, and our LSUN and CelebA-HQ models have 114 million parameters. We also trained a larger variant of the LSUN Bedroom model with approximately 256 million parameters by increasing filter count.\n\nWe used TPU v3-8 (similar to 8 V100 GPUs) for all experiments. Our CIFAR model trains at 21 steps per second at batch size 128 (10.6 hours to train to completion at 800k steps), and sampling a batch of 256 images takes 17 seconds. Our CelebA-HQ/LSUN (256<sup>2</sup>) models train at 2.2 steps per second at batch size 64, and sampling a batch of 128 images takes 300 seconds. We trained on CelebA-HQ for 0.5M steps, LSUN Bedroom for 2.4M steps, LSUN Cat for 1.8M steps, and LSUN Church for 1.2M steps. The larger LSUN Bedroom model was trained for 1.15M steps.\n\nApart from an initial choice of hyperparameters early on to make network size fit within memory constraints, we performed the majority of our hyperparameter search to optimize for CIFAR10 sample quality, then transferred the resulting settings over to the other datasets:\n\n- We chose the  $\\beta_t$  schedule from a set of constant, linear, and quadratic schedules, all constrained so that  $L_T \\approx 0$ . We set T = 1000 without a sweep, and we chose a linear schedule from  $\\beta_1 = 10^{-4}$  to  $\\beta_T = 0.02$ .\n- We set the dropout rate on CIFAR10 to 0.1 by sweeping over the values {0.1, 0.2, 0.3, 0.4}. Without dropout on CIFAR10, we obtained poorer samples reminiscent of the overfitting artifacts in an unregularized PixelCNN++ [52]. We set dropout rate on the other datasets to zero without sweeping.\n- We used random horizontal flips during training for CIFAR10; we tried training both with and without flips, and found flips to improve sample quality slightly. We also used random horizontal flips for all other datasets except LSUN Bedroom.\n- We tried Adam [31] and RMSProp early on in our experimentation process and chose the former. We left the hyperparameters to their standard values. We set the learning rate to 2 × 10<sup>-4</sup> without any sweeping, and we lowered it to 2 × 10<sup>-5</sup> for the 256 × 256 images, which seemed unstable to train with the larger learning rate.\n\n- We set the batch size to 128 for CIFAR10 and 64 for larger images. We did not sweep over these values.\n- We used EMA on model parameters with a decay factor of 0.9999. We did not sweep over this value.\n\nFinal experiments were trained once and evaluated throughout training for sample quality. Sample quality scores and log likelihood are reported on the minimum FID value over the course of training. On CIFAR10, we calculated Inception and FID scores on 50000 samples using the original code from the OpenAI [51] and TTUR [21] repositories, respectively. On LSUN, we calculated FID scores on 50000 samples using code from the StyleGAN2 [30] repository. CIFAR10 and CelebA-HQ were loaded as provided by TensorFlow Datasets (https://www.tensorflow.org/datasets), and LSUN was prepared using code from StyleGAN. Dataset splits (or lack thereof) are standard from the papers that introduced their usage in a generative modeling context. All details can be found in the source code release.\n\n### <span id=\"page-14-1\"></span>C Discussion on related work\n\nOur model architecture, forward process definition, and prior differ from NCSN [55, 56] in subtle but important ways that improve sample quality, and, notably, we directly train our sampler as a latent variable model rather than adding it after training post-hoc. In greater detail:\n\n- 1. We use a U-Net with self-attention; NCSN uses a RefineNet with dilated convolutions. We condition all layers on t by adding in the Transformer sinusoidal position embedding, rather than only in normalization layers (NCSNv1) or only at the output (v2).\n- 2. Diffusion models scale down the data with each forward process step (by a  $\\sqrt{1-\\beta_t}$  factor) so that variance does not grow when adding noise, thus providing consistently scaled inputs to the neural net reverse process. NCSN omits this scaling factor.\n- 3. Unlike NCSN, our forward process destroys signal  $(D_{KL}(q(\\mathbf{x}_T|\\mathbf{x}_0) \\parallel \\mathcal{N}(\\mathbf{0}, \\mathbf{I})) \\approx 0)$ , ensuring a close match between the prior and aggregate posterior of  $\\mathbf{x}_T$ . Also unlike NCSN, our  $\\beta_t$  are very small, which ensures that the forward process is reversible by a Markov chain with conditional Gaussians. Both of these factors prevent distribution shift when sampling.\n- 4. Our Langevin-like sampler has coefficients (learning rate, noise scale, etc.) derived rigorously from  $\\beta_t$  in the forward process. Thus, our training procedure directly trains our sampler to match the data distribution after T steps: it trains the sampler as a latent variable model using variational inference. In contrast, NCSN's sampler coefficients are set by hand post-hoc, and their training procedure is not guaranteed to directly optimize a quality metric of their sampler.\n\n### <span id=\"page-14-0\"></span>**D** Samples\n\n**Additional samples** Figure 11, 13, 16, 17, 18, and 19 show uncurated samples from the diffusion models trained on CelebA-HQ, CIFAR10 and LSUN datasets.\n\nLatent structure and reverse process stochasticity During sampling, both the prior  $\\mathbf{x}_T \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I})$  and Langevin dynamics are stochastic. To understand the significance of the second source of noise, we sampled multiple images conditioned on the same intermediate latent for the CelebA  $256 \\times 256$  dataset. Figure 7 shows multiple draws from the reverse process  $\\mathbf{x}_0 \\sim p_{\\theta}(\\mathbf{x}_0|\\mathbf{x}_t)$  that share the latent  $\\mathbf{x}_t$  for  $t \\in \\{1000, 750, 500, 250\\}$ . To accomplish this, we run a single reverse chain from an initial draw from the prior. At the intermediate timesteps, the chain is split to sample multiple images. When the chain is split after the prior draw at  $\\mathbf{x}_{T=1000}$ , the samples differ significantly. However, when the chain is split after more steps, samples share high-level attributes like gender, hair color, eyewear, saturation, pose and facial expression. This indicates that intermediate latents like  $\\mathbf{x}_{750}$  encode these attributes, despite their imperceptibility.\n\nCoarse-to-fine interpolation Figure 9 shows interpolations between a pair of source CelebA  $256 \\times 256$  images as we vary the number of diffusion steps prior to latent space interpolation. Increasing the number of diffusion steps destroys more structure in the source images, which the\n\nmodel completes during the reverse process. This allows us to interpolate at both fine granularities and coarse granularities. In the limiting case of 0 diffusion steps, the interpolation mixes source images in pixel space. On the other hand, after 1000 diffusion steps, source information is lost and interpolations are novel samples.\n\n<span id=\"page-15-1\"></span>![](_page_15_Figure_1.jpeg)\n\nFigure 9: Coarse-to-fine interpolations that vary the number of diffusion steps prior to latent mixing.\n\n<span id=\"page-15-0\"></span>![](_page_15_Figure_3.jpeg)\n\nFigure 10: Unconditional CIFAR10 progressive sampling quality over time\n\n<span id=\"page-16-0\"></span>![](_page_16_Picture_0.jpeg)\n\nFigure 11: CelebA-HQ 256 × 256 generated samples\n\n![](_page_17_Figure_0.jpeg)\n\n(b) Inception feature space nearest neighbors\n\nFigure 12: CelebA-HQ 256 × 256 nearest neighbors, computed on a 100 × 100 crop surrounding the faces. Generated samples are in the leftmost column, and training set nearest neighbors are in the remaining columns.\n\n<span id=\"page-18-0\"></span>![](_page_18_Figure_0.jpeg)\n\nFigure 13: Unconditional CIFAR10 generated samples\n\n<span id=\"page-19-0\"></span>![](_page_19_Figure_0.jpeg)\n\nFigure 14: Unconditional CIFAR10 progressive generation\n\n![](_page_20_Figure_0.jpeg)\n\n(b) Inception feature space nearest neighbors\n\nFigure 15: Unconditional CIFAR10 nearest neighbors. Generated samples are in the leftmost column, and training set nearest neighbors are in the remaining columns.\n\n<span id=\"page-21-0\"></span>![](_page_21_Picture_0.jpeg)\n\nFigure 16: LSUN Church generated samples. FID=7.89\n\n<span id=\"page-22-0\"></span>![](_page_22_Picture_0.jpeg)\n\nFigure 17: LSUN Bedroom generated samples, large model. FID=4.90\n\n<span id=\"page-23-0\"></span>![](_page_23_Picture_0.jpeg)\n\nFigure 18: LSUN Bedroom generated samples, small model. FID=6.36\n\n<span id=\"page-24-0\"></span>![](_page_24_Picture_0.jpeg)\n\nFigure 19: LSUN Cat generated samples. FID=19.75",
  "chunks": [
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_0",
      "content": "# Denoising Diffusion Probabilistic Models\n\nJonathan Ho UC Berkeley jonathanho@berkeley.edu\n\nAjay Jain UC Berkeley ajayj@berkeley.edu\n\nPieter Abbeel UC Berkeley pabbeel@cs.berkeley.edu\n\n# Abstract\n\nWe present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at <https://github.com/hojonathanho/diffusion>.",
      "metadata": {
        "chunk_index": 0,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 997
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_1",
      "content": "# 1 Introduction\n\nDeep generative models of all kinds have recently exhibited high quality samples in a wide variety of data modalities. Generative adversarial networks (GANs), autoregressive models, flows, and variational autoencoders (VAEs) have synthesized striking image and audio samples [\\[14,](#page-9-0) [27,](#page-9-1) [3,](#page-8-0) [58,](#page-11-0) [38,](#page-10-0) [25,](#page-9-2) [10,](#page-9-3) [32,](#page-10-1) [44,](#page-10-2) [57,](#page-11-1) [26,](#page-9-4) [33,](#page-10-3) [45\\]](#page-10-4), and there have been remarkable advances in energy-based modeling and score matching that have produced images comparable to those of GANs [\\[11,](#page-9-5) [55\\]](#page-11-2).\n\n<span id=\"page-0-0\"></span>![](_page_0_Picture_9.jpeg)\n\nFigure 1: Generated samples on CelebA-HQ 256 × 256 (left) and unconditional CIFAR10 (right)\n\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\n\n![](_page_1_Figure_0.jpeg)",
      "metadata": {
        "chunk_index": 1,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 970
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_2",
      "content": "34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\n\n![](_page_1_Figure_0.jpeg)\n\nFigure 2: The directed graphical model considered in this work.\n\nThis paper presents progress in diffusion probabilistic models [53]. A diffusion probabilistic model (which we will call a \"diffusion model\" for brevity) is a parameterized Markov chain trained using variational inference to produce samples matching the data after finite time. Transitions of this chain are learned to reverse a diffusion process, which is a Markov chain that gradually adds noise to the data in the opposite direction of sampling until signal is destroyed. When the diffusion consists of small amounts of Gaussian noise, it is sufficient to set the sampling chain transitions to conditional Gaussians too, allowing for a particularly simple neural network parameterization.",
      "metadata": {
        "chunk_index": 2,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 877
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_3",
      "content": "Diffusion models are straightforward to define and efficient to train, but to the best of our knowledge, there has been no demonstration that they are capable of generating high quality samples. We show that diffusion models actually are capable of generating high quality samples, sometimes better than the published results on other types of generative models (Section 4). In addition, we show that a certain parameterization of diffusion models reveals an equivalence with denoising score matching over multiple noise levels during training and with annealed Langevin dynamics during sampling (Section 3.2) [55, 61]. We obtained our best sample quality results using this parameterization (Section 4.2), so we consider this equivalence to be one of our primary contributions.",
      "metadata": {
        "chunk_index": 3,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 778
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_4",
      "content": "Despite their sample quality, our models do not have competitive log likelihoods compared to other likelihood-based models (our models do, however, have log likelihoods better than the large estimates annealed importance sampling has been reported to produce for energy based models and score matching [11, 55]). We find that the majority of our models' lossless codelengths are consumed to describe imperceptible image details (Section 4.3). We present a more refined analysis of this phenomenon in the language of lossy compression, and we show that the sampling procedure of diffusion models is a type of progressive decoding that resembles autoregressive decoding along a bit ordering that vastly generalizes what is normally possible with autoregressive models.",
      "metadata": {
        "chunk_index": 4,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 766
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_5",
      "content": "### 2 Background\n\nDiffusion models [53] are latent variable models of the form  $p_{\\theta}(\\mathbf{x}_0) \\coloneqq \\int p_{\\theta}(\\mathbf{x}_{0:T}) d\\mathbf{x}_{1:T}$ , where  $\\mathbf{x}_1, \\dots, \\mathbf{x}_T$  are latents of the same dimensionality as the data  $\\mathbf{x}_0 \\sim q(\\mathbf{x}_0)$ . The joint distribution  $p_{\\theta}(\\mathbf{x}_{0:T})$  is called the *reverse process*, and it is defined as a Markov chain with learned Gaussian transitions starting at  $p(\\mathbf{x}_T) = \\mathcal{N}(\\mathbf{x}_T; \\mathbf{0}, \\mathbf{I})$ :\n\n$$p_{\\theta}(\\mathbf{x}_{0:T}) := p(\\mathbf{x}_T) \\prod_{t=1}^{T} p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t), \\qquad p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t) := \\mathcal{N}(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_{\\theta}(\\mathbf{x}_t, t), \\boldsymbol{\\Sigma}_{\\theta}(\\mathbf{x}_t, t))$$\n(1)",
      "metadata": {
        "chunk_index": 5,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 837
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_6",
      "content": "What distinguishes diffusion models from other types of latent variable models is that the approximate posterior  $q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)$ , called the *forward process* or *diffusion process*, is fixed to a Markov chain that gradually adds Gaussian noise to the data according to a variance schedule  $\\beta_1, \\ldots, \\beta_T$ :\n\n$$q(\\mathbf{x}_{1:T}|\\mathbf{x}_0) := \\prod_{t=1}^{T} q(\\mathbf{x}_t|\\mathbf{x}_{t-1}), \\qquad q(\\mathbf{x}_t|\\mathbf{x}_{t-1}) := \\mathcal{N}(\\mathbf{x}_t; \\sqrt{1-\\beta_t}\\mathbf{x}_{t-1}, \\beta_t \\mathbf{I})$$\n(2)\n\nTraining is performed by optimizing the usual variational bound on negative log likelihood:\n\n$$\\mathbb{E}\\left[-\\log p_{\\theta}(\\mathbf{x}_{0})\\right] \\leq \\mathbb{E}_{q}\\left[-\\log \\frac{p_{\\theta}(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T}|\\mathbf{x}_{0})}\\right] = \\mathbb{E}_{q}\\left[-\\log p(\\mathbf{x}_{T}) - \\sum_{t>1} \\log \\frac{p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_{t})}{q(\\mathbf{x}_{t}|\\mathbf{x}_{t-1})}\\right] =: L \\quad (3)$$",
      "metadata": {
        "chunk_index": 6,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 995
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_7",
      "content": "The forward process variances  $\\beta_t$  can be learned by reparameterization [33] or held constant as hyperparameters, and expressiveness of the reverse process is ensured in part by the choice of Gaussian conditionals in  $p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)$ , because both processes have the same functional form when  $\\beta_t$  are small [53]. A notable property of the forward process is that it admits sampling  $\\mathbf{x}_t$  at an arbitrary timestep t in closed form: using the notation  $\\alpha_t := 1 - \\beta_t$  and  $\\bar{\\alpha}_t := \\prod_{s=1}^t \\alpha_s$ , we have\n\n<span id=\"page-1-2\"></span><span id=\"page-1-1\"></span><span id=\"page-1-0\"></span>\n$$q(\\mathbf{x}_t|\\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t; \\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0, (1 - \\bar{\\alpha}_t)\\mathbf{I})$$\n(4)\n\nEfficient training is therefore possible by optimizing random terms of L with stochastic gradient descent. Further improvements come from variance reduction by rewriting L (3) as:",
      "metadata": {
        "chunk_index": 7,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 986
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_8",
      "content": "Efficient training is therefore possible by optimizing random terms of L with stochastic gradient descent. Further improvements come from variance reduction by rewriting L (3) as:\n\n$$\\mathbb{E}_{q}\\left[\\underbrace{D_{\\mathrm{KL}}(q(\\mathbf{x}_{T}|\\mathbf{x}_{0}) \\parallel p(\\mathbf{x}_{T}))}_{L_{t}} + \\sum_{t>1} \\underbrace{D_{\\mathrm{KL}}(q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0}) \\parallel p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_{t}))}_{L_{t}} \\underbrace{-\\log p_{\\theta}(\\mathbf{x}_{0}|\\mathbf{x}_{1})}_{L_{t}}\\right]$$\n(5)\n\n(See Appendix A for details. The labels on the terms are used in Section 3.) Equation (5) uses KL divergence to directly compare  $p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)$  against forward process posteriors, which are tractable when conditioned on  $\\mathbf{x}_0$ :",
      "metadata": {
        "chunk_index": 8,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 808
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_9",
      "content": "<span id=\"page-2-4\"></span><span id=\"page-2-2\"></span>\n$$q(\\mathbf{x}_{t-1}|\\mathbf{x}_t, \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0), \\tilde{\\beta}_t \\mathbf{I}), \\tag{6}$$\n\nwhere \n$$\\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0) \\coloneqq \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}\\beta_t}{1 - \\bar{\\alpha}_t}\\mathbf{x}_0 + \\frac{\\sqrt{\\alpha_t}(1 - \\bar{\\alpha}_{t-1})}{1 - \\bar{\\alpha}_t}\\mathbf{x}_t$$\n and  $\\tilde{\\beta}_t \\coloneqq \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t}\\beta_t$  (7)\n\nConsequently, all KL divergences in Eq. (5) are comparisons between Gaussians, so they can be calculated in a Rao-Blackwellized fashion with closed form expressions instead of high variance Monte Carlo estimates.",
      "metadata": {
        "chunk_index": 9,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 770
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_10",
      "content": "### <span id=\"page-2-1\"></span>3 Diffusion models and denoising autoencoders\n\nDiffusion models might appear to be a restricted class of latent variable models, but they allow a large number of degrees of freedom in implementation. One must choose the variances  $\\beta_t$  of the forward process and the model architecture and Gaussian distribution parameterization of the reverse process. To guide our choices, we establish a new explicit connection between diffusion models and denoising score matching (Section 3.2) that leads to a simplified, weighted variational bound objective for diffusion models (Section 3.4). Ultimately, our model design is justified by simplicity and empirical results (Section 4). Our discussion is categorized by the terms of Eq. (5).",
      "metadata": {
        "chunk_index": 10,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 765
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_11",
      "content": "### 3.1 Forward process and $L_T$\n\nWe ignore the fact that the forward process variances  $\\beta_t$  are learnable by reparameterization and instead fix them to constants (see Section 4 for details). Thus, in our implementation, the approximate posterior q has no learnable parameters, so  $L_T$  is a constant during training and can be ignored.",
      "metadata": {
        "chunk_index": 11,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 346
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_12",
      "content": "### <span id=\"page-2-0\"></span>**3.2** Reverse process and $L_{1:T-1}$\n\nNow we discuss our choices in  $p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_{\\theta}(\\mathbf{x}_t, t), \\boldsymbol{\\Sigma}_{\\theta}(\\mathbf{x}_t, t))$  for  $1 < t \\leq T$ . First, we set  $\\boldsymbol{\\Sigma}_{\\theta}(\\mathbf{x}_t, t) = \\sigma_t^2 \\mathbf{I}$  to untrained time dependent constants. Experimentally, both  $\\sigma_t^2 = \\beta_t$  and  $\\sigma_t^2 = \\tilde{\\beta}_t = \\frac{1 - \\tilde{\\alpha}_{t-1}}{1 - \\tilde{\\alpha}_t} \\beta_t$  had similar results. The first choice is optimal for  $\\mathbf{x}_0 \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ , and the second is optimal for  $\\mathbf{x}_0$  deterministically set to one point. These are the two extreme choices corresponding to upper and lower bounds on reverse process entropy for data with coordinatewise unit variance [53].",
      "metadata": {
        "chunk_index": 12,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 911
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_13",
      "content": "Second, to represent the mean  $\\mu_{\\theta}(\\mathbf{x}_t,t)$ , we propose a specific parameterization motivated by the following analysis of  $L_t$ . With  $p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\boldsymbol{\\mu}_{\\theta}(\\mathbf{x}_t,t), \\sigma_t^2 \\mathbf{I})$ , we can write:\n\n<span id=\"page-2-5\"></span><span id=\"page-2-3\"></span>\n$$L_{t-1} = \\mathbb{E}_q \\left[ \\frac{1}{2\\sigma_t^2} \\| \\tilde{\\boldsymbol{\\mu}}_t(\\mathbf{x}_t, \\mathbf{x}_0) - \\boldsymbol{\\mu}_{\\theta}(\\mathbf{x}_t, t) \\|^2 \\right] + C$$\n (8)",
      "metadata": {
        "chunk_index": 13,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 551
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_14",
      "content": "where C is a constant that does not depend on  $\\theta$ . So, we see that the most straightforward parameterization of  $\\mu_{\\theta}$  is a model that predicts  $\\tilde{\\mu}_{t}$ , the forward process posterior mean. However, we can expand Eq. (8) further by reparameterizing Eq. (4) as  $\\mathbf{x}_{t}(\\mathbf{x}_{0}, \\boldsymbol{\\epsilon}) = \\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{0} + \\sqrt{1 - \\bar{\\alpha}_{t}}\\boldsymbol{\\epsilon}$  for  $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$  and applying the forward process posterior formula (7):\n\n$$L_{t-1} - C = \\mathbb{E}_{\\mathbf{x}_0, \\epsilon} \\left[ \\frac{1}{2\\sigma_t^2} \\left\\| \\tilde{\\boldsymbol{\\mu}}_t \\left( \\mathbf{x}_t(\\mathbf{x}_0, \\epsilon), \\frac{1}{\\sqrt{\\bar{\\alpha}_t}} (\\mathbf{x}_t(\\mathbf{x}_0, \\epsilon) - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon) \\right) - \\boldsymbol{\\mu}_{\\theta}(\\mathbf{x}_t(\\mathbf{x}_0, \\epsilon), t) \\right\\|^2 \\right]$$\n(9)",
      "metadata": {
        "chunk_index": 14,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 935
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_15",
      "content": "$$= \\mathbb{E}_{\\mathbf{x}_0, \\epsilon} \\left[ \\frac{1}{2\\sigma_t^2} \\left\\| \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t(\\mathbf{x}_0, \\epsilon) - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon \\right) - \\boldsymbol{\\mu}_{\\theta}(\\mathbf{x}_t(\\mathbf{x}_0, \\epsilon), t) \\right\\|^2 \\right]$$\n(10)",
      "metadata": {
        "chunk_index": 15,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 302
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_16",
      "content": "# <span id=\"page-3-1\"></span>Algorithm 1 Training 1: repeat 2: $\\mathbf{x}_0 \\sim q(\\mathbf{x}_0)$ 3: $t \\sim \\text{Uniform}(\\{1, \\dots, T\\})$ 4: $\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ 5: Take gradient descent step on $\\nabla_{\\theta} \\| \\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_{\\theta}(\\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon}, t) \\|^2$ 6: until converged Algorithm 2 Sampling 1: $\\mathbf{x}_T \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ 2: for $t = T, \\dots, 1$ do 3: $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ if t > 1, else $\\mathbf{z} = \\mathbf{0}$ 4: $\\mathbf{x}_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\boldsymbol{\\epsilon}_{\\theta}(\\mathbf{x}_t, t) \\right) + \\sigma_t \\mathbf{z}$ 5: end for 6: return $\\mathbf{x}_0$",
      "metadata": {
        "chunk_index": 16,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 856
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_17",
      "content": "Equation (10) reveals that  $\\mu_{\\theta}$  must predict  $\\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon \\right)$  given  $\\mathbf{x}_t$ . Since  $\\mathbf{x}_t$  is available as input to the model, we may choose the parameterization\n\n$$\\boldsymbol{\\mu}_{\\theta}(\\mathbf{x}_{t}, t) = \\tilde{\\boldsymbol{\\mu}}_{t} \\left( \\mathbf{x}_{t}, \\frac{1}{\\sqrt{\\bar{\\alpha}_{t}}} (\\mathbf{x}_{t} - \\sqrt{1 - \\bar{\\alpha}_{t}} \\boldsymbol{\\epsilon}_{\\theta}(\\mathbf{x}_{t})) \\right) = \\frac{1}{\\sqrt{\\alpha_{t}}} \\left( \\mathbf{x}_{t} - \\frac{\\beta_{t}}{\\sqrt{1 - \\bar{\\alpha}_{t}}} \\boldsymbol{\\epsilon}_{\\theta}(\\mathbf{x}_{t}, t) \\right)$$\n(11)",
      "metadata": {
        "chunk_index": 17,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 686
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_18",
      "content": "where  $\\epsilon_{\\theta}$  is a function approximator intended to predict  $\\epsilon$  from  $\\mathbf{x}_t$ . To sample  $\\mathbf{x}_{t-1} \\sim p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)$  is to compute  $\\mathbf{x}_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( \\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1-\\alpha_t}} \\epsilon_{\\theta}(\\mathbf{x}_t,t) \\right) + \\sigma_t \\mathbf{z}$ , where  $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I})$ . The complete sampling procedure, Algorithm 2, resembles Langevin dynamics with  $\\epsilon_{\\theta}$  as a learned gradient of the data density. Furthermore, with the parameterization (11), Eq. (10) simplifies to:\n\n<span id=\"page-3-3\"></span><span id=\"page-3-2\"></span>\n$$\\mathbb{E}_{\\mathbf{x}_{0},\\epsilon} \\left[ \\frac{\\beta_{t}^{2}}{2\\sigma_{t}^{2}\\alpha_{t}(1-\\bar{\\alpha}_{t})} \\left\\| \\epsilon - \\epsilon_{\\theta}(\\sqrt{\\bar{\\alpha}_{t}}\\mathbf{x}_{0} + \\sqrt{1-\\bar{\\alpha}_{t}}\\epsilon, t) \\right\\|^{2} \\right]$$\n(12)",
      "metadata": {
        "chunk_index": 18,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 960
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_19",
      "content": "which resembles denoising score matching over multiple noise scales indexed by t [55]. As Eq. (12) is equal to (one term of) the variational bound for the Langevin-like reverse process (11), we see that optimizing an objective resembling denoising score matching is equivalent to using variational inference to fit the finite-time marginal of a sampling chain resembling Langevin dynamics.",
      "metadata": {
        "chunk_index": 19,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 389
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_20",
      "content": "To summarize, we can train the reverse process mean function approximator  $\\mu_{\\theta}$  to predict  $\\tilde{\\mu}_{t}$ , or by modifying its parameterization, we can train it to predict  $\\epsilon$ . (There is also the possibility of predicting  $\\mathbf{x}_{0}$ , but we found this to lead to worse sample quality early in our experiments.) We have shown that the  $\\epsilon$ -prediction parameterization both resembles Langevin dynamics and simplifies the diffusion model's variational bound to an objective that resembles denoising score matching. Nonetheless, it is just another parameterization of  $p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_{t})$ , so we verify its effectiveness in Section 4 in an ablation where we compare predicting  $\\epsilon$  against predicting  $\\tilde{\\mu}_{t}$ .",
      "metadata": {
        "chunk_index": 20,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 793
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_21",
      "content": "### 3.3 Data scaling, reverse process decoder, and $L_0$\n\nWe assume that image data consists of integers in  $\\{0, 1, \\dots, 255\\}$  scaled linearly to [-1, 1]. This ensures that the neural network reverse process operates on consistently scaled inputs starting from the standard normal prior  $p(\\mathbf{x}_T)$ . To obtain discrete log likelihoods, we set the last term of the reverse process to an independent discrete decoder derived from the Gaussian  $\\mathcal{N}(\\mathbf{x}_0; \\boldsymbol{\\mu}_{\\theta}(\\mathbf{x}_1, 1), \\sigma_1^2 \\mathbf{I})$ :\n\n<span id=\"page-3-4\"></span>\n$$p_{\\theta}(\\mathbf{x}_{0}|\\mathbf{x}_{1}) = \\prod_{i=1}^{D} \\int_{\\delta_{-}(x_{0}^{i})}^{\\delta_{+}(x_{0}^{i})} \\mathcal{N}(x; \\mu_{\\theta}^{i}(\\mathbf{x}_{1}, 1), \\sigma_{1}^{2}) dx$$",
      "metadata": {
        "chunk_index": 21,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 769
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_22",
      "content": "$$\\delta_{+}(x) = \\begin{cases} \\infty & \\text{if } x = 1\\\\ x + \\frac{1}{255} & \\text{if } x < 1 \\end{cases} \\quad \\delta_{-}(x) = \\begin{cases} -\\infty & \\text{if } x = -1\\\\ x - \\frac{1}{255} & \\text{if } x > -1 \\end{cases}$$\n(13)\n\nwhere D is the data dimensionality and the i superscript indicates extraction of one coordinate. (It would be straightforward to instead incorporate a more powerful decoder like a conditional autoregressive model, but we leave that to future work.) Similar to the discretized continuous distributions used in VAE decoders and autoregressive models [34, 52], our choice here ensures that the variational bound is a lossless codelength of discrete data, without need of adding noise to the data or incorporating the Jacobian of the scaling operation into the log likelihood. At the end of sampling, we display  $\\mu_{\\theta}(\\mathbf{x}_1, 1)$  noiselessly.",
      "metadata": {
        "chunk_index": 22,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 887
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_23",
      "content": "### <span id=\"page-3-0\"></span>3.4 Simplified training objective\n\nWith the reverse process and decoder defined above, the variational bound, consisting of terms derived from Eqs. (12) and (13), is clearly differentiable with respect to  $\\theta$  and is ready to be employed for\n\n<span id=\"page-4-2\"></span>Table 1: CIFAR10 results. NLL measured in bits/dim.",
      "metadata": {
        "chunk_index": 23,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 358
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_24",
      "content": "| Model                                                                     | IS                                                               | FID                                  | NLL Test (Train)                         | —<br>————————————————————————————————————                                                                                         | 1 0751 540                   |                           |\n|---------------------------------------------------------------------------|------------------------------------------------------------------|--------------------------------------|------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|------------------------------|---------------------------|",
      "metadata": {
        "chunk_index": 24,
        "content_type": "text",
        "has_table": true,
        "has_figure": false,
        "char_count": 835
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_25",
      "content": "| Conditional                                                               |                                                                  |                                      |                                          | -Table 2: Unconditio<br>_process parameterizati                                                                                   |                              |                           |\n| EBM [11]<br>JEM [17]<br>BigGAN [3]<br>StyleGAN2 + ADA (v1) [29]           | 8.30<br>8.76<br>9.22<br><b>10.06</b>                             | 37.9<br>38.4<br>14.73<br><b>2.67</b> |                                          | tive ablation. Blank er<br>train and generated por<br>range scores.                                                               | ntries were uns              | stable to                 |",
      "metadata": {
        "chunk_index": 25,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 835
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_26",
      "content": "| Unconditional                                                             |                                                                  |                                      |                                          | Objective                                                                                                                         | IS                           | FID                       |\n| Diffusion (original) [53]                                                 |                                                                  |                                      | < 5.40                                   | $\tilde{\\mu}$ prediction (baseline)                                                                                                |                              |                           |",
      "metadata": {
        "chunk_index": 26,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 835
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_27",
      "content": "| Gated PixelCNN [59]<br>Sparse Transformer [7]<br>PixelIQN [43]            | 4.60<br>5.29                                                     | 65.93<br>49.46                       | 3.03 (2.90)<br><b>2.80</b>               | $L$ , learned diagonal $\\Sigma$ $L$ , fixed isotropic $\\Sigma$ $\\ \\tilde{\\mu} - \\tilde{\\mu}_{\\theta}\\ ^2$                         | $7.28\\pm0.10$ $8.06\\pm0.09$  | 23.69<br>13.22<br>-       |\n| EBM [11]<br>NCSNv2 [56]                                                   | 6.78                                                             | $38.2 \\\\ 31.75$                      |                                          | $\\epsilon$ prediction (ours)                                                                                                      |                              |                           |",
      "metadata": {
        "chunk_index": 27,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 835
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_28",
      "content": "| NCSN [55]<br>SNGAN [39]<br>SNGAN-DDLS [4]<br>StyleGAN2 + ADA (v1) [29]    | $8.87\\pm0.12$<br>$8.22\\pm0.05$<br>$9.09\\pm0.10$<br>$9.74\\pm0.05$ | 25.32<br>21.7<br>15.42<br>3.26       |                                          | $L$ , learned diagonal $\\Sigma$ $L$ , fixed isotropic $\\Sigma$ $\\ \\tilde{\\epsilon} - \\epsilon_{\\theta}\\ ^2 (L_{\\mathrm{simple}})$ | $-7.67\\pm0.13$ $9.46\\pm0.11$ | -<br>13.51<br><b>3.17</b> |\n| Ours $(L, \\text{ fixed isotropic } \\Sigma)$<br>Ours $(L_{\\text{simple}})$ | $7.67 \\pm 0.13$<br>$9.46 \\pm 0.11$                               | 13.51<br><b>3.17</b>                 | $\\leq 3.70 (3.69)$<br>$\\leq 3.75 (3.72)$ |                                                                                                                                   |                              |                           |",
      "metadata": {
        "chunk_index": 28,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 835
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_29",
      "content": "training. However, we found it beneficial to sample quality (and simpler to implement) to train on the following variant of the variational bound:\n\n<span id=\"page-4-1\"></span>\n$$L_{\\text{simple}}(\\theta) := \\mathbb{E}_{t,\\mathbf{x}_0,\\epsilon} \\left[ \\left\\| \\epsilon - \\epsilon_{\\theta} (\\sqrt{\\bar{\\alpha}_t} \\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon, t) \\right\\|^2 \\right]$$\n (14)\n\nwhere t is uniform between 1 and T. The t=1 case corresponds to  $L_0$  with the integral in the discrete decoder definition (13) approximated by the Gaussian probability density function times the bin width, ignoring  $\\sigma_1^2$  and edge effects. The t>1 cases correspond to an unweighted version of Eq. (12), analogous to the loss weighting used by the NCSN denoising score matching model [55]. ( $L_T$  does not appear because the forward process variances  $\\beta_t$  are fixed.) Algorithm 1 displays the complete training procedure with this simplified objective.",
      "metadata": {
        "chunk_index": 29,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 964
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_30",
      "content": "Since our simplified objective (14) discards the weighting in Eq. (12), it is a weighted variational bound that emphasizes different aspects of reconstruction compared to the standard variational bound [18, 22]. In particular, our diffusion process setup in Section 4 causes the simplified objective to down-weight loss terms corresponding to small t. These terms train the network to denoise data with very small amounts of noise, so it is beneficial to down-weight them so that the network can focus on more difficult denoising tasks at larger t terms. We will see in our experiments that this reweighting leads to better sample quality.",
      "metadata": {
        "chunk_index": 30,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 639
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_31",
      "content": "### <span id=\"page-4-0\"></span>4 Experiments\n\nWe set T=1000 for all experiments so that the number of neural network evaluations needed during sampling matches previous work [53, 55]. We set the forward process variances to constants increasing linearly from  $\\beta_1=10^{-4}$  to  $\\beta_T=0.02$ . These constants were chosen to be small relative to data scaled to [-1,1], ensuring that reverse and forward processes have approximately the same functional form while keeping the signal-to-noise ratio at  $\\mathbf{x}_T$  as small as possible ( $L_T=D_{\\mathrm{KL}}(q(\\mathbf{x}_T|\\mathbf{x}_0) \\parallel \\mathcal{N}(\\mathbf{0},\\mathbf{I})) \\approx 10^{-5}$  bits per dimension in our experiments).",
      "metadata": {
        "chunk_index": 31,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 699
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_32",
      "content": "To represent the reverse process, we use a U-Net backbone similar to an unmasked PixelCNN++ [52, 48] with group normalization throughout [66]. Parameters are shared across time, which is specified to the network using the Transformer sinusoidal position embedding [60]. We use self-attention at the  $16 \\times 16$  feature map resolution [63, 60]. Details are in Appendix B.",
      "metadata": {
        "chunk_index": 32,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 375
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_33",
      "content": "### 4.1 Sample quality\n\nTable 1 shows Inception scores, FID scores, and negative log likelihoods (lossless codelengths) on CIFAR10. With our FID score of 3.17, our unconditional model achieves better sample quality than most models in the literature, including class conditional models. Our FID score is computed with respect to the training set, as is standard practice; when we compute it with respect to the test set, the score is 5.24, which is still better than many of the training set FID scores in the literature.\n\n<span id=\"page-5-2\"></span>![](_page_5_Picture_0.jpeg)\n\n![](_page_5_Picture_1.jpeg)\n\nFigure 3: LSUN Church samples. FID=7.89\n\nFigure 4: LSUN Bedroom samples. FID=4.90\n\n<span id=\"page-5-3\"></span>",
      "metadata": {
        "chunk_index": 33,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 718
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_34",
      "content": "| Algorithm 3 Sending $x_0$                                                                                                    | Algorithm 4 Receiving                                                         |\n|------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------|\n| 1: Send $\\mathbf{x}_T \\sim q(\\mathbf{x}_T \\mathbf{x}_0)$ using $p(\\mathbf{x}_T)$                                             | 1: Receive $\\mathbf{x}_T$ using $p(\\mathbf{x}_T)$                             |\n| 2: <b>for</b> $t = T - 1, \\dots, 2, 1$ <b>do</b>                                                                             | 2: for $t = T - 1, \\dots, 1, 0$ do                                            |",
      "metadata": {
        "chunk_index": 34,
        "content_type": "text",
        "has_table": true,
        "has_figure": false,
        "char_count": 835
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_35",
      "content": "| 3: Send $\\mathbf{x}_t \\sim q(\\mathbf{x}_t \\mathbf{x}_{t+1}, \\mathbf{x}_0)$ using $p_{\\theta}(\\mathbf{x}_t \\mathbf{x}_{t+1})$ | 3: Receive $\\mathbf{x}_t$ using $p_{\\theta}(\\mathbf{x}_t   \\mathbf{x}_{t+1})$ |\n| 4: <b>end for</b>                                                                                                            | 4: end for                                                                    |\n| 5: Send $\\mathbf{x}_0$ using $p_{\\theta}(\\mathbf{x}_0 \\mathbf{x}_1)$                                                         | 5: return $\\mathbf{x}_0$                                                      |",
      "metadata": {
        "chunk_index": 35,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 626
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_36",
      "content": "We find that training our models on the true variational bound yields better codelengths than training on the simplified objective, as expected, but the latter yields the best sample quality. See Fig. 1 for CIFAR10 and CelebA-HQ  $256\\times256$  samples, Fig. 3 and Fig. 4 for LSUN  $256\\times256$  samples [71], and Appendix D for more.",
      "metadata": {
        "chunk_index": 36,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 337
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_37",
      "content": "### <span id=\"page-5-0\"></span>4.2 Reverse process parameterization and training objective ablation\n\nIn Table 2, we show the sample quality effects of reverse process parameterizations and training objectives (Section 3.2). We find that the baseline option of predicting  $\\tilde{\\mu}$  works well only when trained on the true variational bound instead of unweighted mean squared error, a simplified objective akin to Eq. (14). We also see that learning reverse process variances (by incorporating a parameterized diagonal  $\\Sigma_{\\theta}(\\mathbf{x}_t)$  into the variational bound) leads to unstable training and poorer sample quality compared to fixed variances. Predicting  $\\epsilon$ , as we proposed, performs approximately as well as predicting  $\\tilde{\\mu}$  when trained on the variational bound with fixed variances, but much better when trained with our simplified objective.",
      "metadata": {
        "chunk_index": 37,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 889
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_38",
      "content": "### <span id=\"page-5-1\"></span>4.3 Progressive coding\n\nTable 1 also shows the codelengths of our CIFAR10 models. The gap between train and test is at most 0.03 bits per dimension, which is comparable to the gaps reported with other likelihood-based models and indicates that our diffusion model is not overfitting (see Appendix D for nearest neighbor visualizations). Still, while our lossless codelengths are better than the large estimates reported for energy based models and score matching using annealed importance sampling [11], they are not competitive with other types of likelihood-based generative models [7].",
      "metadata": {
        "chunk_index": 38,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 619
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_39",
      "content": "Since our samples are nonetheless of high quality, we conclude that diffusion models have an inductive bias that makes them excellent lossy compressors. Treating the variational bound terms  $L_1 + \\cdots + L_T$  as rate and  $L_0$  as distortion, our CIFAR10 model with the highest quality samples has a rate of **1.78** bits/dim and a distortion of **1.97** bits/dim, which amounts to a root mean squared error of 0.95 on a scale from 0 to 255. More than half of the lossless codelength describes imperceptible distortions.",
      "metadata": {
        "chunk_index": 39,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 525
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_40",
      "content": "**Progressive lossy compression** We can probe further into the rate-distortion behavior of our model by introducing a progressive lossy code that mirrors the form of Eq. (5): see Algorithms 3 and 4, which assume access to a procedure, such as minimal random coding [19, 20], that can transmit a sample  $\\mathbf{x} \\sim q(\\mathbf{x})$  using approximately  $D_{\\mathrm{KL}}(q(\\mathbf{x}) \\parallel p(\\mathbf{x}))$  bits on average for any distributions p and q, for which only p is available to the receiver beforehand. When applied to  $\\mathbf{x}_0 \\sim q(\\mathbf{x}_0)$ , Algorithms 3 and 4 transmit  $\\mathbf{x}_T, \\ldots, \\mathbf{x}_0$  in sequence using a total expected codelength equal to Eq. (5). The receiver,\n\nat any time t, has the partial information  $x_t$  fully available and can progressively estimate:\n\n$$\\mathbf{x}_0 \\approx \\hat{\\mathbf{x}}_0 = \\left(\\mathbf{x}_t - \\sqrt{1 - \\bar{\\alpha}_t} \\boldsymbol{\\epsilon}_{\\theta}(\\mathbf{x}_t)\\right) / \\sqrt{\\bar{\\alpha}_t}$$\n (15)",
      "metadata": {
        "chunk_index": 40,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 996
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_41",
      "content": "$$\\mathbf{x}_0 \\approx \\hat{\\mathbf{x}}_0 = \\left(\\mathbf{x}_t - \\sqrt{1 - \\bar{\\alpha}_t} \\boldsymbol{\\epsilon}_{\\theta}(\\mathbf{x}_t)\\right) / \\sqrt{\\bar{\\alpha}_t}$$\n (15)\n\ndue to Eq. (4). (A stochastic reconstruction  $\\mathbf{x}_0 \\sim p_\\theta(\\mathbf{x}_0|\\mathbf{x}_t)$  is also valid, but we do not consider it here because it makes distortion more difficult to evaluate.) Figure 5 shows the resulting rate-distortion plot on the CIFAR10 test set. At each time t, the distortion is calculated as the root mean squared error  $\\sqrt{\\|\\mathbf{x}_0 - \\hat{\\mathbf{x}}_0\\|^2/D}$ , and the rate is calculated as the cumulative number of bits received so far at time t. The distortion decreases steeply in the low-rate region of the rate-distortion plot, indicating that the majority of the bits are indeed allocated to imperceptible distortions.\n\n<span id=\"page-6-0\"></span>![](_page_6_Figure_3.jpeg)",
      "metadata": {
        "chunk_index": 41,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 905
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_42",
      "content": "<span id=\"page-6-0\"></span>![](_page_6_Figure_3.jpeg)\n\nFigure 5: Unconditional CIFAR10 test set rate-distortion vs. time. Distortion is measured in root mean squared error on a [0, 255] scale. See Table 4 for details.\n\n**Progressive generation** We also run a progressive unconditional generation process given by progressive decompression from random bits. In other words, we predict the result of the reverse process,  $\\hat{\\mathbf{x}}_0$ , while sampling from the reverse process using Algorithm 2. Figures 6 and 10 show the resulting sample quality of  $\\hat{\\mathbf{x}}_0$  over the course of the reverse process. Large scale image features appear first and details appear last. Figure 7 shows stochastic predictions  $\\mathbf{x}_0 \\sim p_{\\theta}(\\mathbf{x}_0|\\mathbf{x}_t)$  with  $\\mathbf{x}_t$  frozen for various t. When t is small, all but fine details are preserved, and when t is large, only large scale features are preserved. Perhaps these are hints of conceptual compression [18].",
      "metadata": {
        "chunk_index": 42,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 997
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_43",
      "content": "<span id=\"page-6-1\"></span>![](_page_6_Figure_6.jpeg)\n\n<span id=\"page-6-2\"></span>Figure 6: Unconditional CIFAR10 progressive generation ( $\\hat{\\mathbf{x}}_0$  over time, from left to right). Extended samples and sample quality metrics over time in the appendix (Figs. 10 and 14).\n\n![](_page_6_Figure_8.jpeg)\n\nFigure 7: When conditioned on the same latent, CelebA-HQ  $256 \\times 256$  samples share high-level attributes. Bottom-right quadrants are  $\\mathbf{x}_t$ , and other quadrants are samples from  $p_{\\theta}(\\mathbf{x}_0|\\mathbf{x}_t)$ .\n\n**Connection to autoregressive decoding** Note that the variational bound (5) can be rewritten as:\n\n$$L = D_{\\mathrm{KL}}(q(\\mathbf{x}_T) \\parallel p(\\mathbf{x}_T)) + \\mathbb{E}_q \\left[ \\sum_{t>1} D_{\\mathrm{KL}}(q(\\mathbf{x}_{t-1}|\\mathbf{x}_t) \\parallel p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)) \\right] + H(\\mathbf{x}_0)$$\n (16)",
      "metadata": {
        "chunk_index": 43,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 883
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_44",
      "content": "(See Appendix A for a derivation.) Now consider setting the diffusion process length T to the dimensionality of the data, defining the forward process so that  $q(\\mathbf{x}_t|\\mathbf{x}_0)$  places all probability mass on  $\\mathbf{x}_0$  with the first t coordinates masked out (i.e.  $q(\\mathbf{x}_t|\\mathbf{x}_{t-1})$  masks out the  $t^{th}$  coordinate), setting  $p(\\mathbf{x}_T)$  to place all mass on a blank image, and, for the sake of argument, taking  $p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)$  to\n\n<span id=\"page-7-0\"></span>![](_page_7_Figure_0.jpeg)\n\nFigure 8: Interpolations of CelebA-HQ 256x256 images with 500 timesteps of diffusion.",
      "metadata": {
        "chunk_index": 44,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 653
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_45",
      "content": "<span id=\"page-7-0\"></span>![](_page_7_Figure_0.jpeg)\n\nFigure 8: Interpolations of CelebA-HQ 256x256 images with 500 timesteps of diffusion.\n\nbe a fully expressive conditional distribution. With these choices,  $D_{\\mathrm{KL}}(q(\\mathbf{x}_T) \\parallel p(\\mathbf{x}_T)) = 0$ , and minimizing  $D_{\\mathrm{KL}}(q(\\mathbf{x}_{t-1}|\\mathbf{x}_t) \\parallel p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t))$  trains  $p_{\\theta}$  to copy coordinates  $t+1,\\ldots,T$  unchanged and to predict the  $t^{\\mathrm{th}}$  coordinate given  $t+1,\\ldots,T$ . Thus, training  $p_{\\theta}$  with this particular diffusion is training an autoregressive model.",
      "metadata": {
        "chunk_index": 45,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 639
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_46",
      "content": "We can therefore interpret the Gaussian diffusion model (2) as a kind of autoregressive model with a generalized bit ordering that cannot be expressed by reordering data coordinates. Prior work has shown that such reorderings introduce inductive biases that have an impact on sample quality [38], so we speculate that the Gaussian diffusion serves a similar purpose, perhaps to greater effect since Gaussian noise might be more natural to add to images compared to masking noise. Moreover, the Gaussian diffusion length is not restricted to equal the data dimension; for instance, we use T=1000, which is less than the dimension of the  $32\\times32\\times3$  or  $256\\times256\\times3$  images in our experiments. Gaussian diffusions can be made shorter for fast sampling or longer for model expressiveness.",
      "metadata": {
        "chunk_index": 46,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 805
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_47",
      "content": "### 4.4 Interpolation",
      "metadata": {
        "chunk_index": 47,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 21
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_48",
      "content": "We can interpolate source images  $\\mathbf{x}_0, \\mathbf{x}_0' \\sim q(\\mathbf{x}_0)$  in latent space using q as a stochastic encoder,  $\\mathbf{x}_t, \\mathbf{x}_t' \\sim q(\\mathbf{x}_t|\\mathbf{x}_0)$ , then decoding the linearly interpolated latent  $\\bar{\\mathbf{x}}_t = (1-\\lambda)\\mathbf{x}_0 + \\lambda\\mathbf{x}_0'$  into image space by the reverse process,  $\\bar{\\mathbf{x}}_0 \\sim p(\\mathbf{x}_0|\\bar{\\mathbf{x}}_t)$ . In effect, we use the reverse process to remove artifacts from linearly interpolating corrupted versions of the source images, as depicted in Fig. 8 (left). We fixed the noise for different values of  $\\lambda$  so  $\\mathbf{x}_t$  and  $\\mathbf{x}_t'$  remain the same. Fig. 8 (right) shows interpolations and reconstructions of original CelebA-HQ  $256 \\times 256$  images (t = 500)",
      "metadata": {
        "chunk_index": 48,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 810
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_49",
      "content": ". Fig. 8 (right) shows interpolations and reconstructions of original CelebA-HQ  $256 \\times 256$  images (t = 500). The reverse process produces high-quality reconstructions, and plausible interpolations that smoothly vary attributes such as pose, skin tone, hairstyle, expression and background, but not eyewear. Larger t results in coarser and more varied interpolations, with novel samples at t = 1000 (Appendix Fig. 9).",
      "metadata": {
        "chunk_index": 49,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 424
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_50",
      "content": "### 5 Related Work",
      "metadata": {
        "chunk_index": 50,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 18
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_51",
      "content": "While diffusion models might resemble flows [9, 46, 10, 32, 5, 16, 23] and VAEs [33, 47, 37], diffusion models are designed so that q has no parameters and the top-level latent  $\\mathbf{x}_T$  has nearly zero mutual information with the data  $\\mathbf{x}_0$ . Our  $\\epsilon$ -prediction reverse process parameterization establishes a connection between diffusion models and denoising score matching over multiple noise levels with annealed Langevin dynamics for sampling [55, 56]. Diffusion models, however, admit straightforward log likelihood evaluation, and the training procedure explicitly trains the Langevin dynamics sampler using variational inference (see Appendix C for details). The connection also has the reverse implication that a certain weighted form of denoising score matching is the same as variational inference to train a Langevin-like sampler",
      "metadata": {
        "chunk_index": 51,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 866
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_52",
      "content": ". The connection also has the reverse implication that a certain weighted form of denoising score matching is the same as variational inference to train a Langevin-like sampler. Other methods for learning transition operators of Markov chains include infusion training [2], variational walkback [15], generative stochastic networks [1], and others [50, 54, 36, 42, 35, 65].",
      "metadata": {
        "chunk_index": 52,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 373
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_53",
      "content": "By the known connection between score matching and energy-based modeling, our work could have implications for other recent work on energy-based models [67–69, 12, 70, 13, 11, 41, 17, 8]. Our rate-distortion curves are computed over time in one evaluation of the variational bound, reminiscent of how rate-distortion curves can be computed over distortion penalties in one run of annealed importance sampling [24]. Our progressive decoding argument can be seen in convolutional DRAW and related models [18, 40] and may also lead to more general designs for subscale orderings or sampling strategies for autoregressive models [38, 64].\n\n# 6 Conclusion",
      "metadata": {
        "chunk_index": 53,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 650
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_54",
      "content": "# 6 Conclusion\n\nWe have presented high quality image samples using diffusion models, and we have found connections among diffusion models and variational inference for training Markov chains, denoising score matching and annealed Langevin dynamics (and energy-based models by extension), autoregressive models, and progressive lossy compression. Since diffusion models seem to have excellent inductive biases for image data, we look forward to investigating their utility in other data modalities and as components in other types of generative models and machine learning systems.\n\n# Broader Impact",
      "metadata": {
        "chunk_index": 54,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 598
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_55",
      "content": "# Broader Impact\n\nOur work on diffusion models takes on a similar scope as existing work on other types of deep generative models, such as efforts to improve the sample quality of GANs, flows, autoregressive models, and so forth. Our paper represents progress in making diffusion models a generally useful tool in this family of techniques, so it may serve to amplify any impacts that generative models have had (and will have) on the broader world.",
      "metadata": {
        "chunk_index": 55,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 449
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_56",
      "content": "Unfortunately, there are numerous well-known malicious uses of generative models. Sample generation techniques can be employed to produce fake images and videos of high profile figures for political purposes. While fake images were manually created long before software tools were available, generative models such as ours make the process easier. Fortunately, CNN-generated images currently have subtle flaws that allow detection [\\[62\\]](#page-11-18), but improvements in generative models may make this more difficult. Generative models also reflect the biases in the datasets on which they are trained. As many large datasets are collected from the internet by automated systems, it can be difficult to remove these biases, especially when the images are unlabeled. If samples from generative models trained on these datasets proliferate throughout the internet, then these biases will only be reinforced further.",
      "metadata": {
        "chunk_index": 56,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 917
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_57",
      "content": "On the other hand, diffusion models may be useful for data compression, which, as data becomes higher resolution and as global internet traffic increases, might be crucial to ensure accessibility of the internet to wide audiences. Our work might contribute to representation learning on unlabeled raw data for a large range of downstream tasks, from image classification to reinforcement learning, and diffusion models might also become viable for creative uses in art, photography, and music.\n\n# Acknowledgments and Disclosure of Funding\n\nThis work was supported by ONR PECASE and the NSF Graduate Research Fellowship under grant number DGE-1752814. Google's TensorFlow Research Cloud (TFRC) provided Cloud TPUs.\n\n# References",
      "metadata": {
        "chunk_index": 57,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 727
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_58",
      "content": "- <span id=\"page-8-5\"></span>[1] Guillaume Alain, Yoshua Bengio, Li Yao, Jason Yosinski, Eric Thibodeau-Laufer, Saizheng Zhang, and Pascal Vincent. GSNs: generative stochastic networks. *Information and Inference: A Journal of the IMA*, 5(2):210–249, 2016.\n- <span id=\"page-8-4\"></span>[2] Florian Bordes, Sina Honari, and Pascal Vincent. Learning to generate samples from noise through infusion training. In *International Conference on Learning Representations*, 2017.\n- <span id=\"page-8-0\"></span>[3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In *International Conference on Learning Representations*, 2019.\n- <span id=\"page-8-2\"></span>[4] Tong Che, Ruixiang Zhang, Jascha Sohl-Dickstein, Hugo Larochelle, Liam Paull, Yuan Cao, and Yoshua Bengio. Your GAN is secretly an energy-based model and you should use discriminator driven latent sampling. *arXiv preprint arXiv:2003.06060*, 2020.",
      "metadata": {
        "chunk_index": 58,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 965
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_59",
      "content": "- <span id=\"page-8-3\"></span>[5] Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. In *Advances in Neural Information Processing Systems*, pages 6571–6583, 2018.\n- <span id=\"page-8-6\"></span>[6] Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. PixelSNAIL: An improved autoregressive generative model. In *International Conference on Machine Learning*, pages 863–871, 2018.\n- <span id=\"page-8-1\"></span>[7] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. *arXiv preprint arXiv:1904.10509*, 2019.",
      "metadata": {
        "chunk_index": 59,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 633
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_60",
      "content": "- <span id=\"page-9-17\"></span>[8] Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc'Aurelio Ranzato. Residual energy-based models for text generation. *arXiv preprint arXiv:2004.11714*, 2020.\n- <span id=\"page-9-11\"></span>[9] Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components estimation. *arXiv preprint arXiv:1410.8516*, 2014.\n- <span id=\"page-9-3\"></span>[10] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. *arXiv preprint arXiv:1605.08803*, 2016.\n- <span id=\"page-9-5\"></span>[11] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In *Advances in Neural Information Processing Systems*, pages 3603–3613, 2019.",
      "metadata": {
        "chunk_index": 60,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 741
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_61",
      "content": "- <span id=\"page-9-5\"></span>[11] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In *Advances in Neural Information Processing Systems*, pages 3603–3613, 2019.\n- <span id=\"page-9-15\"></span>[12] Ruiqi Gao, Yang Lu, Junpei Zhou, Song-Chun Zhu, and Ying Nian Wu. Learning generative ConvNets via multi-grid modeling and sampling. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 9155–9164, 2018.\n- <span id=\"page-9-16\"></span>[13] Ruiqi Gao, Erik Nijkamp, Diederik P Kingma, Zhen Xu, Andrew M Dai, and Ying Nian Wu. Flow contrastive estimation of energy-based models. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 7518–7528, 2020.",
      "metadata": {
        "chunk_index": 61,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 757
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_62",
      "content": "- <span id=\"page-9-0\"></span>[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In *Advances in Neural Information Processing Systems*, pages 2672–2680, 2014.\n- <span id=\"page-9-14\"></span>[15] Anirudh Goyal, Nan Rosemary Ke, Surya Ganguli, and Yoshua Bengio. Variational walkback: Learning a transition operator as a stochastic recurrent net. In *Advances in Neural Information Processing Systems*, pages 4392–4402, 2017.\n- <span id=\"page-9-12\"></span>[16] Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, and David Duvenaud. FFJORD: Free-form continuous dynamics for scalable reversible generative models. In *International Conference on Learning Representations*, 2019.",
      "metadata": {
        "chunk_index": 62,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 788
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_63",
      "content": "- <span id=\"page-9-6\"></span>[17] Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like one. In *International Conference on Learning Representations*, 2020.\n- <span id=\"page-9-7\"></span>[18] Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards conceptual compression. In *Advances In Neural Information Processing Systems*, pages 3549–3557, 2016.\n- <span id=\"page-9-9\"></span>[19] Prahladh Harsha, Rahul Jain, David McAllester, and Jaikumar Radhakrishnan. The communication complexity of correlation. In *Twenty-Second Annual IEEE Conference on Computational Complexity (CCC'07)*, pages 10–23. IEEE, 2007.",
      "metadata": {
        "chunk_index": 63,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 783
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_64",
      "content": "- <span id=\"page-9-10\"></span>[20] Marton Havasi, Robert Peharz, and José Miguel Hernández-Lobato. Minimal random code learning: Getting bits back from compressed model parameters. In *International Conference on Learning Representations*, 2019.\n- <span id=\"page-9-20\"></span>[21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In *Advances in Neural Information Processing Systems*, pages 6626–6637, 2017.\n- <span id=\"page-9-8\"></span>[22] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. In *International Conference on Learning Representations*, 2017.",
      "metadata": {
        "chunk_index": 64,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 849
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_65",
      "content": "- <span id=\"page-9-13\"></span>[23] Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving flow-based generative models with variational dequantization and architecture design. In *International Conference on Machine Learning*, 2019.\n- <span id=\"page-9-18\"></span>[24] Sicong Huang, Alireza Makhzani, Yanshuai Cao, and Roger Grosse. Evaluating lossy compression rates of deep generative models. In *International Conference on Machine Learning*, 2020.\n- <span id=\"page-9-2\"></span>[25] Nal Kalchbrenner, Aaron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex Graves, and Koray Kavukcuoglu. Video pixel networks. In *International Conference on Machine Learning*, pages 1771–1779, 2017.",
      "metadata": {
        "chunk_index": 65,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 732
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_66",
      "content": "- <span id=\"page-9-4\"></span>[26] Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg, Aaron van den Oord, Sander Dieleman, and Koray Kavukcuoglu. Efficient neural audio synthesis. In *International Conference on Machine Learning*, pages 2410–2419, 2018.\n- <span id=\"page-9-1\"></span>[27] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In *International Conference on Learning Representations*, 2018.\n- <span id=\"page-9-19\"></span>[28] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages",
      "metadata": {
        "chunk_index": 66,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 788
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_67",
      "content": "- 4401–4410, 2019.\n- <span id=\"page-10-6\"></span>[29] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. *arXiv preprint arXiv:2006.06676v1*, 2020.\n- <span id=\"page-10-19\"></span>[30] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of StyleGAN. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 8110–8119, 2020.\n- <span id=\"page-10-21\"></span>[31] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In *International Conference on Learning Representations*, 2015.\n- <span id=\"page-10-1\"></span>[32] Diederik P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In *Advances in Neural Information Processing Systems*, pages 10215–10224, 2018.",
      "metadata": {
        "chunk_index": 67,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 928
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_68",
      "content": "- <span id=\"page-10-3\"></span>[33] Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. *arXiv preprint arXiv:1312.6114*, 2013.\n- <span id=\"page-10-5\"></span>[34] Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. In *Advances in Neural Information Processing Systems*, pages 4743–4751, 2016.\n- <span id=\"page-10-16\"></span>[35] John Lawson, George Tucker, Bo Dai, and Rajesh Ranganath. Energy-inspired models: Learning with sampler-induced distributions. In *Advances in Neural Information Processing Systems*, pages 8501–8513, 2019.\n- <span id=\"page-10-14\"></span>[36] Daniel Levy, Matt D. Hoffman, and Jascha Sohl-Dickstein. Generalizing Hamiltonian Monte Carlo with neural networks. In *International Conference on Learning Representations*, 2018.",
      "metadata": {
        "chunk_index": 68,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 873
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_69",
      "content": "- <span id=\"page-10-12\"></span>[37] Lars Maaløe, Marco Fraccaro, Valentin Liévin, and Ole Winther. BIVA: A very deep hierarchy of latent variables for generative modeling. In *Advances in Neural Information Processing Systems*, pages 6548–6558, 2019.\n- <span id=\"page-10-0\"></span>[38] Jacob Menick and Nal Kalchbrenner. Generating high fidelity images with subscale pixel networks and multidimensional upscaling. In *International Conference on Learning Representations*, 2019.\n- <span id=\"page-10-8\"></span>[39] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In *International Conference on Learning Representations*, 2018.\n- <span id=\"page-10-18\"></span>[40] Alex Nichol. VQ-DRAW: A sequential discrete VAE. *arXiv preprint arXiv:2003.01599*, 2020.",
      "metadata": {
        "chunk_index": 69,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 833
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_70",
      "content": "- <span id=\"page-10-18\"></span>[40] Alex Nichol. VQ-DRAW: A sequential discrete VAE. *arXiv preprint arXiv:2003.01599*, 2020.\n- <span id=\"page-10-17\"></span>[41] Erik Nijkamp, Mitch Hill, Tian Han, Song-Chun Zhu, and Ying Nian Wu. On the anatomy of MCMC-based maximum likelihood learning of energy-based models. *arXiv preprint arXiv:1903.12370*, 2019.\n- <span id=\"page-10-15\"></span>[42] Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying Nian Wu. Learning non-convergent non-persistent short-run MCMC toward energy-based model. In *Advances in Neural Information Processing Systems*, pages 5233–5243, 2019.\n- <span id=\"page-10-7\"></span>[43] Georg Ostrovski, Will Dabney, and Remi Munos. Autoregressive quantile networks for generative modeling. In *International Conference on Machine Learning*, pages 3936–3945, 2018.",
      "metadata": {
        "chunk_index": 70,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 820
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_71",
      "content": "- <span id=\"page-10-2\"></span>[44] Ryan Prenger, Rafael Valle, and Bryan Catanzaro. WaveGlow: A flow-based generative network for speech synthesis. In *ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*, pages 3617–3621. IEEE, 2019.\n- <span id=\"page-10-4\"></span>[45] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with VQ-VAE-2. In *Advances in Neural Information Processing Systems*, pages 14837–14847, 2019.\n- <span id=\"page-10-10\"></span>[46] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In *International Conference on Machine Learning*, pages 1530–1538, 2015.\n- <span id=\"page-10-11\"></span>[47] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In *International Conference on Machine Learning*, pages 1278–1286, 2014.",
      "metadata": {
        "chunk_index": 71,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 943
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_72",
      "content": "- <span id=\"page-10-9\"></span>[48] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical image segmentation. In *International Conference on Medical Image Computing and Computer-Assisted Intervention*, pages 234–241. Springer, 2015.\n- <span id=\"page-10-20\"></span>[49] Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In *Advances in Neural Information Processing Systems*, pages 901–909, 2016.\n- <span id=\"page-10-13\"></span>[50] Tim Salimans, Diederik Kingma, and Max Welling. Markov Chain Monte Carlo and variational inference: Bridging the gap. In *International Conference on Machine Learning*, pages 1218–1226, 2015.",
      "metadata": {
        "chunk_index": 72,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 746
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_73",
      "content": "- <span id=\"page-11-20\"></span>[51] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In *Advances in Neural Information Processing Systems*, pages 2234–2242, 2016.\n- <span id=\"page-11-5\"></span>[52] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications. In *International Conference on Learning Representations*, 2017.\n- <span id=\"page-11-3\"></span>[53] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In *International Conference on Machine Learning*, pages 2256–2265, 2015.\n- <span id=\"page-11-12\"></span>[54] Jiaming Song, Shengjia Zhao, and Stefano Ermon. A-NICE-MC: Adversarial training for MCMC. In *Advances in Neural Information Processing Systems*, pages 5140–5150, 2017.",
      "metadata": {
        "chunk_index": 73,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 964
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_74",
      "content": "- <span id=\"page-11-2\"></span>[55] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In *Advances in Neural Information Processing Systems*, pages 11895–11907, 2019.\n- <span id=\"page-11-7\"></span>[56] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. *arXiv preprint arXiv:2006.09011*, 2020.\n- <span id=\"page-11-1\"></span>[57] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. WaveNet: A generative model for raw audio. *arXiv preprint arXiv:1609.03499*, 2016.\n- <span id=\"page-11-0\"></span>[58] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. *International Conference on Machine Learning*, 2016.",
      "metadata": {
        "chunk_index": 74,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 833
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_75",
      "content": "- <span id=\"page-11-0\"></span>[58] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. *International Conference on Machine Learning*, 2016.\n- <span id=\"page-11-6\"></span>[59] Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. Conditional image generation with PixelCNN decoders. In *Advances in Neural Information Processing Systems*, pages 4790–4798, 2016.\n- <span id=\"page-11-9\"></span>[60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In *Advances in Neural Information Processing Systems*, pages 5998–6008, 2017.\n- <span id=\"page-11-4\"></span>[61] Pascal Vincent. A connection between score matching and denoising autoencoders. *Neural Computation*, 23(7):1661–1674, 2011.",
      "metadata": {
        "chunk_index": 75,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 880
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_76",
      "content": "- <span id=\"page-11-4\"></span>[61] Pascal Vincent. A connection between score matching and denoising autoencoders. *Neural Computation*, 23(7):1661–1674, 2011.\n- <span id=\"page-11-18\"></span>[62] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A Efros. Cnn-generated images are surprisingly easy to spot...for now. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, 2020.\n- <span id=\"page-11-10\"></span>[63] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 7794–7803, 2018.\n- <span id=\"page-11-17\"></span>[64] Auke J Wiggers and Emiel Hoogeboom. Predictive sampling with forecasting autoregressive models. *arXiv preprint arXiv:2002.09928*, 2020.\n- <span id=\"page-11-13\"></span>[65] Hao Wu, Jonas Köhler, and Frank Noé. Stochastic normalizing flows. *arXiv preprint arXiv:2002.06707*, 2020.",
      "metadata": {
        "chunk_index": 76,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 972
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_77",
      "content": "- <span id=\"page-11-13\"></span>[65] Hao Wu, Jonas Köhler, and Frank Noé. Stochastic normalizing flows. *arXiv preprint arXiv:2002.06707*, 2020.\n- <span id=\"page-11-8\"></span>[66] Yuxin Wu and Kaiming He. Group normalization. In *Proceedings of the European Conference on Computer Vision (ECCV)*, pages 3–19, 2018.\n- <span id=\"page-11-14\"></span>[67] Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu. A theory of generative convnet. In *International Conference on Machine Learning*, pages 2635–2644, 2016.\n- [68] Jianwen Xie, Song-Chun Zhu, and Ying Nian Wu. Synthesizing dynamic patterns by spatial-temporal generative convnet. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 7093–7101, 2017.",
      "metadata": {
        "chunk_index": 77,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 739
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_78",
      "content": "- <span id=\"page-11-15\"></span>[69] Jianwen Xie, Zilong Zheng, Ruiqi Gao, Wenguan Wang, Song-Chun Zhu, and Ying Nian Wu. Learning descriptor networks for 3d shape synthesis and analysis. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 8629–8638, 2018.\n- <span id=\"page-11-16\"></span>[70] Jianwen Xie, Song-Chun Zhu, and Ying Nian Wu. Learning energy-based spatial-temporal generative convnets for dynamic patterns. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 2019.\n- <span id=\"page-11-11\"></span>[71] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop. *arXiv preprint arXiv:1506.03365*, 2015.\n- <span id=\"page-11-19\"></span>[72] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. *arXiv preprint arXiv:1605.07146*, 2016.",
      "metadata": {
        "chunk_index": 78,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 906
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_79",
      "content": "# Extra information\n\n<span id=\"page-12-2\"></span>LSUN FID scores for LSUN datasets are included in Table [3.](#page-12-2) Scores marked with <sup>∗</sup> are reported by StyleGAN2 as baselines, and other scores are reported by their respective authors.\n\n|  | Table 3: FID scores for LSUN 256 × 256 datasets |  |  |\n|--|-------------------------------------------------|--|--|\n|  |                                                 |  |  |\n\n| Model                 | LSUN Bedroom | LSUN Church | LSUN Cat |\n|-----------------------|--------------|-------------|----------|\n| ProgressiveGAN [27]   | 8.34         | 6.42        | 37.52    |\n| StyleGAN [28]         | 2.65         | 4.21∗       | 8.53∗    |\n| StyleGAN2 [30]        | -            | 3.86        | 6.93     |\n| Ours (Lsimple)        | 6.36         | 7.89        | 19.75    |\n| Ours (Lsimple, large) | 4.90         | -           | -        |",
      "metadata": {
        "chunk_index": 79,
        "content_type": "text",
        "has_table": true,
        "has_figure": false,
        "char_count": 899
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_80",
      "content": "Progressive compression Our lossy compression argument in Section [4.3](#page-5-1) is only a proof of concept, because Algorithms [3](#page-5-3) and [4](#page-5-3) depend on a procedure such as minimal random coding [\\[20\\]](#page-9-10), which is not tractable for high dimensional data. These algorithms serve as a compression interpretation of the variational bound [\\(5\\)](#page-2-2) of Sohl-Dickstein et al. [\\[53\\]](#page-11-3), not yet as a practical compression system.\n\n<span id=\"page-12-1\"></span>Table 4: Unconditional CIFAR10 test set rate-distortion values (accompanies Fig. [5\\)](#page-6-0)",
      "metadata": {
        "chunk_index": 80,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 603
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_81",
      "content": "| Reverse process time (T − t + 1) | Rate (bits/dim) | Distortion (RMSE [0, 255]) |\n|----------------------------------|-----------------|----------------------------|\n| 1000                             | 1.77581         | 0.95136                    |\n| 900                              | 0.11994         | 12.02277                   |\n| 800                              | 0.05415         | 18.47482                   |\n| 700                              | 0.02866         | 24.43656                   |\n| 600                              | 0.01507         | 30.80948                   |\n| 500                              | 0.00716         | 38.03236                   |\n| 400                              | 0.00282         | 46.12765                   |\n| 300                              | 0.00081         | 54.18826                   |\n| 200                              | 0.00013         | 60.97170                   |",
      "metadata": {
        "chunk_index": 81,
        "content_type": "text",
        "has_table": true,
        "has_figure": false,
        "char_count": 923
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_82",
      "content": "| 300                              | 0.00081         | 54.18826                   |\n| 200                              | 0.00013         | 60.97170                   |\n| 100                              | 0.00000         | 67.60125                   |",
      "metadata": {
        "chunk_index": 82,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 251
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_83",
      "content": "# <span id=\"page-12-0\"></span>A Extended derivations\n\nBelow is a derivation of Eq. [\\(5\\)](#page-2-2), the reduced variance variational bound for diffusion models. This material is from Sohl-Dickstein et al. [\\[53\\]](#page-11-3); we include it here only for completeness.\n\n$$L = \\mathbb{E}_q \\left[ -\\log \\frac{p_{\\theta}(\\mathbf{x}_{0:T})}{q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)} \\right]$$\n (17)\n\n$$= \\mathbb{E}_q \\left[ -\\log p(\\mathbf{x}_T) - \\sum_{t \\ge 1} \\log \\frac{p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)}{q(\\mathbf{x}_t|\\mathbf{x}_{t-1})} \\right]$$\n(18)\n\n$$= \\mathbb{E}_q \\left[ -\\log p(\\mathbf{x}_T) - \\sum_{t>1} \\log \\frac{p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)}{q(\\mathbf{x}_t|\\mathbf{x}_{t-1})} - \\log \\frac{p_{\\theta}(\\mathbf{x}_0|\\mathbf{x}_1)}{q(\\mathbf{x}_1|\\mathbf{x}_0)} \\right]$$\n(19)",
      "metadata": {
        "chunk_index": 83,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 802
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_84",
      "content": "$$= \\mathbb{E}_q \\left[ -\\log p(\\mathbf{x}_T) - \\sum_{t>1} \\log \\frac{p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}|\\mathbf{x}_t, \\mathbf{x}_0)} \\cdot \\frac{q(\\mathbf{x}_{t-1}|\\mathbf{x}_0)}{q(\\mathbf{x}_t|\\mathbf{x}_0)} - \\log \\frac{p_{\\theta}(\\mathbf{x}_0|\\mathbf{x}_1)}{q(\\mathbf{x}_1|\\mathbf{x}_0)} \\right]$$\n(20)\n\n$$= \\mathbb{E}_q \\left[ -\\log \\frac{p(\\mathbf{x}_T)}{q(\\mathbf{x}_T | \\mathbf{x}_0)} - \\sum_{t>1} \\log \\frac{p_{\\theta}(\\mathbf{x}_{t-1} | \\mathbf{x}_t)}{q(\\mathbf{x}_{t-1} | \\mathbf{x}_t, \\mathbf{x}_0)} - \\log p_{\\theta}(\\mathbf{x}_0 | \\mathbf{x}_1) \\right]$$\n(21)\n\n$$= \\mathbb{E}_{q} \\left[ D_{\\mathrm{KL}}(q(\\mathbf{x}_{T}|\\mathbf{x}_{0}) \\parallel p(\\mathbf{x}_{T})) + \\sum_{t>1} D_{\\mathrm{KL}}(q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0}) \\parallel p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_{t})) - \\log p_{\\theta}(\\mathbf{x}_{0}|\\mathbf{x}_{1}) \\right]$$\n(22)",
      "metadata": {
        "chunk_index": 84,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 903
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_85",
      "content": "The following is an alternate version of L. It is not tractable to estimate, but it is useful for our discussion in Section 4.3.\n\n$$L = \\mathbb{E}_q \\left[ -\\log p(\\mathbf{x}_T) - \\sum_{t \\ge 1} \\log \\frac{p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)}{q(\\mathbf{x}_t|\\mathbf{x}_{t-1})} \\right]$$\n(23)\n\n$$= \\mathbb{E}_q \\left[ -\\log p(\\mathbf{x}_T) - \\sum_{t>1} \\log \\frac{p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}|\\mathbf{x}_t)} \\cdot \\frac{q(\\mathbf{x}_{t-1})}{q(\\mathbf{x}_t)} \\right]$$\n(24)\n\n$$= \\mathbb{E}_q \\left[ -\\log \\frac{p(\\mathbf{x}_T)}{q(\\mathbf{x}_T)} - \\sum_{t \\ge 1} \\log \\frac{p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)}{q(\\mathbf{x}_{t-1}|\\mathbf{x}_t)} - \\log q(\\mathbf{x}_0) \\right]$$\n(25)\n\n$$= D_{\\mathrm{KL}}(q(\\mathbf{x}_T) \\parallel p(\\mathbf{x}_T)) + \\mathbb{E}_q \\left[ \\sum_{t \\ge 1} D_{\\mathrm{KL}}(q(\\mathbf{x}_{t-1}|\\mathbf{x}_t) \\parallel p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)) \\right] + H(\\mathbf{x}_0)$$\n (26)",
      "metadata": {
        "chunk_index": 85,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 961
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_86",
      "content": "### <span id=\"page-13-0\"></span>**B** Experimental details\n\nOur neural network architecture follows the backbone of PixelCNN++ [52], which is a U-Net [48] based on a Wide ResNet [72]. We replaced weight normalization [49] with group normalization [66] to make the implementation simpler. Our  $32 \\times 32$  models use four feature map resolutions ( $32 \\times 32$  to  $4 \\times 4$ ), and our  $256 \\times 256$  models use six. All models have two convolutional residual blocks per resolution level and self-attention blocks at the  $16 \\times 16$  resolution between the convolutional blocks [6]. Diffusion time t is specified by adding the Transformer sinusoidal position embedding [60] into each residual block. Our CIFAR10 model has 35.7 million parameters, and our LSUN and CelebA-HQ models have 114 million parameters. We also trained a larger variant of the LSUN Bedroom model with approximately 256 million parameters by increasing filter count.",
      "metadata": {
        "chunk_index": 86,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 955
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_87",
      "content": "We used TPU v3-8 (similar to 8 V100 GPUs) for all experiments. Our CIFAR model trains at 21 steps per second at batch size 128 (10.6 hours to train to completion at 800k steps), and sampling a batch of 256 images takes 17 seconds. Our CelebA-HQ/LSUN (256<sup>2</sup>) models train at 2.2 steps per second at batch size 64, and sampling a batch of 128 images takes 300 seconds. We trained on CelebA-HQ for 0.5M steps, LSUN Bedroom for 2.4M steps, LSUN Cat for 1.8M steps, and LSUN Church for 1.2M steps. The larger LSUN Bedroom model was trained for 1.15M steps.\n\nApart from an initial choice of hyperparameters early on to make network size fit within memory constraints, we performed the majority of our hyperparameter search to optimize for CIFAR10 sample quality, then transferred the resulting settings over to the other datasets:",
      "metadata": {
        "chunk_index": 87,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 834
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_88",
      "content": "- We chose the  $\\beta_t$  schedule from a set of constant, linear, and quadratic schedules, all constrained so that  $L_T \\approx 0$ . We set T = 1000 without a sweep, and we chose a linear schedule from  $\\beta_1 = 10^{-4}$  to  $\\beta_T = 0.02$ .\n- We set the dropout rate on CIFAR10 to 0.1 by sweeping over the values {0.1, 0.2, 0.3, 0.4}. Without dropout on CIFAR10, we obtained poorer samples reminiscent of the overfitting artifacts in an unregularized PixelCNN++ [52]. We set dropout rate on the other datasets to zero without sweeping.\n- We used random horizontal flips during training for CIFAR10; we tried training both with and without flips, and found flips to improve sample quality slightly. We also used random horizontal flips for all other datasets except LSUN Bedroom.",
      "metadata": {
        "chunk_index": 88,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 787
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_89",
      "content": "- We tried Adam [31] and RMSProp early on in our experimentation process and chose the former. We left the hyperparameters to their standard values. We set the learning rate to 2 × 10<sup>-4</sup> without any sweeping, and we lowered it to 2 × 10<sup>-5</sup> for the 256 × 256 images, which seemed unstable to train with the larger learning rate.",
      "metadata": {
        "chunk_index": 89,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 347
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_90",
      "content": "- We set the batch size to 128 for CIFAR10 and 64 for larger images. We did not sweep over these values.\n- We used EMA on model parameters with a decay factor of 0.9999. We did not sweep over this value.\n\nFinal experiments were trained once and evaluated throughout training for sample quality. Sample quality scores and log likelihood are reported on the minimum FID value over the course of training. On CIFAR10, we calculated Inception and FID scores on 50000 samples using the original code from the OpenAI [51] and TTUR [21] repositories, respectively. On LSUN, we calculated FID scores on 50000 samples using code from the StyleGAN2 [30] repository. CIFAR10 and CelebA-HQ were loaded as provided by TensorFlow Datasets (https://www.tensorflow.org/datasets), and LSUN was prepared using code from StyleGAN. Dataset splits (or lack thereof) are standard from the papers that introduced their usage in a generative modeling context. All details can be found in the source code release.",
      "metadata": {
        "chunk_index": 90,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 988
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_91",
      "content": "### <span id=\"page-14-1\"></span>C Discussion on related work\n\nOur model architecture, forward process definition, and prior differ from NCSN [55, 56] in subtle but important ways that improve sample quality, and, notably, we directly train our sampler as a latent variable model rather than adding it after training post-hoc. In greater detail:",
      "metadata": {
        "chunk_index": 91,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 344
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_92",
      "content": "- 1. We use a U-Net with self-attention; NCSN uses a RefineNet with dilated convolutions. We condition all layers on t by adding in the Transformer sinusoidal position embedding, rather than only in normalization layers (NCSNv1) or only at the output (v2).\n- 2. Diffusion models scale down the data with each forward process step (by a  $\\sqrt{1-\\beta_t}$  factor) so that variance does not grow when adding noise, thus providing consistently scaled inputs to the neural net reverse process. NCSN omits this scaling factor.\n- 3. Unlike NCSN, our forward process destroys signal  $(D_{KL}(q(\\mathbf{x}_T|\\mathbf{x}_0) \\parallel \\mathcal{N}(\\mathbf{0}, \\mathbf{I})) \\approx 0)$ , ensuring a close match between the prior and aggregate posterior of  $\\mathbf{x}_T$ . Also unlike NCSN, our  $\\beta_t$  are very small, which ensures that the forward process is reversible by a Markov chain with conditional Gaussians. Both of these factors prevent distribution shift when sampling.",
      "metadata": {
        "chunk_index": 92,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 976
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_93",
      "content": "- 4. Our Langevin-like sampler has coefficients (learning rate, noise scale, etc.) derived rigorously from  $\\beta_t$  in the forward process. Thus, our training procedure directly trains our sampler to match the data distribution after T steps: it trains the sampler as a latent variable model using variational inference. In contrast, NCSN's sampler coefficients are set by hand post-hoc, and their training procedure is not guaranteed to directly optimize a quality metric of their sampler.",
      "metadata": {
        "chunk_index": 93,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 493
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_94",
      "content": "### <span id=\"page-14-0\"></span>**D** Samples\n\n**Additional samples** Figure 11, 13, 16, 17, 18, and 19 show uncurated samples from the diffusion models trained on CelebA-HQ, CIFAR10 and LSUN datasets.",
      "metadata": {
        "chunk_index": 94,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 201
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_95",
      "content": "Latent structure and reverse process stochasticity During sampling, both the prior  $\\mathbf{x}_T \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I})$  and Langevin dynamics are stochastic. To understand the significance of the second source of noise, we sampled multiple images conditioned on the same intermediate latent for the CelebA  $256 \\times 256$  dataset. Figure 7 shows multiple draws from the reverse process  $\\mathbf{x}_0 \\sim p_{\\theta}(\\mathbf{x}_0|\\mathbf{x}_t)$  that share the latent  $\\mathbf{x}_t$  for  $t \\in \\{1000, 750, 500, 250\\}$ . To accomplish this, we run a single reverse chain from an initial draw from the prior. At the intermediate timesteps, the chain is split to sample multiple images. When the chain is split after the prior draw at  $\\mathbf{x}_{T=1000}$ , the samples differ significantly. However, when the chain is split after more steps, samples share high-level attributes like gender, hair color, eyewear, saturation, pose and facial expression",
      "metadata": {
        "chunk_index": 95,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 977
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_96",
      "content": ". However, when the chain is split after more steps, samples share high-level attributes like gender, hair color, eyewear, saturation, pose and facial expression. This indicates that intermediate latents like  $\\mathbf{x}_{750}$  encode these attributes, despite their imperceptibility.",
      "metadata": {
        "chunk_index": 96,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 286
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_97",
      "content": "Coarse-to-fine interpolation Figure 9 shows interpolations between a pair of source CelebA  $256 \\times 256$  images as we vary the number of diffusion steps prior to latent space interpolation. Increasing the number of diffusion steps destroys more structure in the source images, which the\n\nmodel completes during the reverse process. This allows us to interpolate at both fine granularities and coarse granularities. In the limiting case of 0 diffusion steps, the interpolation mixes source images in pixel space. On the other hand, after 1000 diffusion steps, source information is lost and interpolations are novel samples.\n\n<span id=\"page-15-1\"></span>![](_page_15_Figure_1.jpeg)\n\nFigure 9: Coarse-to-fine interpolations that vary the number of diffusion steps prior to latent mixing.\n\n<span id=\"page-15-0\"></span>![](_page_15_Figure_3.jpeg)\n\nFigure 10: Unconditional CIFAR10 progressive sampling quality over time\n\n<span id=\"page-16-0\"></span>![](_page_16_Picture_0.jpeg)",
      "metadata": {
        "chunk_index": 97,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 978
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_98",
      "content": "<span id=\"page-15-0\"></span>![](_page_15_Figure_3.jpeg)\n\nFigure 10: Unconditional CIFAR10 progressive sampling quality over time\n\n<span id=\"page-16-0\"></span>![](_page_16_Picture_0.jpeg)\n\nFigure 11: CelebA-HQ 256 × 256 generated samples\n\n![](_page_17_Figure_0.jpeg)\n\n(b) Inception feature space nearest neighbors\n\nFigure 12: CelebA-HQ 256 × 256 nearest neighbors, computed on a 100 × 100 crop surrounding the faces. Generated samples are in the leftmost column, and training set nearest neighbors are in the remaining columns.\n\n<span id=\"page-18-0\"></span>![](_page_18_Figure_0.jpeg)\n\nFigure 13: Unconditional CIFAR10 generated samples\n\n<span id=\"page-19-0\"></span>![](_page_19_Figure_0.jpeg)\n\nFigure 14: Unconditional CIFAR10 progressive generation\n\n![](_page_20_Figure_0.jpeg)\n\n(b) Inception feature space nearest neighbors\n\nFigure 15: Unconditional CIFAR10 nearest neighbors. Generated samples are in the leftmost column, and training set nearest neighbors are in the remaining columns.",
      "metadata": {
        "chunk_index": 98,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 989
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_chunk_99",
      "content": "Figure 15: Unconditional CIFAR10 nearest neighbors. Generated samples are in the leftmost column, and training set nearest neighbors are in the remaining columns.\n\n<span id=\"page-21-0\"></span>![](_page_21_Picture_0.jpeg)\n\nFigure 16: LSUN Church generated samples. FID=7.89\n\n<span id=\"page-22-0\"></span>![](_page_22_Picture_0.jpeg)\n\nFigure 17: LSUN Bedroom generated samples, large model. FID=4.90\n\n<span id=\"page-23-0\"></span>![](_page_23_Picture_0.jpeg)\n\nFigure 18: LSUN Bedroom generated samples, small model. FID=6.36\n\n<span id=\"page-24-0\"></span>![](_page_24_Picture_0.jpeg)\n\nFigure 19: LSUN Cat generated samples. FID=19.75",
      "metadata": {
        "chunk_index": 99,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 628
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_table_1",
      "content": "| Model                                                                     | IS                                                               | FID                                  | NLL Test (Train)                         | —<br>————————————————————————————————————                                                                                         | 1 0751 540                   |                           |\n|---------------------------------------------------------------------------|------------------------------------------------------------------|--------------------------------------|------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|------------------------------|---------------------------|\n| Conditional                                                               |                                                                  |                                      |                                          | -Table 2: Unconditio<br>_process parameterizati                                                                                   |                              |                           |\n| EBM [11]<br>JEM [17]<br>BigGAN [3]<br>StyleGAN2 + ADA (v1) [29]           | 8.30<br>8.76<br>9.22<br><b>10.06</b>                             | 37.9<br>38.4<br>14.73<br><b>2.67</b> |                                          | tive ablation. Blank er<br>train and generated por<br>range scores.                                                               | ntries were uns              | stable to                 |\n| Unconditional                                                             |                                                                  |                                      |                                          | Objective                                                                                                                         | IS                           | FID                       |\n| Diffusion (original) [53]                                                 |                                                                  |                                      | < 5.40                                   | $\tilde{\\mu}$ prediction (baseline)                                                                                                |                              |                           |\n| Gated PixelCNN [59]<br>Sparse Transformer [7]<br>PixelIQN [43]            | 4.60<br>5.29                                                     | 65.93<br>49.46                       | 3.03 (2.90)<br><b>2.80</b>               | $L$ , learned diagonal $\\Sigma$ $L$ , fixed isotropic $\\Sigma$ $\\ \\tilde{\\mu} - \\tilde{\\mu}_{\\theta}\\ ^2$                         | $7.28\\pm0.10$ $8.06\\pm0.09$  | 23.69<br>13.22<br>-       |\n| EBM [11]<br>NCSNv2 [56]                                                   | 6.78                                                             | $38.2 \\\\ 31.75$                      |                                          | $\\epsilon$ prediction (ours)                                                                                                      |                              |                           |\n| NCSN [55]<br>SNGAN [39]<br>SNGAN-DDLS [4]<br>StyleGAN2 + ADA (v1) [29]    | $8.87\\pm0.12$<br>$8.22\\pm0.05$<br>$9.09\\pm0.10$<br>$9.74\\pm0.05$ | 25.32<br>21.7<br>15.42<br>3.26       |                                          | $L$ , learned diagonal $\\Sigma$ $L$ , fixed isotropic $\\Sigma$ $\\ \\tilde{\\epsilon} - \\epsilon_{\\theta}\\ ^2 (L_{\\mathrm{simple}})$ | $-7.67\\pm0.13$ $9.46\\pm0.11$ | -<br>13.51<br><b>3.17</b> |\n| Ours $(L, \\text{ fixed isotropic } \\Sigma)$<br>Ours $(L_{\\text{simple}})$ | $7.67 \\pm 0.13$<br>$9.46 \\pm 0.11$                               | 13.51<br><b>3.17</b>                 | $\\leq 3.70 (3.69)$<br>$\\leq 3.75 (3.72)$ |                                                                                                                                   |                              |                           |\n",
      "metadata": {
        "content_type": "table",
        "table_id": "table_1",
        "headers": [
          "Model",
          "IS",
          "FID",
          "NLL Test (Train)",
          "—<br>————————————————————————————————————",
          "1 0751 540",
          ""
        ],
        "row_count": 8
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_table_2",
      "content": "| Algorithm 3 Sending $x_0$                                                                                                    | Algorithm 4 Receiving                                                         |\n|------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------|\n| 1: Send $\\mathbf{x}_T \\sim q(\\mathbf{x}_T \\mathbf{x}_0)$ using $p(\\mathbf{x}_T)$                                             | 1: Receive $\\mathbf{x}_T$ using $p(\\mathbf{x}_T)$                             |\n| 2: <b>for</b> $t = T - 1, \\dots, 2, 1$ <b>do</b>                                                                             | 2: for $t = T - 1, \\dots, 1, 0$ do                                            |\n| 3: Send $\\mathbf{x}_t \\sim q(\\mathbf{x}_t \\mathbf{x}_{t+1}, \\mathbf{x}_0)$ using $p_{\\theta}(\\mathbf{x}_t \\mathbf{x}_{t+1})$ | 3: Receive $\\mathbf{x}_t$ using $p_{\\theta}(\\mathbf{x}_t   \\mathbf{x}_{t+1})$ |\n| 4: <b>end for</b>                                                                                                            | 4: end for                                                                    |\n| 5: Send $\\mathbf{x}_0$ using $p_{\\theta}(\\mathbf{x}_0 \\mathbf{x}_1)$                                                         | 5: return $\\mathbf{x}_0$                                                      |\n",
      "metadata": {
        "content_type": "table",
        "table_id": "table_2",
        "headers": [
          "Algorithm 3 Sending $x_0$",
          "Algorithm 4 Receiving"
        ],
        "row_count": 5
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_table_3",
      "content": "|  | Table 3: FID scores for LSUN 256 × 256 datasets |  |  |\n|--|-------------------------------------------------|--|--|\n|  |                                                 |  |  |\n",
      "metadata": {
        "content_type": "table",
        "table_id": "table_3",
        "headers": [
          "",
          "Table 3: FID scores for LSUN 256 × 256 datasets",
          "",
          ""
        ],
        "row_count": 1
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_table_4",
      "content": "| Model                 | LSUN Bedroom | LSUN Church | LSUN Cat |\n|-----------------------|--------------|-------------|----------|\n| ProgressiveGAN [27]   | 8.34         | 6.42        | 37.52    |\n| StyleGAN [28]         | 2.65         | 4.21∗       | 8.53∗    |\n| StyleGAN2 [30]        | -            | 3.86        | 6.93     |\n| Ours (Lsimple)        | 6.36         | 7.89        | 19.75    |\n| Ours (Lsimple, large) | 4.90         | -           | -        |\n",
      "metadata": {
        "content_type": "table",
        "table_id": "table_4",
        "headers": [
          "Model",
          "LSUN Bedroom",
          "LSUN Church",
          "LSUN Cat"
        ],
        "row_count": 5
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_table_5",
      "content": "| Reverse process time (T − t + 1) | Rate (bits/dim) | Distortion (RMSE [0, 255]) |\n|----------------------------------|-----------------|----------------------------|\n| 1000                             | 1.77581         | 0.95136                    |\n| 900                              | 0.11994         | 12.02277                   |\n| 800                              | 0.05415         | 18.47482                   |\n| 700                              | 0.02866         | 24.43656                   |\n| 600                              | 0.01507         | 30.80948                   |\n| 500                              | 0.00716         | 38.03236                   |\n| 400                              | 0.00282         | 46.12765                   |\n| 300                              | 0.00081         | 54.18826                   |\n| 200                              | 0.00013         | 60.97170                   |\n| 100                              | 0.00000         | 67.60125                   |\n",
      "metadata": {
        "content_type": "table",
        "table_id": "table_5",
        "headers": [
          "Reverse process time (T − t + 1)",
          "Rate (bits/dim)",
          "Distortion (RMSE [0, 255])"
        ],
        "row_count": 10
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_figure_1",
      "content": "<span id=\"page-0-0\"></span>\n\nFigure: Generated samples on CelebA-HQ 256 × 256 (left) and unconditional CIFAR10 (right)\n\nVisual Description: The academic figure presents generated samples from two distinct datasets: CelebA-HQ (left) and CIFAR10 (right). The left side showcases high-quality human face images, while the right displays a variety of animal and object images typical of CIFAR10, indicating the model's capability in generating diverse outputs. The key finding appears to be the contrasting quality and type of generated samples, highlighting the model's performance across different domains in image generation.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_1",
        "caption": "Generated samples on CelebA-HQ 256 × 256 (left) and unconditional CIFAR10 (right)",
        "image_key": "_page_0_Picture_9.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_figure_2",
      "content": "Figure: The directed graphical model considered in this work.\n\nVisual Description: The directed graphical model shown in the figure consists of a sequence of latent variables (denoted as \\(X_T\\) to \\(X_0\\)) representing a generative process transitioning from noise to a final image. The arrows indicate the relationships between consecutive latent variables, with \\(p_\\theta(X_{t-1}|X_t)\\) representing the forward process and \\(q(X_t|X_{t-1})\\) the reverse process. Key findings suggest that this model effectively captures the evolution of data through latent variables, illustrating how initial noise can be transformed into coherent images over time.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_2",
        "caption": "The directed graphical model considered in this work.",
        "image_key": "_page_1_Figure_0.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_figure_3",
      "content": "<span id=\"page-5-2\"></span>\n\nFigure: ![](_page_5_Picture_1.jpeg)\n\nVisual Description: I'm unable to view the image directly, but I can help you outline how to describe an academic figure based on what it typically includes.\n\nIf the figure is a collage of buildings, you might want to structure your analysis like this:\n\nThe figure displays a collage of historical and architectural structures, illustrating various architectural styles across different regions. The x-axis could represent different architectural styles, while the y-axis may indicate frequency or significance of these styles across specific cultures. Key trends may include a predominance of religious architecture, with notable findings highlighting the diversity in design and construction methods, reflecting cultural influences and historical context.\n\nFeel free to replace the details with specifics from your image!",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_3",
        "caption": "![](_page_5_Picture_1.jpeg)",
        "image_key": "_page_5_Picture_0.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_figure_4",
      "content": "<span id=\"page-6-0\"></span>\n\nFigure: Unconditional CIFAR10 test set rate-distortion vs. time. Distortion is measured in root mean squared error on a [0, 255] scale. See Table 4 for details.\n\nVisual Description: The figure depicts three plots illustrating the relationship between rate-distortion and reverse process steps in an unconditional CIFAR10 test set. The leftmost plot shows that as the number of reverse process steps increases, the distortion (measured in RMSE) decreases from approximately 80 to near 0. The middle plot reveals a sharp increase in the rate (in bits/dim) as reverse process steps approach 1000, while the rightmost plot indicates that higher rates correspond to lower distortion, highlighting the trade-off between the two metrics.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_4",
        "caption": "Unconditional CIFAR10 test set rate-distortion vs. time. Distortion is measured in root mean squared error on a [0, 255] scale. See Table 4 for details.",
        "image_key": "_page_6_Figure_3.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_figure_5",
      "content": "<span id=\"page-6-1\"></span>\n\nFigure: <span id=\"page-6-2\"></span>Figure 6: Unconditional CIFAR10 progressive generation ( $\\hat{\\mathbf{x}}_0$  over time, from left to right). Extended samples and sample quality metrics over time in the appendix (Figs. 10 and 14).\n\nVisual Description: Figure 6 presents the progressive generation of images from the CIFAR10 dataset, showing a sequence of generations ranging from abstract noise to clearer images over time, displayed from left to right. The first row illustrates the initial chaotic representations, which gradually evolve into identifiable images of various objects, such as planes and birds, in subsequent rows. Key findings indicate that with each progressive step, the clarity and quality of the generated samples significantly improve, showcasing the model's ability to refine its outputs over time.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_5",
        "caption": "<span id=\"page-6-2\"></span>Figure 6: Unconditional CIFAR10 progressive generation ( $\\hat{\\mathbf{x}}_0$  over time, from left to right). Extended samples and sample quality metrics over time in the appendix (Figs. 10 and 14).",
        "image_key": "_page_6_Figure_6.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_figure_6",
      "content": "Figure: When conditioned on the same latent, CelebA-HQ  $256 \\times 256$  samples share high-level attributes. Bottom-right quadrants are  $\\mathbf{x}_t$ , and other quadrants are samples from  $p_{\\theta}(\\mathbf{x}_0|\\mathbf{x}_t)$ .\n\nVisual Description: The figure presents a grid of images that illustrate how conditioned latent variables affect high-level attributes in CelebA-HQ samples at different resolutions. The horizontal axes are labeled with latent variable representations: \\( \\mathbf{x}_{1000}, \\mathbf{x}_{750}, \\mathbf{x}_{500}, \\mathbf{x}_{250}, \\) and \\( \\mathbf{x}_{0} \\). The top row displays clearer, more coherent images, while the bottom row contains more disordered samples, particularly in the leftmost quadrants that share the latent representation \\( \\mathbf{x}_{1000} \\), suggesting that higher-resolution latent representations retain more distinct features.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_6",
        "caption": "When conditioned on the same latent, CelebA-HQ  $256 \\times 256$  samples share high-level attributes. Bottom-right quadrants are  $\\mathbf{x}_t$ , and other quadrants are samples from  $p_{\\theta}(\\mathbf{x}_0|\\mathbf{x}_t)$ .",
        "image_key": "_page_6_Figure_8.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_figure_7",
      "content": "<span id=\"page-7-0\"></span>\n\nFigure: Interpolations of CelebA-HQ 256x256 images with 500 timesteps of diffusion.\n\nVisual Description: The figure illustrates interpolations of CelebA-HQ images using a diffusion model across various timesteps, denoted by the parameter \\(\\lambda\\) (ranging from 0.1 to 0.9). The left panel depicts the process of pixel-space interpolation and denoised interpolation, with a focus on achieving a smooth transition along the image manifold. The right side displays the resulting images for different \\(\\lambda\\) values, highlighting how variations in this parameter affect the reconstructed images and maintain the features of the original source images.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_7",
        "caption": "Interpolations of CelebA-HQ 256x256 images with 500 timesteps of diffusion.",
        "image_key": "_page_7_Figure_0.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_figure_8",
      "content": "<span id=\"page-15-1\"></span>\n\nFigure: Coarse-to-fine interpolations that vary the number of diffusion steps prior to latent mixing.\n\nVisual Description: The figure displays a grid of interpolated images generated through a diffusion model, with the vertical axis indicating the number of diffusion steps (ranging from 0 to 1000) and the horizontal axis denoting different values of the mixing parameter \\( \\lambda \\) (from 0.1 to 0.9). Key trends observed include a gradual improvement in image quality and detail as the number of diffusion steps increases, particularly for higher \\( \\lambda \\) values, indicating greater control over the blending process. The findings suggest that both the number of steps and the choice of \\( \\lambda \\) significantly impact the fidelity and appearance of the generated images.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_8",
        "caption": "Coarse-to-fine interpolations that vary the number of diffusion steps prior to latent mixing.",
        "image_key": "_page_15_Figure_1.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_figure_9",
      "content": "<span id=\"page-15-0\"></span>\n\nFigure: Unconditional CIFAR10 progressive sampling quality over time\n\nVisual Description: The figure presents two plots illustrating the quality of unconditional CIFAR10 progressive sampling as a function of reverse process steps (T - t). The left graph shows the Inception Score, which increases steadily from approximately 2 to 10 as the number of sampling steps increases, indicating improved sample quality. Conversely, the right graph displays the Fréchet Inception Distance (FID), which decreases from around 300 to nearly 0, suggesting a reduction in the distance between generated and real data distributions, thereby indicating enhanced sampling fidelity over time.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_9",
        "caption": "Unconditional CIFAR10 progressive sampling quality over time",
        "image_key": "_page_15_Figure_3.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_figure_10",
      "content": "<span id=\"page-16-0\"></span>\n\nFigure: CelebA-HQ 256 × 256 generated samples\n\nVisual Description: The figure presents a grid of generated facial images from the CelebA-HQ dataset at a resolution of 256 × 256 pixels. Each row and column showcases diverse facial characteristics, illustrating the model's ability to create realistic and varied representations of individuals. Key findings include an apparent quality in facial detail and diversity, with an emphasis on capturing a range of expressions, ethnicities, and hairstyles, demonstrating the effectiveness of the generative model in producing high-quality synthetic images.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_10",
        "caption": "CelebA-HQ 256 × 256 generated samples",
        "image_key": "_page_16_Picture_0.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_figure_11",
      "content": "Figure: (b) Inception feature space nearest neighbors\n\nVisual Description: The figure presents a grid of facial images organized into two sections: the top section labeled \"(a) Pixel space nearest neighbors\" and the bottom section labeled \"(b) Inception feature space nearest neighbors.\" While the axes are not explicitly shown, the layout suggests an analysis of facial similarity based on pixel values compared to a more abstract feature representation. \n\nKey findings likely involve contrasting the relationships among faces based on raw pixel data versus features derived from the Inception model, highlighting differences in clustering and similarity perceptions. The data trend indicates that the Inception feature space may capture more nuanced similarities among faces, as represented by relationships in the bottom grid.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_11",
        "caption": "(b) Inception feature space nearest neighbors",
        "image_key": "_page_17_Figure_0.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_figure_12",
      "content": "<span id=\"page-18-0\"></span>\n\nFigure: Unconditional CIFAR10 generated samples\n\nVisual Description: The figure presents a grid of 1000 generated images from the CIFAR10 dataset, showcasing diverse categories such as animals, vehicles, and natural scenes. While there are no explicit axes in the traditional sense, the layout visually represents the variety of samples generated without direct labels, highlighting effective variations in appearance and style across categories. Key findings from this visual representation suggest that the generative model successfully captures the complexity of the CIFAR10 dataset, producing high-quality images that resemble their original dataset counterparts.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_12",
        "caption": "Unconditional CIFAR10 generated samples",
        "image_key": "_page_18_Figure_0.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_figure_13",
      "content": "<span id=\"page-19-0\"></span>\n\nFigure: Unconditional CIFAR10 progressive generation\n\nVisual Description: The figure presents a grid showcasing the progressive generation of images from the CIFAR10 dataset. The vertical axis likely indicates the stages of generation, while the horizontal axis represents different classes of objects in the dataset. A notable trend is the increasing clarity and detail in the images as one moves down the grid, highlighting the effectiveness of the unconditional generation process in progressively refining the generated outputs. Key findings suggest improvements in image quality and fidelity to real-world classes over successive generation steps.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_13",
        "caption": "Unconditional CIFAR10 progressive generation",
        "image_key": "_page_19_Figure_0.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_figure_14",
      "content": "Figure: (b) Inception feature space nearest neighbors\n\nVisual Description: The academic figure showcases a collection of images organized in a grid format, representing the nearest neighbors in the Inception feature space. The x and y axes are not explicitly labeled, but each cell in the grid displays images that are likely to share similar features according to the Inception model. Key findings from this display include visual clusters of related images, indicating how the model groups similar objects together based on feature representations, highlighting the model’s effectiveness in distinguishing various categories such as animals and landscapes.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_14",
        "caption": "(b) Inception feature space nearest neighbors",
        "image_key": "_page_20_Figure_0.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_figure_15",
      "content": "<span id=\"page-21-0\"></span>\n\nFigure: LSUN Church generated samples. FID=7.89\n\nVisual Description: The figure presents a collage of images depicting various church structures generated by the LSUN model. The data exhibits a variety of architectural styles and characteristics, demonstrating the model's effectiveness in producing diverse and realistic representations of churches, as indicated by the Fréchet Inception Distance (FID) score of 7.89, suggesting high-quality and varied sample generation. The lack of explicit axis labels suggests a focus on visual data exploration rather than quantitative analysis.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_15",
        "caption": "LSUN Church generated samples. FID=7.89",
        "image_key": "_page_21_Picture_0.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_figure_16",
      "content": "<span id=\"page-22-0\"></span>\n\nFigure: LSUN Bedroom generated samples, large model. FID=4.90\n\nVisual Description: The figure showcases a grid of generated bedroom samples, illustrating the output quality of a large model trained on the LSUN Bedroom dataset. Although specific axes are not labeled in the image, the visual diversity of styles and layouts indicates a broad range of design elements captured by the model. A key finding highlighted by the Fréchet Inception Distance (FID) score of 4.90 suggests that the generated images are of high quality, closely resembling real bedroom photographs, which reflects the model's effectiveness in generating realistic samples.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_16",
        "caption": "LSUN Bedroom generated samples, large model. FID=4.90",
        "image_key": "_page_22_Picture_0.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_figure_17",
      "content": "<span id=\"page-23-0\"></span>\n\nFigure: LSUN Bedroom generated samples, small model. FID=6.36\n\nVisual Description: The figure presents a collage of LSUN Bedroom generated samples from a small model, illustrating the model's ability to synthesize diverse bedroom environments. While specific axes are not provided in the image, the data trends suggest a high quality of generated images, as indicated by the low Frechet Inception Distance (FID) score of 6.36, which measures the distance between the generated samples and real images. Key findings include the model's capacity to produce realistic and varied interior designs, demonstrating effective learning from the training data.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_17",
        "caption": "LSUN Bedroom generated samples, small model. FID=6.36",
        "image_key": "_page_23_Picture_0.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Denoising Diffusion Probabilistic Models.pdf_figure_18",
      "content": "<span id=\"page-24-0\"></span>\n\nFigure: LSUN Cat generated samples. FID=19.75\n\nVisual Description: The academic figure displays a grid of generated cat images, showcasing a diverse array of feline appearances. While specific axes are not applicable here, the notable data trend indicates the capacity of the LSUN Cat model to produce high-quality cat images with a Fréchet Inception Distance (FID) score of 19.75, suggesting relatively good fidelity and variety in the generated samples. Key findings highlight the effectiveness of the model in capturing various cat breeds and expressions, reflecting its robustness in image synthesis tasks.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_18",
        "caption": "LSUN Cat generated samples. FID=19.75",
        "image_key": "_page_24_Picture_0.jpeg",
        "has_vision_description": true
      }
    }
  ],
  "tables": [
    {
      "table_id": "table_1",
      "headers": [
        "Model",
        "IS",
        "FID",
        "NLL Test (Train)",
        "—<br>————————————————————————————————————",
        "1 0751 540",
        ""
      ],
      "rows": [
        [
          "Conditional",
          "",
          "",
          "",
          "-Table 2: Unconditio<br>_process parameterizati",
          "",
          ""
        ],
        [
          "EBM [11]<br>JEM [17]<br>BigGAN [3]<br>StyleGAN2 + ADA (v1) [29]",
          "8.30<br>8.76<br>9.22<br><b>10.06</b>",
          "37.9<br>38.4<br>14.73<br><b>2.67</b>",
          "",
          "tive ablation. Blank er<br>train and generated por<br>range scores.",
          "ntries were uns",
          "stable to"
        ],
        [
          "Unconditional",
          "",
          "",
          "",
          "Objective",
          "IS",
          "FID"
        ],
        [
          "Diffusion (original) [53]",
          "",
          "",
          "< 5.40",
          "$\tilde{\\mu}$ prediction (baseline)",
          "",
          ""
        ],
        [
          "Gated PixelCNN [59]<br>Sparse Transformer [7]<br>PixelIQN [43]",
          "4.60<br>5.29",
          "65.93<br>49.46",
          "3.03 (2.90)<br><b>2.80</b>",
          "$L$ , learned diagonal $\\Sigma$ $L$ , fixed isotropic $\\Sigma$ $\\ \\tilde{\\mu} - \\tilde{\\mu}_{\\theta}\\ ^2$",
          "$7.28\\pm0.10$ $8.06\\pm0.09$",
          "23.69<br>13.22<br>-"
        ],
        [
          "EBM [11]<br>NCSNv2 [56]",
          "6.78",
          "$38.2 \\\\ 31.75$",
          "",
          "$\\epsilon$ prediction (ours)",
          "",
          ""
        ],
        [
          "NCSN [55]<br>SNGAN [39]<br>SNGAN-DDLS [4]<br>StyleGAN2 + ADA (v1) [29]",
          "$8.87\\pm0.12$<br>$8.22\\pm0.05$<br>$9.09\\pm0.10$<br>$9.74\\pm0.05$",
          "25.32<br>21.7<br>15.42<br>3.26",
          "",
          "$L$ , learned diagonal $\\Sigma$ $L$ , fixed isotropic $\\Sigma$ $\\ \\tilde{\\epsilon} - \\epsilon_{\\theta}\\ ^2 (L_{\\mathrm{simple}})$",
          "$-7.67\\pm0.13$ $9.46\\pm0.11$",
          "-<br>13.51<br><b>3.17</b>"
        ],
        [
          "Ours $(L, \\text{ fixed isotropic } \\Sigma)$<br>Ours $(L_{\\text{simple}})$",
          "$7.67 \\pm 0.13$<br>$9.46 \\pm 0.11$",
          "13.51<br><b>3.17</b>",
          "$\\leq 3.70 (3.69)$<br>$\\leq 3.75 (3.72)$",
          "",
          "",
          ""
        ]
      ],
      "markdown": "| Model                                                                     | IS                                                               | FID                                  | NLL Test (Train)                         | —<br>————————————————————————————————————                                                                                         | 1 0751 540                   |                           |\n|---------------------------------------------------------------------------|------------------------------------------------------------------|--------------------------------------|------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|------------------------------|---------------------------|\n| Conditional                                                               |                                                                  |                                      |                                          | -Table 2: Unconditio<br>_process parameterizati                                                                                   |                              |                           |\n| EBM [11]<br>JEM [17]<br>BigGAN [3]<br>StyleGAN2 + ADA (v1) [29]           | 8.30<br>8.76<br>9.22<br><b>10.06</b>                             | 37.9<br>38.4<br>14.73<br><b>2.67</b> |                                          | tive ablation. Blank er<br>train and generated por<br>range scores.                                                               | ntries were uns              | stable to                 |\n| Unconditional                                                             |                                                                  |                                      |                                          | Objective                                                                                                                         | IS                           | FID                       |\n| Diffusion (original) [53]                                                 |                                                                  |                                      | < 5.40                                   | $\tilde{\\mu}$ prediction (baseline)                                                                                                |                              |                           |\n| Gated PixelCNN [59]<br>Sparse Transformer [7]<br>PixelIQN [43]            | 4.60<br>5.29                                                     | 65.93<br>49.46                       | 3.03 (2.90)<br><b>2.80</b>               | $L$ , learned diagonal $\\Sigma$ $L$ , fixed isotropic $\\Sigma$ $\\ \\tilde{\\mu} - \\tilde{\\mu}_{\\theta}\\ ^2$                         | $7.28\\pm0.10$ $8.06\\pm0.09$  | 23.69<br>13.22<br>-       |\n| EBM [11]<br>NCSNv2 [56]                                                   | 6.78                                                             | $38.2 \\\\ 31.75$                      |                                          | $\\epsilon$ prediction (ours)                                                                                                      |                              |                           |\n| NCSN [55]<br>SNGAN [39]<br>SNGAN-DDLS [4]<br>StyleGAN2 + ADA (v1) [29]    | $8.87\\pm0.12$<br>$8.22\\pm0.05$<br>$9.09\\pm0.10$<br>$9.74\\pm0.05$ | 25.32<br>21.7<br>15.42<br>3.26       |                                          | $L$ , learned diagonal $\\Sigma$ $L$ , fixed isotropic $\\Sigma$ $\\ \\tilde{\\epsilon} - \\epsilon_{\\theta}\\ ^2 (L_{\\mathrm{simple}})$ | $-7.67\\pm0.13$ $9.46\\pm0.11$ | -<br>13.51<br><b>3.17</b> |\n| Ours $(L, \\text{ fixed isotropic } \\Sigma)$<br>Ours $(L_{\\text{simple}})$ | $7.67 \\pm 0.13$<br>$9.46 \\pm 0.11$                               | 13.51<br><b>3.17</b>                 | $\\leq 3.70 (3.69)$<br>$\\leq 3.75 (3.72)$ |                                                                                                                                   |                              |                           |\n"
    },
    {
      "table_id": "table_2",
      "headers": [
        "Algorithm 3 Sending $x_0$",
        "Algorithm 4 Receiving"
      ],
      "rows": [
        [
          "1: Send $\\mathbf{x}_T \\sim q(\\mathbf{x}_T \\mathbf{x}_0)$ using $p(\\mathbf{x}_T)$",
          "1: Receive $\\mathbf{x}_T$ using $p(\\mathbf{x}_T)$"
        ],
        [
          "2: <b>for</b> $t = T - 1, \\dots, 2, 1$ <b>do</b>",
          "2: for $t = T - 1, \\dots, 1, 0$ do"
        ],
        [
          "3: Send $\\mathbf{x}_t \\sim q(\\mathbf{x}_t \\mathbf{x}_{t+1}, \\mathbf{x}_0)$ using $p_{\\theta}(\\mathbf{x}_t \\mathbf{x}_{t+1})$",
          "3: Receive $\\mathbf{x}_t$ using $p_{\\theta}(\\mathbf{x}_t   \\mathbf{x}_{t+1})$"
        ],
        [
          "4: <b>end for</b>",
          "4: end for"
        ],
        [
          "5: Send $\\mathbf{x}_0$ using $p_{\\theta}(\\mathbf{x}_0 \\mathbf{x}_1)$",
          "5: return $\\mathbf{x}_0$"
        ]
      ],
      "markdown": "| Algorithm 3 Sending $x_0$                                                                                                    | Algorithm 4 Receiving                                                         |\n|------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------|\n| 1: Send $\\mathbf{x}_T \\sim q(\\mathbf{x}_T \\mathbf{x}_0)$ using $p(\\mathbf{x}_T)$                                             | 1: Receive $\\mathbf{x}_T$ using $p(\\mathbf{x}_T)$                             |\n| 2: <b>for</b> $t = T - 1, \\dots, 2, 1$ <b>do</b>                                                                             | 2: for $t = T - 1, \\dots, 1, 0$ do                                            |\n| 3: Send $\\mathbf{x}_t \\sim q(\\mathbf{x}_t \\mathbf{x}_{t+1}, \\mathbf{x}_0)$ using $p_{\\theta}(\\mathbf{x}_t \\mathbf{x}_{t+1})$ | 3: Receive $\\mathbf{x}_t$ using $p_{\\theta}(\\mathbf{x}_t   \\mathbf{x}_{t+1})$ |\n| 4: <b>end for</b>                                                                                                            | 4: end for                                                                    |\n| 5: Send $\\mathbf{x}_0$ using $p_{\\theta}(\\mathbf{x}_0 \\mathbf{x}_1)$                                                         | 5: return $\\mathbf{x}_0$                                                      |\n"
    },
    {
      "table_id": "table_3",
      "headers": [
        "",
        "Table 3: FID scores for LSUN 256 × 256 datasets",
        "",
        ""
      ],
      "rows": [
        [
          "",
          "",
          "",
          ""
        ]
      ],
      "markdown": "|  | Table 3: FID scores for LSUN 256 × 256 datasets |  |  |\n|--|-------------------------------------------------|--|--|\n|  |                                                 |  |  |\n"
    },
    {
      "table_id": "table_4",
      "headers": [
        "Model",
        "LSUN Bedroom",
        "LSUN Church",
        "LSUN Cat"
      ],
      "rows": [
        [
          "ProgressiveGAN [27]",
          "8.34",
          "6.42",
          "37.52"
        ],
        [
          "StyleGAN [28]",
          "2.65",
          "4.21∗",
          "8.53∗"
        ],
        [
          "StyleGAN2 [30]",
          "-",
          "3.86",
          "6.93"
        ],
        [
          "Ours (Lsimple)",
          "6.36",
          "7.89",
          "19.75"
        ],
        [
          "Ours (Lsimple, large)",
          "4.90",
          "-",
          "-"
        ]
      ],
      "markdown": "| Model                 | LSUN Bedroom | LSUN Church | LSUN Cat |\n|-----------------------|--------------|-------------|----------|\n| ProgressiveGAN [27]   | 8.34         | 6.42        | 37.52    |\n| StyleGAN [28]         | 2.65         | 4.21∗       | 8.53∗    |\n| StyleGAN2 [30]        | -            | 3.86        | 6.93     |\n| Ours (Lsimple)        | 6.36         | 7.89        | 19.75    |\n| Ours (Lsimple, large) | 4.90         | -           | -        |\n"
    },
    {
      "table_id": "table_5",
      "headers": [
        "Reverse process time (T − t + 1)",
        "Rate (bits/dim)",
        "Distortion (RMSE [0, 255])"
      ],
      "rows": [
        [
          "1000",
          "1.77581",
          "0.95136"
        ],
        [
          "900",
          "0.11994",
          "12.02277"
        ],
        [
          "800",
          "0.05415",
          "18.47482"
        ],
        [
          "700",
          "0.02866",
          "24.43656"
        ],
        [
          "600",
          "0.01507",
          "30.80948"
        ],
        [
          "500",
          "0.00716",
          "38.03236"
        ],
        [
          "400",
          "0.00282",
          "46.12765"
        ],
        [
          "300",
          "0.00081",
          "54.18826"
        ],
        [
          "200",
          "0.00013",
          "60.97170"
        ],
        [
          "100",
          "0.00000",
          "67.60125"
        ]
      ],
      "markdown": "| Reverse process time (T − t + 1) | Rate (bits/dim) | Distortion (RMSE [0, 255]) |\n|----------------------------------|-----------------|----------------------------|\n| 1000                             | 1.77581         | 0.95136                    |\n| 900                              | 0.11994         | 12.02277                   |\n| 800                              | 0.05415         | 18.47482                   |\n| 700                              | 0.02866         | 24.43656                   |\n| 600                              | 0.01507         | 30.80948                   |\n| 500                              | 0.00716         | 38.03236                   |\n| 400                              | 0.00282         | 46.12765                   |\n| 300                              | 0.00081         | 54.18826                   |\n| 200                              | 0.00013         | 60.97170                   |\n| 100                              | 0.00000         | 67.60125                   |\n"
    }
  ],
  "figures": [
    {
      "figure_id": "figure_1",
      "image_key": "_page_0_Picture_9.jpeg",
      "caption": "Generated samples on CelebA-HQ 256 × 256 (left) and unconditional CIFAR10 (right)",
      "alt_text": "",
      "context_before": "<span id=\"page-0-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_2",
      "image_key": "_page_1_Figure_0.jpeg",
      "caption": "The directed graphical model considered in this work.",
      "alt_text": "",
      "context_before": "",
      "context_after": ""
    },
    {
      "figure_id": "figure_3",
      "image_key": "_page_5_Picture_0.jpeg",
      "caption": "![](_page_5_Picture_1.jpeg)",
      "alt_text": "",
      "context_before": "<span id=\"page-5-2\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_4",
      "image_key": "_page_6_Figure_3.jpeg",
      "caption": "Unconditional CIFAR10 test set rate-distortion vs. time. Distortion is measured in root mean squared error on a [0, 255] scale. See Table 4 for details.",
      "alt_text": "",
      "context_before": "<span id=\"page-6-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_5",
      "image_key": "_page_6_Figure_6.jpeg",
      "caption": "<span id=\"page-6-2\"></span>Figure 6: Unconditional CIFAR10 progressive generation ( $\\hat{\\mathbf{x}}_0$  over time, from left to right). Extended samples and sample quality metrics over time in the appendix (Figs. 10 and 14).",
      "alt_text": "",
      "context_before": "<span id=\"page-6-1\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_6",
      "image_key": "_page_6_Figure_8.jpeg",
      "caption": "When conditioned on the same latent, CelebA-HQ  $256 \\times 256$  samples share high-level attributes. Bottom-right quadrants are  $\\mathbf{x}_t$ , and other quadrants are samples from  $p_{\\theta}(\\mathbf{x}_0|\\mathbf{x}_t)$ .",
      "alt_text": "",
      "context_before": "",
      "context_after": ""
    },
    {
      "figure_id": "figure_7",
      "image_key": "_page_7_Figure_0.jpeg",
      "caption": "Interpolations of CelebA-HQ 256x256 images with 500 timesteps of diffusion.",
      "alt_text": "",
      "context_before": "<span id=\"page-7-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_8",
      "image_key": "_page_15_Figure_1.jpeg",
      "caption": "Coarse-to-fine interpolations that vary the number of diffusion steps prior to latent mixing.",
      "alt_text": "",
      "context_before": "<span id=\"page-15-1\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_9",
      "image_key": "_page_15_Figure_3.jpeg",
      "caption": "Unconditional CIFAR10 progressive sampling quality over time",
      "alt_text": "",
      "context_before": "<span id=\"page-15-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_10",
      "image_key": "_page_16_Picture_0.jpeg",
      "caption": "CelebA-HQ 256 × 256 generated samples",
      "alt_text": "",
      "context_before": "<span id=\"page-16-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_11",
      "image_key": "_page_17_Figure_0.jpeg",
      "caption": "(b) Inception feature space nearest neighbors",
      "alt_text": "",
      "context_before": "",
      "context_after": ""
    },
    {
      "figure_id": "figure_12",
      "image_key": "_page_18_Figure_0.jpeg",
      "caption": "Unconditional CIFAR10 generated samples",
      "alt_text": "",
      "context_before": "<span id=\"page-18-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_13",
      "image_key": "_page_19_Figure_0.jpeg",
      "caption": "Unconditional CIFAR10 progressive generation",
      "alt_text": "",
      "context_before": "<span id=\"page-19-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_14",
      "image_key": "_page_20_Figure_0.jpeg",
      "caption": "(b) Inception feature space nearest neighbors",
      "alt_text": "",
      "context_before": "",
      "context_after": ""
    },
    {
      "figure_id": "figure_15",
      "image_key": "_page_21_Picture_0.jpeg",
      "caption": "LSUN Church generated samples. FID=7.89",
      "alt_text": "",
      "context_before": "<span id=\"page-21-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_16",
      "image_key": "_page_22_Picture_0.jpeg",
      "caption": "LSUN Bedroom generated samples, large model. FID=4.90",
      "alt_text": "",
      "context_before": "<span id=\"page-22-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_17",
      "image_key": "_page_23_Picture_0.jpeg",
      "caption": "LSUN Bedroom generated samples, small model. FID=6.36",
      "alt_text": "",
      "context_before": "<span id=\"page-23-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_18",
      "image_key": "_page_24_Picture_0.jpeg",
      "caption": "LSUN Cat generated samples. FID=19.75",
      "alt_text": "",
      "context_before": "<span id=\"page-24-0\"></span>",
      "context_after": ""
    }
  ],
  "stats": {
    "total_chunks": 123,
    "text_chunks": 100,
    "tables_extracted": 5,
    "images_extracted": 19,
    "figure_chunks_created": 18,
    "figures_with_vision": 18,
    "markdown_chars": 71695,
    "processing_time_seconds": 276.61
  }
}