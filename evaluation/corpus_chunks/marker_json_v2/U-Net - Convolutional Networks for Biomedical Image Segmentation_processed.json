{
  "source": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf",
  "processor": "marker",
  "processed_date": "2025-12-14T07:37:03.437806",
  "markdown": "# U-Net: Convolutional Networks for Biomedical Image Segmentation\n\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox\n\nComputer Science Department and BIOSS Centre for Biological Signalling Studies, University of Freiburg, Germany\n\n> ronneber@informatik.uni-freiburg.de , WWW home page: http://lmb.informatik.uni-freiburg.de/\n\nAbstract. There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at <http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net> .\n\n### 1 Introduction\n\nIn the last two years, deep convolutional networks have outperformed the state of the art in many visual recognition tasks, e.g. [ [7](#page-7-0) , [3\\]](#page-7-1). While convolutional networks have already existed for a long time [ [8\\]](#page-7-2), their success was limited due to the size of the available training sets and the size of the considered networks. The breakthrough by Krizhevsky et al. [ [7\\]](#page-7-0) was due to supervised training of a large network with 8 layers and millions of parameters on the ImageNet dataset with 1 million training images. Since then, even larger and deeper networks have been trained [\\[12\\]](#page-7-3).\n\nThe typical use of convolutional networks is on classification tasks, where the output to an image is a single class label. However, in many visual tasks, especially in biomedical image processing, the desired output should include localization, i.e., a class label is supposed to be assigned to each pixel. Moreover, thousands of training images are usually beyond reach in biomedical tasks. Hence, Ciresan et al. [ [1\\]](#page-7-4) trained a network in a sliding-window setup to predict the class label of each pixel by providing a local region (patch) around that pixel\n\n<span id=\"page-1-0\"></span>![](_page_1_Figure_1.jpeg)\n\n**Fig. 1.** U-net architecture (example for 32x32 pixels in the lowest resolution). Each blue box corresponds to a multi-channel feature map. The number of channels is denoted on top of the box. The x-y-size is provided at the lower left edge of the box. White boxes represent copied feature maps. The arrows denote the different operations.\n\nas input. First, this network can localize. Secondly, the training data in terms of patches is much larger than the number of training images. The resulting network won the EM segmentation challenge at ISBI 2012 by a large margin.\n\nObviously, the strategy in Ciresan et al. [1] has two drawbacks. First, it is quite slow because the network must be run separately for each patch, and there is a lot of redundancy due to overlapping patches. Secondly, there is a trade-off between localization accuracy and the use of context. Larger patches require more max-pooling layers that reduce the localization accuracy, while small patches allow the network to see only little context. More recent approaches [11,4] proposed a classifier output that takes into account the features from multiple layers. Good localization and the use of context are possible at the same time.\n\nIn this paper, we build upon a more elegant architecture, the so-called \"fully convolutional network\" [9]. We modify and extend this architecture such that it works with very few training images and yields more precise segmentations; see Figure 1. The main idea in [9] is to supplement a usual contracting network by successive layers, where pooling operators are replaced by upsampling operators. Hence, these layers increase the resolution of the output. In order to localize, high resolution features from the contracting path are combined with the upsampled\n\n<span id=\"page-2-0\"></span>![](_page_2_Picture_1.jpeg)\n\nFig. 2. Overlap-tile strategy for seamless segmentation of arbitrary large images (here segmentation of neuronal structures in EM stacks). Prediction of the segmentation in the yellow area, requires image data within the blue area as input. Missing input data is extrapolated by mirroring\n\noutput. A successive convolution layer can then learn to assemble a more precise output based on this information.\n\nOne important modification in our architecture is that in the upsampling part we have also a large number of feature channels, which allow the network to propagate context information to higher resolution layers. As a consequence, the expansive path is more or less symmetric to the contracting path, and yields a u-shaped architecture. The network does not have any fully connected layers and only uses the valid part of each convolution, i.e., the segmentation map only contains the pixels, for which the full context is available in the input image. This strategy allows the seamless segmentation of arbitrarily large images by an overlap-tile strategy (see [Figure 2\\)](#page-2-0). To predict the pixels in the border region of the image, the missing context is extrapolated by mirroring the input image. This tiling strategy is important to apply the network to large images, since otherwise the resolution would be limited by the GPU memory.\n\nAs for our tasks there is very little training data available, we use excessive data augmentation by applying elastic deformations to the available training images. This allows the network to learn invariance to such deformations, without the need to see these transformations in the annotated image corpus. This is particularly important in biomedical segmentation, since deformation used to be the most common variation in tissue and realistic deformations can be simulated efficiently. The value of data augmentation for learning invariance has been shown in Dosovitskiy et al. [\\[2\\]](#page-7-8) in the scope of unsupervised feature learning.\n\nAnother challenge in many cell segmentation tasks is the separation of touching objects of the same class; see [Figure 3.](#page-4-0) To this end, we propose the use of a weighted loss, where the separating background labels between touching cells obtain a large weight in the loss function.\n\nThe resulting network is applicable to various biomedical segmentation problems. In this paper, we show results on the segmentation of neuronal structures in EM stacks (an ongoing competition started at ISBI 2012), where we outperformed the network of Ciresan et al. [\\[1\\]](#page-7-4). Furthermore, we show results for cell segmentation in light microscopy images from the ISBI cell tracking challenge 2015. Here we won with a large margin on the two most challenging 2D transmitted light datasets.\n\n### 2 Network Architecture\n\nThe network architecture is illustrated in [Figure 1.](#page-1-0) It consists of a contracting path (left side) and an expansive path (right side). The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels. Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 convolution (\"up-convolution\") that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution. At the final layer a 1x1 convolution is used to map each 64 component feature vector to the desired number of classes. In total the network has 23 convolutional layers.\n\nTo allow a seamless tiling of the output segmentation map (see [Figure 2\\)](#page-2-0), it is important to select the input tile size such that all 2x2 max-pooling operations are applied to a layer with an even x- and y-size.\n\n### 3 Training\n\nThe input images and their corresponding segmentation maps are used to train the network with the stochastic gradient descent implementation of Caffe [\\[6\\]](#page-7-9). Due to the unpadded convolutions, the output image is smaller than the input by a constant border width. To minimize the overhead and make maximum use of the GPU memory, we favor large input tiles over a large batch size and hence reduce the batch to a single image. Accordingly we use a high momentum (0.99) such that a large number of the previously seen training samples determine the update in the current optimization step.\n\nThe energy function is computed by a pixel-wise soft-max over the final feature map combined with the cross entropy loss function. The soft-max is defined as pk(x) = exp(ak(x))/ P<sup>K</sup> <sup>k</sup>0=1 exp(ak<sup>0</sup> (x)) where ak(x) denotes the activation in feature channel k at the pixel position x ∈ Ω with Ω ⊂ Z 2 . K is the number of classes and pk(x) is the approximated maximum-function. I.e. pk(x) ≈ 1 for the k that has the maximum activation ak(x) and pk(x) ≈ 0 for all other k. The cross entropy then penalizes at each position the deviation of p`(x)(x) from 1 using\n\n$$E = \\sum_{\\mathbf{x} \\in \\Omega} w(\\mathbf{x}) \\log(p_{\\ell(\\mathbf{x})}(\\mathbf{x}))$$\n (1)\n\n<span id=\"page-4-0\"></span>![](_page_4_Figure_1.jpeg)\n\nFig. 3. HeLa cells on glass recorded with DIC (differential interference contrast) microscopy. (a) raw image. (b) overlay with ground truth segmentation. Different colors indicate different instances of the HeLa cells. (c) generated segmentation mask (white: foreground, black: background). (d) map with a pixel-wise loss weight to force the network to learn the border pixels.\n\nwhere ` : Ω → {1, . . . , K} is the true label of each pixel and w : Ω → R is a weight map that we introduced to give some pixels more importance in the training.\n\nWe pre-compute the weight map for each ground truth segmentation to compensate the different frequency of pixels from a certain class in the training data set, and to force the network to learn the small separation borders that we introduce between touching cells (See [Figure 3c](#page-4-0) and d).\n\nThe separation border is computed using morphological operations. The weight map is then computed as\n\n$$w(\\mathbf{x}) = w_c(\\mathbf{x}) + w_0 \\cdot \\exp\\left(-\\frac{(d_1(\\mathbf{x}) + d_2(\\mathbf{x}))^2}{2\\sigma^2}\\right)$$\n(2)\n\nwhere w<sup>c</sup> : Ω → R is the weight map to balance the class frequencies, d<sup>1</sup> : Ω → R denotes the distance to the border of the nearest cell and d<sup>2</sup> : Ω → R the distance to the border of the second nearest cell. In our experiments we set w<sup>0</sup> = 10 and σ ≈ 5 pixels.\n\nIn deep networks with many convolutional layers and different paths through the network, a good initialization of the weights is extremely important. Otherwise, parts of the network might give excessive activations, while other parts never contribute. Ideally the initial weights should be adapted such that each feature map in the network has approximately unit variance. For a network with our architecture (alternating convolution and ReLU layers) this can be achieved by drawing the initial weights from a Gaussian distribution with a standard deviation of p 2/N, where N denotes the number of incoming nodes of one neuron [\\[5\\]](#page-7-10). E.g. for a 3x3 convolution and 64 feature channels in the previous layer N = 9 · 64 = 576.\n\n#### 3.1 Data Augmentation\n\nData augmentation is essential to teach the network the desired invariance and robustness properties, when only few training samples are available. In case of microscopical images we primarily need shift and rotation invariance as well as robustness to deformations and gray value variations. Especially random elastic deformations of the training samples seem to be the key concept to train a segmentation network with very few annotated images. We generate smooth deformations using random displacement vectors on a coarse 3 by 3 grid. The displacements are sampled from a Gaussian distribution with 10 pixels standard deviation. Per-pixel displacements are then computed using bicubic interpolation. Drop-out layers at the end of the contracting path perform further implicit data augmentation.\n\n#### 4 Experiments\n\nWe demonstrate the application of the u-net to three different segmentation tasks. The first task is the segmentation of neuronal structures in electron microscopic recordings. An example of the data set and our obtained segmentation is displayed in Figure 2. We provide the full result as Supplementary Material. The data set is provided by the EM segmentation challenge [14] that was started at ISBI 2012 and is still open for new contributions. The training data is a set of 30 images (512x512 pixels) from serial section transmission electron microscopy of the Drosophila first instar larva ventral nerve cord (VNC). Each image comes with a corresponding fully annotated ground truth segmentation map for cells (white) and membranes (black). The test set is publicly available, but its segmentation maps are kept secret. An evaluation can be obtained by sending the predicted membrane probability map to the organizers. The evaluation is done by thresholding the map at 10 different levels and computation of the \"warping error\", the \"Rand error\" and the \"pixel error\" [14].\n\nThe u-net (averaged over 7 rotated versions of the input data) achieves without any further pre- or postprocessing a warping error of 0.0003529 (the new best score, see Table 1) and a rand-error of 0.0382.\n\nThis is significantly better than the sliding-window convolutional network result by Ciresan et al. [1], whose best submission had a warping error of 0.000420 and a rand error of 0.0504. In terms of rand error the only better performing\n\n<span id=\"page-5-0\"></span>\n\n| Table 1. Ranking on | the EM segmentation | challenge [14] (march | 6th, 2015), sorted |\n|---------------------|---------------------|-----------------------|--------------------|\n| by warping error.   |                     |                       |                    |\n\n| Rank | Group name         | Warping Error | Rand Error | Pixel Error |\n|------|--------------------|---------------|------------|-------------|\n|      | ** human values ** | 0.000005      | 0.0021     | 0.0010      |\n| 1.   | u-net              | 0.000353      | 0.0382     | 0.0611      |\n| 2.   | DIVE-SCI           | 0.000355      | 0.0305     | 0.0584      |\n| 3.   | IDSIA [1]          | 0.000420      | 0.0504     | 0.0613      |\n| 4.   | DIVE               | 0.000430      | 0.0545     | 0.0582      |\n| :    |                    |               |            |             |\n| 10   | IDGIA GGI          | 0.000059      | 0.0100     | 0.1007      |\n| 10.  | IDSIA-SCI          | 0.000653      | 0.0189     | 0.1027      |\n\n<span id=\"page-6-2\"></span>![](_page_6_Figure_1.jpeg)\n\nFig. 4. Result on the ISBI cell tracking challenge. (a) part of an input image of the \"PhC-U373\" data set. (b) Segmentation result (cyan mask) with manual ground truth (yellow border) (c) input image of the \"DIC-HeLa\" data set. (d) Segmentation result (random colored masks) with manual ground truth (yellow border).\n\n<span id=\"page-6-3\"></span>Table 2. Segmentation results (IOU) on the ISBI cell tracking challenge 2015.\n\n| PhC-U373 | DIC-HeLa |\n|----------|----------|\n| 0.2669   | 0.2935   |\n| 0.7953   | 0.4607   |\n| 0.5323   | -        |\n| 0.83     | 0.46     |\n| 0.9203   | 0.7756   |\n|          |          |\n\nalgorithms on this data set use highly data set specific post-processing methods[1](#page-6-0) applied to the probability map of Ciresan et al. [\\[1\\]](#page-7-4).\n\nWe also applied the u-net to a cell segmentation task in light microscopic images. This segmenation task is part of the ISBI cell tracking challenge 2014 and 2015 [\\[10,](#page-7-12)[13\\]](#page-7-13). The first data set \"PhC-U373\"[2](#page-6-1) contains Glioblastoma-astrocytoma U373 cells on a polyacrylimide substrate recorded by phase contrast microscopy (see [Figure 4a](#page-6-2),b and Supp. Material). It contains 35 partially annotated training images. Here we achieve an average IOU (\"intersection over union\") of 92%, which is significantly better than the second best algorithm with 83% (see [Ta](#page-6-3)[ble 2\\)](#page-6-3). The second data set \"DIC-HeLa\"[3](#page-6-4) are HeLa cells on a flat glass recorded by differential interference contrast (DIC) microscopy (see [Figure 3,](#page-4-0) [Figure 4c](#page-6-2),d and Supp. Material). It contains 20 partially annotated training images. Here we achieve an average IOU of 77.5% which is significantly better than the second best algorithm with 46%.\n\n### 5 Conclusion\n\nThe u-net architecture achieves very good performance on very different biomedical segmentation applications. Thanks to data augmentation with elastic defor-\n\n<span id=\"page-6-0\"></span><sup>1</sup> The authors of this algorithm have submitted 78 different solutions to achieve this result.\n\n<span id=\"page-6-1\"></span><sup>2</sup> Data set provided by Dr. Sanjay Kumar. Department of Bioengineering University of California at Berkeley. Berkeley CA (USA)\n\n<span id=\"page-6-4\"></span><sup>3</sup> Data set provided by Dr. Gert van Cappellen Erasmus Medical Center. Rotterdam. The Netherlands\n\nmations, it only needs very few annotated images and has a very reasonable training time of only 10 hours on a NVidia Titan GPU (6 GB). We provide the full Caffe[\\[6\\]](#page-7-9)-based implementation and the trained networks[4](#page-7-14) . We are sure that the u-net architecture can be applied easily to many more tasks.\n\n## Acknowlegements\n\nThis study was supported by the Excellence Initiative of the German Federal and State governments (EXC 294) and by the BMBF (Fkz 0316185B).\n\n# References\n\n- <span id=\"page-7-4\"></span>1. Ciresan, D.C., Gambardella, L.M., Giusti, A., Schmidhuber, J.: Deep neural networks segment neuronal membranes in electron microscopy images. In: NIPS. pp. 2852–2860 (2012)\n- <span id=\"page-7-8\"></span>2. Dosovitskiy, A., Springenberg, J.T., Riedmiller, M., Brox, T.: Discriminative unsupervised feature learning with convolutional neural networks. In: NIPS (2014)\n- <span id=\"page-7-1\"></span>3. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object detection and semantic segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2014)\n- <span id=\"page-7-6\"></span>4. Hariharan, B., Arbelez, P., Girshick, R., Malik, J.: Hypercolumns for object segmentation and fine-grained localization (2014), arXiv:1411.5752 [cs.CV]\n- <span id=\"page-7-10\"></span>5. He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification (2015), arXiv:1502.01852 [cs.CV]\n- <span id=\"page-7-9\"></span>6. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., Darrell, T.: Caffe: Convolutional architecture for fast feature embedding (2014), arXiv:1408.5093 [cs.CV]\n- <span id=\"page-7-0\"></span>7. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: NIPS. pp. 1106–1114 (2012)\n- <span id=\"page-7-2\"></span>8. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural Computation 1(4), 541–551 (1989)\n- <span id=\"page-7-7\"></span>9. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation (2014), arXiv:1411.4038 [cs.CV]\n- <span id=\"page-7-12\"></span>10. Maska, M., (...), de Solorzano, C.O.: A benchmark for comparison of cell tracking algorithms. Bioinformatics 30, 1609–1617 (2014)\n- <span id=\"page-7-5\"></span>11. Seyedhosseini, M., Sajjadi, M., Tasdizen, T.: Image segmentation with cascaded hierarchical models and logistic disjunctive normal networks. In: Computer Vision (ICCV), 2013 IEEE International Conference on. pp. 2168–2175 (2013)\n- <span id=\"page-7-3\"></span>12. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition (2014), arXiv:1409.1556 [cs.CV]\n- <span id=\"page-7-13\"></span>13. WWW: Web page of the cell tracking challenge, [http://www.codesolorzano.com/](http://www.codesolorzano.com/ celltrackingchallenge/Cell_Tracking_Challenge/Welcome.html) [celltrackingchallenge/Cell\\\\_Tracking\\\\_Challenge/Welcome.html](http://www.codesolorzano.com/ celltrackingchallenge/Cell_Tracking_Challenge/Welcome.html)\n- <span id=\"page-7-11\"></span>14. WWW: Web page of the em segmentation challenge, [http://brainiac2.mit.edu/](http://brainiac2.mit.edu/isbi_challenge/) [isbi\\\\_challenge/](http://brainiac2.mit.edu/isbi_challenge/)\n\n<span id=\"page-7-14\"></span><sup>4</sup> U-net implementation, trained networks and supplementary material available at <http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net>",
  "chunks": [
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_0",
      "content": "# U-Net: Convolutional Networks for Biomedical Image Segmentation\n\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox\n\nComputer Science Department and BIOSS Centre for Biological Signalling Studies, University of Freiburg, Germany\n\n> ronneber@informatik.uni-freiburg.de , WWW home page: http://lmb.informatik.uni-freiburg.de/",
      "metadata": {
        "chunk_index": 0,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 325
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_1",
      "content": "Abstract. There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU",
      "metadata": {
        "chunk_index": 1,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 955
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_2",
      "content": ". Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at <http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net> .",
      "metadata": {
        "chunk_index": 2,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 253
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_3",
      "content": "### 1 Introduction\n\nIn the last two years, deep convolutional networks have outperformed the state of the art in many visual recognition tasks, e.g. [ [7](#page-7-0) , [3\\]](#page-7-1). While convolutional networks have already existed for a long time [ [8\\]](#page-7-2), their success was limited due to the size of the available training sets and the size of the considered networks. The breakthrough by Krizhevsky et al. [ [7\\]](#page-7-0) was due to supervised training of a large network with 8 layers and millions of parameters on the ImageNet dataset with 1 million training images. Since then, even larger and deeper networks have been trained [\\[12\\]](#page-7-3).",
      "metadata": {
        "chunk_index": 3,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 672
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_4",
      "content": "The typical use of convolutional networks is on classification tasks, where the output to an image is a single class label. However, in many visual tasks, especially in biomedical image processing, the desired output should include localization, i.e., a class label is supposed to be assigned to each pixel. Moreover, thousands of training images are usually beyond reach in biomedical tasks. Hence, Ciresan et al. [ [1\\]](#page-7-4) trained a network in a sliding-window setup to predict the class label of each pixel by providing a local region (patch) around that pixel\n\n<span id=\"page-1-0\"></span>![](_page_1_Figure_1.jpeg)\n\n**Fig. 1.** U-net architecture (example for 32x32 pixels in the lowest resolution). Each blue box corresponds to a multi-channel feature map. The number of channels is denoted on top of the box. The x-y-size is provided at the lower left edge of the box. White boxes represent copied feature maps. The arrows denote the different operations.",
      "metadata": {
        "chunk_index": 4,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 970
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_5",
      "content": "as input. First, this network can localize. Secondly, the training data in terms of patches is much larger than the number of training images. The resulting network won the EM segmentation challenge at ISBI 2012 by a large margin.\n\nObviously, the strategy in Ciresan et al. [1] has two drawbacks. First, it is quite slow because the network must be run separately for each patch, and there is a lot of redundancy due to overlapping patches. Secondly, there is a trade-off between localization accuracy and the use of context. Larger patches require more max-pooling layers that reduce the localization accuracy, while small patches allow the network to see only little context. More recent approaches [11,4] proposed a classifier output that takes into account the features from multiple layers. Good localization and the use of context are possible at the same time.",
      "metadata": {
        "chunk_index": 5,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 867
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_6",
      "content": "In this paper, we build upon a more elegant architecture, the so-called \"fully convolutional network\" [9]. We modify and extend this architecture such that it works with very few training images and yields more precise segmentations; see Figure 1. The main idea in [9] is to supplement a usual contracting network by successive layers, where pooling operators are replaced by upsampling operators. Hence, these layers increase the resolution of the output. In order to localize, high resolution features from the contracting path are combined with the upsampled\n\n<span id=\"page-2-0\"></span>![](_page_2_Picture_1.jpeg)\n\nFig. 2. Overlap-tile strategy for seamless segmentation of arbitrary large images (here segmentation of neuronal structures in EM stacks). Prediction of the segmentation in the yellow area, requires image data within the blue area as input. Missing input data is extrapolated by mirroring",
      "metadata": {
        "chunk_index": 6,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 907
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_7",
      "content": "output. A successive convolution layer can then learn to assemble a more precise output based on this information.",
      "metadata": {
        "chunk_index": 7,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 114
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_8",
      "content": "One important modification in our architecture is that in the upsampling part we have also a large number of feature channels, which allow the network to propagate context information to higher resolution layers. As a consequence, the expansive path is more or less symmetric to the contracting path, and yields a u-shaped architecture. The network does not have any fully connected layers and only uses the valid part of each convolution, i.e., the segmentation map only contains the pixels, for which the full context is available in the input image. This strategy allows the seamless segmentation of arbitrarily large images by an overlap-tile strategy (see [Figure 2\\)](#page-2-0). To predict the pixels in the border region of the image, the missing context is extrapolated by mirroring the input image. This tiling strategy is important to apply the network to large images, since otherwise the resolution would be limited by the GPU memory.",
      "metadata": {
        "chunk_index": 8,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 947
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_9",
      "content": "As for our tasks there is very little training data available, we use excessive data augmentation by applying elastic deformations to the available training images. This allows the network to learn invariance to such deformations, without the need to see these transformations in the annotated image corpus. This is particularly important in biomedical segmentation, since deformation used to be the most common variation in tissue and realistic deformations can be simulated efficiently. The value of data augmentation for learning invariance has been shown in Dosovitskiy et al. [\\[2\\]](#page-7-8) in the scope of unsupervised feature learning.\n\nAnother challenge in many cell segmentation tasks is the separation of touching objects of the same class; see [Figure 3.](#page-4-0) To this end, we propose the use of a weighted loss, where the separating background labels between touching cells obtain a large weight in the loss function.",
      "metadata": {
        "chunk_index": 9,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 939
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_10",
      "content": "The resulting network is applicable to various biomedical segmentation problems. In this paper, we show results on the segmentation of neuronal structures in EM stacks (an ongoing competition started at ISBI 2012), where we outperformed the network of Ciresan et al. [\\[1\\]](#page-7-4). Furthermore, we show results for cell segmentation in light microscopy images from the ISBI cell tracking challenge 2015. Here we won with a large margin on the two most challenging 2D transmitted light datasets.",
      "metadata": {
        "chunk_index": 10,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 499
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_11",
      "content": "### 2 Network Architecture",
      "metadata": {
        "chunk_index": 11,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 26
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_12",
      "content": "The network architecture is illustrated in [Figure 1.](#page-1-0) It consists of a contracting path (left side) and an expansive path (right side). The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels. Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 convolution (\"up-convolution\") that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution",
      "metadata": {
        "chunk_index": 12,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 888
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_13",
      "content": ". The cropping is necessary due to the loss of border pixels in every convolution. At the final layer a 1x1 convolution is used to map each 64 component feature vector to the desired number of classes. In total the network has 23 convolutional layers.",
      "metadata": {
        "chunk_index": 13,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 251
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_14",
      "content": "To allow a seamless tiling of the output segmentation map (see [Figure 2\\)](#page-2-0), it is important to select the input tile size such that all 2x2 max-pooling operations are applied to a layer with an even x- and y-size.",
      "metadata": {
        "chunk_index": 14,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 225
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_15",
      "content": "### 3 Training\n\nThe input images and their corresponding segmentation maps are used to train the network with the stochastic gradient descent implementation of Caffe [\\[6\\]](#page-7-9). Due to the unpadded convolutions, the output image is smaller than the input by a constant border width. To minimize the overhead and make maximum use of the GPU memory, we favor large input tiles over a large batch size and hence reduce the batch to a single image. Accordingly we use a high momentum (0.99) such that a large number of the previously seen training samples determine the update in the current optimization step.",
      "metadata": {
        "chunk_index": 15,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 614
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_16",
      "content": "The energy function is computed by a pixel-wise soft-max over the final feature map combined with the cross entropy loss function. The soft-max is defined as pk(x) = exp(ak(x))/ P<sup>K</sup> <sup>k</sup>0=1 exp(ak<sup>0</sup> (x)) where ak(x) denotes the activation in feature channel k at the pixel position x ∈ Ω with Ω ⊂ Z 2 . K is the number of classes and pk(x) is the approximated maximum-function. I.e. pk(x) ≈ 1 for the k that has the maximum activation ak(x) and pk(x) ≈ 0 for all other k. The cross entropy then penalizes at each position the deviation of p`(x)(x) from 1 using\n\n$$E = \\sum_{\\mathbf{x} \\in \\Omega} w(\\mathbf{x}) \\log(p_{\\ell(\\mathbf{x})}(\\mathbf{x}))$$\n (1)\n\n<span id=\"page-4-0\"></span>![](_page_4_Figure_1.jpeg)",
      "metadata": {
        "chunk_index": 16,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 739
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_17",
      "content": "$$E = \\sum_{\\mathbf{x} \\in \\Omega} w(\\mathbf{x}) \\log(p_{\\ell(\\mathbf{x})}(\\mathbf{x}))$$\n (1)\n\n<span id=\"page-4-0\"></span>![](_page_4_Figure_1.jpeg)\n\nFig. 3. HeLa cells on glass recorded with DIC (differential interference contrast) microscopy. (a) raw image. (b) overlay with ground truth segmentation. Different colors indicate different instances of the HeLa cells. (c) generated segmentation mask (white: foreground, black: background). (d) map with a pixel-wise loss weight to force the network to learn the border pixels.\n\nwhere ` : Ω → {1, . . . , K} is the true label of each pixel and w : Ω → R is a weight map that we introduced to give some pixels more importance in the training.\n\nWe pre-compute the weight map for each ground truth segmentation to compensate the different frequency of pixels from a certain class in the training data set, and to force the network to learn the small separation borders that we introduce between touching cells (See [Figure 3c](#page-4-0) and d).",
      "metadata": {
        "chunk_index": 17,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 993
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_18",
      "content": "The separation border is computed using morphological operations. The weight map is then computed as\n\n$$w(\\mathbf{x}) = w_c(\\mathbf{x}) + w_0 \\cdot \\exp\\left(-\\frac{(d_1(\\mathbf{x}) + d_2(\\mathbf{x}))^2}{2\\sigma^2}\\right)$$\n(2)\n\nwhere w<sup>c</sup> : Ω → R is the weight map to balance the class frequencies, d<sup>1</sup> : Ω → R denotes the distance to the border of the nearest cell and d<sup>2</sup> : Ω → R the distance to the border of the second nearest cell. In our experiments we set w<sup>0</sup> = 10 and σ ≈ 5 pixels.",
      "metadata": {
        "chunk_index": 18,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 529
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_19",
      "content": "In deep networks with many convolutional layers and different paths through the network, a good initialization of the weights is extremely important. Otherwise, parts of the network might give excessive activations, while other parts never contribute. Ideally the initial weights should be adapted such that each feature map in the network has approximately unit variance. For a network with our architecture (alternating convolution and ReLU layers) this can be achieved by drawing the initial weights from a Gaussian distribution with a standard deviation of p 2/N, where N denotes the number of incoming nodes of one neuron [\\[5\\]](#page-7-10). E.g. for a 3x3 convolution and 64 feature channels in the previous layer N = 9 · 64 = 576.",
      "metadata": {
        "chunk_index": 19,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 738
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_20",
      "content": "#### 3.1 Data Augmentation\n\nData augmentation is essential to teach the network the desired invariance and robustness properties, when only few training samples are available. In case of microscopical images we primarily need shift and rotation invariance as well as robustness to deformations and gray value variations. Especially random elastic deformations of the training samples seem to be the key concept to train a segmentation network with very few annotated images. We generate smooth deformations using random displacement vectors on a coarse 3 by 3 grid. The displacements are sampled from a Gaussian distribution with 10 pixels standard deviation. Per-pixel displacements are then computed using bicubic interpolation. Drop-out layers at the end of the contracting path perform further implicit data augmentation.",
      "metadata": {
        "chunk_index": 20,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 825
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_21",
      "content": "#### 4 Experiments",
      "metadata": {
        "chunk_index": 21,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 18
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_22",
      "content": "We demonstrate the application of the u-net to three different segmentation tasks. The first task is the segmentation of neuronal structures in electron microscopic recordings. An example of the data set and our obtained segmentation is displayed in Figure 2. We provide the full result as Supplementary Material. The data set is provided by the EM segmentation challenge [14] that was started at ISBI 2012 and is still open for new contributions. The training data is a set of 30 images (512x512 pixels) from serial section transmission electron microscopy of the Drosophila first instar larva ventral nerve cord (VNC). Each image comes with a corresponding fully annotated ground truth segmentation map for cells (white) and membranes (black). The test set is publicly available, but its segmentation maps are kept secret. An evaluation can be obtained by sending the predicted membrane probability map to the organizers",
      "metadata": {
        "chunk_index": 22,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 922
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_23",
      "content": ". The test set is publicly available, but its segmentation maps are kept secret. An evaluation can be obtained by sending the predicted membrane probability map to the organizers. The evaluation is done by thresholding the map at 10 different levels and computation of the \"warping error\", the \"Rand error\" and the \"pixel error\" [14].",
      "metadata": {
        "chunk_index": 23,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 334
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_24",
      "content": "The u-net (averaged over 7 rotated versions of the input data) achieves without any further pre- or postprocessing a warping error of 0.0003529 (the new best score, see Table 1) and a rand-error of 0.0382.\n\nThis is significantly better than the sliding-window convolutional network result by Ciresan et al. [1], whose best submission had a warping error of 0.000420 and a rand error of 0.0504. In terms of rand error the only better performing\n\n<span id=\"page-5-0\"></span>\n\n| Table 1. Ranking on | the EM segmentation | challenge [14] (march | 6th, 2015), sorted |\n|---------------------|---------------------|-----------------------|--------------------|\n| by warping error.   |                     |                       |                    |",
      "metadata": {
        "chunk_index": 24,
        "content_type": "text",
        "has_table": true,
        "has_figure": false,
        "char_count": 746
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_25",
      "content": "| Rank | Group name         | Warping Error | Rand Error | Pixel Error |\n|------|--------------------|---------------|------------|-------------|\n|      | ** human values ** | 0.000005      | 0.0021     | 0.0010      |\n| 1.   | u-net              | 0.000353      | 0.0382     | 0.0611      |\n| 2.   | DIVE-SCI           | 0.000355      | 0.0305     | 0.0584      |\n| 3.   | IDSIA [1]          | 0.000420      | 0.0504     | 0.0613      |\n| 4.   | DIVE               | 0.000430      | 0.0545     | 0.0582      |\n| :    |                    |               |            |             |\n| 10   | IDGIA GGI          | 0.000059      | 0.0100     | 0.1007      |\n| 10.  | IDSIA-SCI          | 0.000653      | 0.0189     | 0.1027      |\n\n<span id=\"page-6-2\"></span>![](_page_6_Figure_1.jpeg)",
      "metadata": {
        "chunk_index": 25,
        "content_type": "text",
        "has_table": true,
        "has_figure": true,
        "char_count": 784
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_26",
      "content": "<span id=\"page-6-2\"></span>![](_page_6_Figure_1.jpeg)\n\nFig. 4. Result on the ISBI cell tracking challenge. (a) part of an input image of the \"PhC-U373\" data set. (b) Segmentation result (cyan mask) with manual ground truth (yellow border) (c) input image of the \"DIC-HeLa\" data set. (d) Segmentation result (random colored masks) with manual ground truth (yellow border).\n\n<span id=\"page-6-3\"></span>Table 2. Segmentation results (IOU) on the ISBI cell tracking challenge 2015.\n\n| PhC-U373 | DIC-HeLa |\n|----------|----------|\n| 0.2669   | 0.2935   |\n| 0.7953   | 0.4607   |\n| 0.5323   | -        |\n| 0.83     | 0.46     |\n| 0.9203   | 0.7756   |\n|          |          |\n\nalgorithms on this data set use highly data set specific post-processing methods[1](#page-6-0) applied to the probability map of Ciresan et al. [\\[1\\]](#page-7-4).",
      "metadata": {
        "chunk_index": 26,
        "content_type": "text",
        "has_table": true,
        "has_figure": true,
        "char_count": 835
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_27",
      "content": "We also applied the u-net to a cell segmentation task in light microscopic images. This segmenation task is part of the ISBI cell tracking challenge 2014 and 2015 [\\[10,](#page-7-12)[13\\]](#page-7-13). The first data set \"PhC-U373\"[2](#page-6-1) contains Glioblastoma-astrocytoma U373 cells on a polyacrylimide substrate recorded by phase contrast microscopy (see [Figure 4a](#page-6-2),b and Supp. Material). It contains 35 partially annotated training images. Here we achieve an average IOU (\"intersection over union\") of 92%, which is significantly better than the second best algorithm with 83% (see [Ta](#page-6-3)[ble 2\\)](#page-6-3). The second data set \"DIC-HeLa\"[3](#page-6-4) are HeLa cells on a flat glass recorded by differential interference contrast (DIC) microscopy (see [Figure 3,](#page-4-0) [Figure 4c](#page-6-2),d and Supp. Material). It contains 20 partially annotated training images",
      "metadata": {
        "chunk_index": 27,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 905
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_28",
      "content": ". Material). It contains 20 partially annotated training images. Here we achieve an average IOU of 77.5% which is significantly better than the second best algorithm with 46%.",
      "metadata": {
        "chunk_index": 28,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 175
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_29",
      "content": "### 5 Conclusion\n\nThe u-net architecture achieves very good performance on very different biomedical segmentation applications. Thanks to data augmentation with elastic defor-\n\n<span id=\"page-6-0\"></span><sup>1</sup> The authors of this algorithm have submitted 78 different solutions to achieve this result.\n\n<span id=\"page-6-1\"></span><sup>2</sup> Data set provided by Dr. Sanjay Kumar. Department of Bioengineering University of California at Berkeley. Berkeley CA (USA)\n\n<span id=\"page-6-4\"></span><sup>3</sup> Data set provided by Dr. Gert van Cappellen Erasmus Medical Center. Rotterdam. The Netherlands\n\nmations, it only needs very few annotated images and has a very reasonable training time of only 10 hours on a NVidia Titan GPU (6 GB). We provide the full Caffe[\\[6\\]](#page-7-9)-based implementation and the trained networks[4](#page-7-14) . We are sure that the u-net architecture can be applied easily to many more tasks.",
      "metadata": {
        "chunk_index": 29,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 935
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_30",
      "content": "## Acknowlegements\n\nThis study was supported by the Excellence Initiative of the German Federal and State governments (EXC 294) and by the BMBF (Fkz 0316185B).\n\n# References",
      "metadata": {
        "chunk_index": 30,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 173
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_31",
      "content": "- <span id=\"page-7-4\"></span>1. Ciresan, D.C., Gambardella, L.M., Giusti, A., Schmidhuber, J.: Deep neural networks segment neuronal membranes in electron microscopy images. In: NIPS. pp. 2852–2860 (2012)\n- <span id=\"page-7-8\"></span>2. Dosovitskiy, A., Springenberg, J.T., Riedmiller, M., Brox, T.: Discriminative unsupervised feature learning with convolutional neural networks. In: NIPS (2014)\n- <span id=\"page-7-1\"></span>3. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object detection and semantic segmentation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2014)\n- <span id=\"page-7-6\"></span>4. Hariharan, B., Arbelez, P., Girshick, R., Malik, J.: Hypercolumns for object segmentation and fine-grained localization (2014), arXiv:1411.5752 [cs.CV]",
      "metadata": {
        "chunk_index": 31,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 841
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_32",
      "content": "- <span id=\"page-7-6\"></span>4. Hariharan, B., Arbelez, P., Girshick, R., Malik, J.: Hypercolumns for object segmentation and fine-grained localization (2014), arXiv:1411.5752 [cs.CV]\n- <span id=\"page-7-10\"></span>5. He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification (2015), arXiv:1502.01852 [cs.CV]\n- <span id=\"page-7-9\"></span>6. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., Darrell, T.: Caffe: Convolutional architecture for fast feature embedding (2014), arXiv:1408.5093 [cs.CV]\n- <span id=\"page-7-0\"></span>7. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: NIPS. pp. 1106–1114 (2012)",
      "metadata": {
        "chunk_index": 32,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 778
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_33",
      "content": "- <span id=\"page-7-0\"></span>7. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: NIPS. pp. 1106–1114 (2012)\n- <span id=\"page-7-2\"></span>8. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural Computation 1(4), 541–551 (1989)\n- <span id=\"page-7-7\"></span>9. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic segmentation (2014), arXiv:1411.4038 [cs.CV]\n- <span id=\"page-7-12\"></span>10. Maska, M., (...), de Solorzano, C.O.: A benchmark for comparison of cell tracking algorithms. Bioinformatics 30, 1609–1617 (2014)\n- <span id=\"page-7-5\"></span>11. Seyedhosseini, M., Sajjadi, M., Tasdizen, T.: Image segmentation with cascaded hierarchical models and logistic disjunctive normal networks. In: Computer Vision (ICCV), 2013 IEEE International Conference on. pp. 2168–2175 (2013)",
      "metadata": {
        "chunk_index": 33,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 980
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_34",
      "content": "- <span id=\"page-7-3\"></span>12. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition (2014), arXiv:1409.1556 [cs.CV]\n- <span id=\"page-7-13\"></span>13. WWW: Web page of the cell tracking challenge, [http://www.codesolorzano.com/](http://www.codesolorzano.com/ celltrackingchallenge/Cell_Tracking_Challenge/Welcome.html) [celltrackingchallenge/Cell\\\\_Tracking\\\\_Challenge/Welcome.html](http://www.codesolorzano.com/ celltrackingchallenge/Cell_Tracking_Challenge/Welcome.html)\n- <span id=\"page-7-11\"></span>14. WWW: Web page of the em segmentation challenge, [http://brainiac2.mit.edu/](http://brainiac2.mit.edu/isbi_challenge/) [isbi\\\\_challenge/](http://brainiac2.mit.edu/isbi_challenge/)",
      "metadata": {
        "chunk_index": 34,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 731
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_chunk_35",
      "content": "<span id=\"page-7-14\"></span><sup>4</sup> U-net implementation, trained networks and supplementary material available at <http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net>",
      "metadata": {
        "chunk_index": 35,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 181
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_table_1",
      "content": "| Table 1. Ranking on | the EM segmentation | challenge [14] (march | 6th, 2015), sorted |\n|---------------------|---------------------|-----------------------|--------------------|\n| by warping error.   |                     |                       |                    |\n",
      "metadata": {
        "content_type": "table",
        "table_id": "table_1",
        "headers": [
          "Table 1. Ranking on",
          "the EM segmentation",
          "challenge [14] (march",
          "6th, 2015), sorted"
        ],
        "row_count": 1
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_table_2",
      "content": "| Rank | Group name         | Warping Error | Rand Error | Pixel Error |\n|------|--------------------|---------------|------------|-------------|\n|      | ** human values ** | 0.000005      | 0.0021     | 0.0010      |\n| 1.   | u-net              | 0.000353      | 0.0382     | 0.0611      |\n| 2.   | DIVE-SCI           | 0.000355      | 0.0305     | 0.0584      |\n| 3.   | IDSIA [1]          | 0.000420      | 0.0504     | 0.0613      |\n| 4.   | DIVE               | 0.000430      | 0.0545     | 0.0582      |\n| :    |                    |               |            |             |\n| 10   | IDGIA GGI          | 0.000059      | 0.0100     | 0.1007      |\n| 10.  | IDSIA-SCI          | 0.000653      | 0.0189     | 0.1027      |\n",
      "metadata": {
        "content_type": "table",
        "table_id": "table_2",
        "headers": [
          "Rank",
          "Group name",
          "Warping Error",
          "Rand Error",
          "Pixel Error"
        ],
        "row_count": 8
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_table_3",
      "content": "| PhC-U373 | DIC-HeLa |\n|----------|----------|\n| 0.2669   | 0.2935   |\n| 0.7953   | 0.4607   |\n| 0.5323   | -        |\n| 0.83     | 0.46     |\n| 0.9203   | 0.7756   |\n|          |          |\n",
      "metadata": {
        "content_type": "table",
        "table_id": "table_3",
        "headers": [
          "PhC-U373",
          "DIC-HeLa"
        ],
        "row_count": 6
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_figure_1",
      "content": "<span id=\"page-1-0\"></span>\n\nFigure: Fig. 1.** U-net architecture (example for 32x32 pixels in the lowest resolution). Each blue box corresponds to a multi-channel feature map. The number of channels is denoted on top of the box. The x-y-size is provided at the lower left edge of the box. White boxes represent copied feature maps. The arrows denote the different operations.\n\nVisual Description: The figure illustrates the U-Net architecture designed for processing images of size 32x32 pixels at the lowest resolution. The axes are not explicitly labeled, but the input image tile and output segmentation map dimensions are indicated alongside feature map channels. Key findings from the architecture include a sequence of operations such as convolution, pooling, and up-convolution, which facilitate multi-scale feature extraction and reconstruction, enabling accurate segmentation of the input imagery.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_1",
        "caption": "Fig. 1.** U-net architecture (example for 32x32 pixels in the lowest resolution). Each blue box corresponds to a multi-channel feature map. The number of channels is denoted on top of the box. The x-y-size is provided at the lower left edge of the box. White boxes represent copied feature maps. The arrows denote the different operations.",
        "image_key": "_page_1_Figure_1.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_figure_2",
      "content": "<span id=\"page-2-0\"></span>\n\nFigure: Fig. 2. Overlap-tile strategy for seamless segmentation of arbitrary large images (here segmentation of neuronal structures in EM stacks). Prediction of the segmentation in the yellow area, requires image data within the blue area as input. Missing input data is extrapolated by mirroring\n\nVisual Description: The figure illustrates the overlap-tile strategy for segmenting neuronal structures in electron microscopy (EM) stacks. The left side shows the original image data, with a blue area indicating the input region, while the yellow area represents the segment to be predicted. The right side displays the resulting segmentation, highlighting the key finding that missing input data is effectively extrapolated using image mirroring. This approach allows for seamless segmentation of large images by utilizing overlapping tiles.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_2",
        "caption": "Fig. 2. Overlap-tile strategy for seamless segmentation of arbitrary large images (here segmentation of neuronal structures in EM stacks). Prediction of the segmentation in the yellow area, requires image data within the blue area as input. Missing input data is extrapolated by mirroring",
        "image_key": "_page_2_Picture_1.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_figure_3",
      "content": "<span id=\"page-4-0\"></span>\n\nFigure: Fig. 3. HeLa cells on glass recorded with DIC (differential interference contrast) microscopy. (a) raw image. (b) overlay with ground truth segmentation. Different colors indicate different instances of the HeLa cells. (c) generated segmentation mask (white: foreground, black: background). (d) map with a pixel-wise loss weight to force the network to learn the border pixels.\n\nVisual Description: Figure 3 illustrates the segmentation of HeLa cells observed through differential interference contrast (DIC) microscopy. Panel (a) presents the raw image of the cells, while (b) shows an overlay with ground truth segmentation, using distinct colors to represent individual cell instances. Panel (c) displays the generated segmentation mask, with white indicating cell areas and black the background, and panel (d) provides a pixel-wise loss weight map designed to enhance the network's learning of cell borders, indicating regions of interest for improved segmentation accuracy.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_3",
        "caption": "Fig. 3. HeLa cells on glass recorded with DIC (differential interference contrast) microscopy. (a) raw image. (b) overlay with ground truth segmentation. Different colors indicate different instances of the HeLa cells. (c) generated segmentation mask (white: foreground, black: background). (d) map with a pixel-wise loss weight to force the network to learn the border pixels.",
        "image_key": "_page_4_Figure_1.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "U-Net - Convolutional Networks for Biomedical Image Segmentation.pdf_figure_4",
      "content": "<span id=\"page-6-2\"></span>\n\nFigure: Fig. 4. Result on the ISBI cell tracking challenge. (a) part of an input image of the \"PhC-U373\" data set. (b) Segmentation result (cyan mask) with manual ground truth (yellow border) (c) input image of the \"DIC-HeLa\" data set. (d) Segmentation result (random colored masks) with manual ground truth (yellow border).\n\nVisual Description: Figure 4 presents results from the ISBI cell tracking challenge, showcasing both input images and segmentation outcomes. In part (a), an input image from the \"PhC-U373\" dataset is displayed, while (b) illustrates a segmentation result with the cyan mask representing segmented cells and a yellow border indicating the manual ground truth. Parts (c) and (d) display an input image of the \"DIC-HeLa\" dataset and its corresponding segmentation with random colored masks, respectively, also highlighting the manual ground truth with a yellow border. The figure underscores the effectiveness of the segmentation methods in accurately delineating cell boundaries in different cellular environments.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_4",
        "caption": "Fig. 4. Result on the ISBI cell tracking challenge. (a) part of an input image of the \"PhC-U373\" data set. (b) Segmentation result (cyan mask) with manual ground truth (yellow border) (c) input image of the \"DIC-HeLa\" data set. (d) Segmentation result (random colored masks) with manual ground truth (yellow border).",
        "image_key": "_page_6_Figure_1.jpeg",
        "has_vision_description": true
      }
    }
  ],
  "tables": [
    {
      "table_id": "table_1",
      "headers": [
        "Table 1. Ranking on",
        "the EM segmentation",
        "challenge [14] (march",
        "6th, 2015), sorted"
      ],
      "rows": [
        [
          "by warping error.",
          "",
          "",
          ""
        ]
      ],
      "markdown": "| Table 1. Ranking on | the EM segmentation | challenge [14] (march | 6th, 2015), sorted |\n|---------------------|---------------------|-----------------------|--------------------|\n| by warping error.   |                     |                       |                    |\n"
    },
    {
      "table_id": "table_2",
      "headers": [
        "Rank",
        "Group name",
        "Warping Error",
        "Rand Error",
        "Pixel Error"
      ],
      "rows": [
        [
          "",
          "** human values **",
          "0.000005",
          "0.0021",
          "0.0010"
        ],
        [
          "1.",
          "u-net",
          "0.000353",
          "0.0382",
          "0.0611"
        ],
        [
          "2.",
          "DIVE-SCI",
          "0.000355",
          "0.0305",
          "0.0584"
        ],
        [
          "3.",
          "IDSIA [1]",
          "0.000420",
          "0.0504",
          "0.0613"
        ],
        [
          "4.",
          "DIVE",
          "0.000430",
          "0.0545",
          "0.0582"
        ],
        [
          ":",
          "",
          "",
          "",
          ""
        ],
        [
          "10",
          "IDGIA GGI",
          "0.000059",
          "0.0100",
          "0.1007"
        ],
        [
          "10.",
          "IDSIA-SCI",
          "0.000653",
          "0.0189",
          "0.1027"
        ]
      ],
      "markdown": "| Rank | Group name         | Warping Error | Rand Error | Pixel Error |\n|------|--------------------|---------------|------------|-------------|\n|      | ** human values ** | 0.000005      | 0.0021     | 0.0010      |\n| 1.   | u-net              | 0.000353      | 0.0382     | 0.0611      |\n| 2.   | DIVE-SCI           | 0.000355      | 0.0305     | 0.0584      |\n| 3.   | IDSIA [1]          | 0.000420      | 0.0504     | 0.0613      |\n| 4.   | DIVE               | 0.000430      | 0.0545     | 0.0582      |\n| :    |                    |               |            |             |\n| 10   | IDGIA GGI          | 0.000059      | 0.0100     | 0.1007      |\n| 10.  | IDSIA-SCI          | 0.000653      | 0.0189     | 0.1027      |\n"
    },
    {
      "table_id": "table_3",
      "headers": [
        "PhC-U373",
        "DIC-HeLa"
      ],
      "rows": [
        [
          "0.2669",
          "0.2935"
        ],
        [
          "0.7953",
          "0.4607"
        ],
        [
          "0.5323",
          "-"
        ],
        [
          "0.83",
          "0.46"
        ],
        [
          "0.9203",
          "0.7756"
        ],
        [
          "",
          ""
        ]
      ],
      "markdown": "| PhC-U373 | DIC-HeLa |\n|----------|----------|\n| 0.2669   | 0.2935   |\n| 0.7953   | 0.4607   |\n| 0.5323   | -        |\n| 0.83     | 0.46     |\n| 0.9203   | 0.7756   |\n|          |          |\n"
    }
  ],
  "figures": [
    {
      "figure_id": "figure_1",
      "image_key": "_page_1_Figure_1.jpeg",
      "caption": "Fig. 1.** U-net architecture (example for 32x32 pixels in the lowest resolution). Each blue box corresponds to a multi-channel feature map. The number of channels is denoted on top of the box. The x-y-size is provided at the lower left edge of the box. White boxes represent copied feature maps. The arrows denote the different operations.",
      "alt_text": "",
      "context_before": "<span id=\"page-1-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_2",
      "image_key": "_page_2_Picture_1.jpeg",
      "caption": "Fig. 2. Overlap-tile strategy for seamless segmentation of arbitrary large images (here segmentation of neuronal structures in EM stacks). Prediction of the segmentation in the yellow area, requires image data within the blue area as input. Missing input data is extrapolated by mirroring",
      "alt_text": "",
      "context_before": "<span id=\"page-2-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_3",
      "image_key": "_page_4_Figure_1.jpeg",
      "caption": "Fig. 3. HeLa cells on glass recorded with DIC (differential interference contrast) microscopy. (a) raw image. (b) overlay with ground truth segmentation. Different colors indicate different instances of the HeLa cells. (c) generated segmentation mask (white: foreground, black: background). (d) map with a pixel-wise loss weight to force the network to learn the border pixels.",
      "alt_text": "",
      "context_before": "<span id=\"page-4-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_4",
      "image_key": "_page_6_Figure_1.jpeg",
      "caption": "Fig. 4. Result on the ISBI cell tracking challenge. (a) part of an input image of the \"PhC-U373\" data set. (b) Segmentation result (cyan mask) with manual ground truth (yellow border) (c) input image of the \"DIC-HeLa\" data set. (d) Segmentation result (random colored masks) with manual ground truth (yellow border).",
      "alt_text": "",
      "context_before": "<span id=\"page-6-2\"></span>",
      "context_after": ""
    }
  ],
  "stats": {
    "total_chunks": 43,
    "text_chunks": 36,
    "tables_extracted": 3,
    "images_extracted": 4,
    "figure_chunks_created": 4,
    "figures_with_vision": 4,
    "markdown_chars": 21683,
    "processing_time_seconds": 66.15
  }
}