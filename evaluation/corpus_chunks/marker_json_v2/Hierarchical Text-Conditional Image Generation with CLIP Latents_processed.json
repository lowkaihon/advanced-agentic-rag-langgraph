{
  "source": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf",
  "processor": "marker",
  "processed_date": "2025-12-15T08:02:48.730486",
  "markdown": "# Hierarchical Text-Conditional Image Generation with CLIP Latents\n\nAditya Ramesh<sup>∗</sup> OpenAI aramesh@openai.com\n\nPrafulla Dhariwal<sup>∗</sup> OpenAI prafulla@openai.com\n\nAlex Nichol<sup>∗</sup> OpenAI alex@openai.com\n\nCasey Chu<sup>∗</sup> OpenAI casey@openai.com\n\nMark Chen OpenAI mark@openai.com\n\n# Abstract\n\nContrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.\n\n# 1 Introduction\n\nRecent progress in computer vision has been driven by scaling models on large datasets of captioned images collected from the internet [\\[10,](#page-18-0) [44,](#page-20-0) [60,](#page-21-0) [39,](#page-20-1) [31,](#page-19-0) [16\\]](#page-19-1). Within this framework, CLIP [\\[39\\]](#page-20-1) has emerged as a successful representation learner for images. CLIP embeddings have a number of desirable properties: they are robust to image distribution shift, have impressive zero-shot capabilities, and have been fine-tuned to achieve state-of-the-art results on a wide variety of vision and language tasks [\\[45\\]](#page-20-2). Concurrently, diffusion models [\\[46,](#page-20-3) [48,](#page-20-4) [25\\]](#page-19-2) have emerged as a promising generative modeling framework, pushing the state-of-the-art on image and video generation tasks [\\[11,](#page-18-1) [26,](#page-19-3) [24\\]](#page-19-4). To achieve best results, diffusion models leverage a guidance technique [\\[11,](#page-18-1) [24\\]](#page-19-4) which improves sample fidelity (for images, photorealism) at the cost of sample diversity.\n\nIn this work, we combine these two approaches for the problem of text-conditional image generation. We first train a diffusion *decoder* to invert the CLIP image *encoder*. Our inverter is non-deterministic, and can produce multiple images corresponding to a given image embedding. The presence of an encoder and its approximate inverse (the decoder) allows for capabilities beyond text-to-image translation. As in GAN inversion [\\[62,](#page-21-1) [55\\]](#page-21-2), encoding and decoding an input image produces semantically similar output images (Figure [3\\)](#page-4-0). We can also interpolate between input images by inverting interpolations of their image embeddings (Figure [4\\)](#page-5-0). However, one notable advantage of using the CLIP latent space is the ability to semantically modify images by moving in the direction of any encoded text vector (Figure [5\\)](#page-6-0), whereas discovering these directions in GAN latent space involves\n\n<sup>∗</sup>Equal contribution\n\n<span id=\"page-1-0\"></span>![](_page_1_Picture_0.jpeg)\n\nvibrant portrait painting of Salvador Dalí with a robotic half face a shiba inu wearing a beret and black turtleneck a close up of a handpalm with leaves growing from it\n\n![](_page_1_Picture_2.jpeg)\n\n![](_page_1_Picture_4.jpeg)\n\n![](_page_1_Picture_6.jpeg)\n\nan espresso machine that makes coffee from human souls, artstation panda mad scientist mixing sparkling chemicals, artstation a corgi's head depicted as an explosion of a nebula\n\n![](_page_1_Picture_8.jpeg)\n\n![](_page_1_Picture_10.jpeg)\n\n![](_page_1_Picture_12.jpeg)\n\n![](_page_1_Picture_14.jpeg)\n\na dolphin in an astronaut suit on saturn, artstation a propaganda poster depicting a cat dressed as french emperor napoleon holding a piece of cheese a teddy bear on a skateboard in times square\n\n![](_page_1_Picture_16.jpeg)\n\nFigure 1: Selected 1024 × 1024 samples from a production version of our model.\n\n![](_page_2_Picture_0.jpeg)\n\nFigure 2: A high-level overview of unCLIP. Above the dotted line, we depict the CLIP training process, through which we learn a joint representation space for text and images. Below the dotted line, we depict our text-to-image generation process: a CLIP text embedding is first fed to an autoregressive or diffusion prior to produce an image embedding, and then this embedding is used to condition a diffusion decoder which produces a final image. Note that the CLIP model is frozen during training of the prior and decoder.\n\nluck and diligent manual examination. Furthermore, encoding and decoding images also provides us with a tool for observing which features of the image are recognized or disregarded by CLIP.\n\nTo obtain a full generative model of images, we combine the CLIP image embedding *decoder* with a *prior* model, which generates possible CLIP image embeddings from a given text caption. We compare our text-to-image system with other systems such as DALL-E [\\[40\\]](#page-20-5) and GLIDE [\\[35\\]](#page-20-6), finding that our samples are comparable in quality to GLIDE, but with greater diversity in our generations. We also develop methods for training diffusion priors in latent space, and show that they achieve comparable performance to autoregressive priors, while being more compute-efficient. We refer to our full text-conditional image generation stack as *unCLIP*, since it generates images by inverting the CLIP image encoder.\n\n## 2 Method\n\nOur training dataset consists of pairs (x, y) of images x and their corresponding captions y. Given an image x, let z<sup>i</sup> and z<sup>t</sup> be its CLIP image and text embeddings, respectively. We design our generative stack to produce images from captions using two components:\n\n- A *prior* P(z<sup>i</sup> |y) that produces CLIP image embeddings z<sup>i</sup> conditioned on captions y.\n- A *decoder* P(x|z<sup>i</sup> , y) that produces images x conditioned on CLIP image embeddings z<sup>i</sup> (and optionally text captions y).\n\nThe decoder allows us to invert images given their CLIP image embeddings, while the prior allows us to learn a generative model of the image embeddings themselves. Stacking these two components yields a generative model P(x|y) of images x given captions y:\n\n$$P(x|y) = P(x, z_i|y) = P(x|z_i, y)P(z_i|y).$$\n\nThe first equality holds because z<sup>i</sup> is a deterministic function of x. The second equality holds because of the chain rule. Thus, we can sample from the true conditional distribution P(x|y) by first sampling z<sup>i</sup> using the prior, and then sampling x using the decoder. In the following sections, we describe our decoder and prior stacks. For training details and hyperparameters, refer to Appendix [C.](#page-22-0)\n\n#### 2.1 Decoder\n\nWe use diffusion models [\\[25,](#page-19-2) [48\\]](#page-20-4) to produce images conditioned on CLIP image embeddings (and optionally text captions). Specifically, we modify the architecture described in [Nichol et al.](#page-20-6) [\\(2021\\)](#page-20-6) by projecting and adding CLIP embeddings to the existing timestep embedding, and by projecting CLIP embeddings into four extra tokens of context that are concatenated to the sequence of outputs from the GLIDE text encoder. We retained the text conditioning pathway present in the original GLIDE model, hypothesizing that it could allow the diffusion model to learn aspects of natural language that CLIP fails to capture (e.g. variable binding), but find that it offers little help in this regard (Section [7\\)](#page-15-0).\n\nWhile we can sample from the conditional distribution of the decoder directly, past work using diffusion models shows using guidance on the conditioning information [\\[11,](#page-18-1) [24,](#page-19-4) [35\\]](#page-20-6) improves sample quality a lot. We enable classifier-free guidance [\\[24\\]](#page-19-4) by randomly setting the CLIP embeddings to zero (or a learned embedding) 10% of the time, and randomly dropping the text caption 50% of the time during training.\n\nTo generate high resolution images, we train two diffusion upsampler models [\\[34,](#page-20-7) [43\\]](#page-20-8): one to upsample images from 64×64 to 256×256 resolution, and another to further upsample those to 1024×1024 resolution. To improve the robustness of our upsamplers, we slightly corrupt the conditioning images during training. For the first upsampling stage, we use gaussian blur [\\[43\\]](#page-20-8), and for the second, we use a more diverse BSR degradation [\\[42,](#page-20-9) [59\\]](#page-21-3). To reduce training compute and improve numerical stability, we follow [Rombach et al.](#page-20-9) [\\[42\\]](#page-20-9) and train on random crops of images that are one-fourth the target size. We use only spatial convolutions in the model (i.e., no attention layers) and at inference time directly apply the model at the target resolution, observing that it readily generalizes to the higher resolution. We found no benefit from conditioning the upsamplers on the caption, and use unconditional ADMNets [\\[11\\]](#page-18-1) with no guidance.\n\n#### 2.2 Prior\n\nWhile a decoder can invert CLIP image embeddings z<sup>i</sup> to produce images x, we need a prior model that produces z<sup>i</sup> from captions y to enable image generations from text captions. We explore two different model classes for the prior model:\n\n- *Autoregressive (AR)* prior: the CLIP image embedding z<sup>i</sup> is converted into a sequence of discrete codes and predicted autoregressively conditioned on the caption y.\n- *Diffusion* prior: The continuous vector z<sup>i</sup> is directly modelled using a Gaussian diffusion model conditioned on the caption y.\n\nIn addition to the caption, we can condition the prior on the CLIP text embedding z<sup>t</sup> since it is a deterministic function of the caption. To improve sample quality we also enable sampling using classifier-free guidance for both the AR and diffusion prior, by randomly dropping this text conditioning information 10% of the time during training.\n\nTo train and sample from the AR prior more efficiently, we first reduce the dimensionality of the CLIP image embeddings z<sup>i</sup> by applying Principal Component Analysis (PCA) [\\[37\\]](#page-20-10). In particular, we find that the rank of the CLIP representation space is drastically reduced when training CLIP with SAM [\\[15\\]](#page-18-2) while slightly improving evaluation metrics. We are able to preserve nearly all of the information[2](#page-3-0) by retaining only 319 principal components out of the original 1,024. After applying PCA, we order the principal components by decreasing eigenvalue magnitude, quantize each of the 319 dimensions into 1,024 discrete buckets, and\n\n<span id=\"page-3-0\"></span><sup>2</sup> I.e., less than 1% average mean-squared error in reconstructing the image representations.\n\n<span id=\"page-4-0\"></span>![](_page_4_Picture_0.jpeg)\n\nFigure 3: Variations of an input image by encoding with CLIP and then decoding with a diffusion model. The variations preserve both semantic information like presence of a clock in the painting and the overlapping strokes in the logo, as well as stylistic elements like the surrealism in the painting and the color gradients in the logo, while varying the non-essential details.\n\npredict the resulting sequence using a Transformer [\\[53\\]](#page-21-4) model with a causal attention mask. This results in a threefold reduction in the number of tokens predicted during inference, and improves training stability.\n\nWe condition the AR prior on the text caption and the CLIP text embedding by encoding them as a prefix to the sequence. Additionally, we prepend a token indicating the (quantized) dot product between the text embedding and image embedding, z<sup>i</sup> · zt. This allows us to condition the model on a higher dot product, since higher text-image dot products correspond to captions which better describe the image. In practice, we find it beneficial to sample the dot product from the top half of the distribution.[3](#page-4-1)\n\nFor the diffusion prior, we train a decoder-only Transformer with a causal attention mask on a sequence consisting of, in order: the encoded text, the CLIP text embedding, an embedding for the diffusion timestep, the noised CLIP image embedding, and a final embedding whose output from the Transformer is used to predict the unnoised CLIP image embedding. We choose not to condition the diffusion prior on z<sup>i</sup> · z<sup>t</sup> like in the AR prior; instead, we improve quality during sampling time by generating two samples of z<sup>i</sup> and selecting the one with a higher dot product with zt. Instead of using the -prediction formulation from [Ho et al.](#page-19-2) [\\[25\\]](#page-19-2), we find it better to train our model to predict the unnoised z<sup>i</sup> directly, and use a mean-squared error loss on this prediction:\n\n$$L_{\\text{prior}} = \\mathbb{E}_{t \\sim [1, T], z_i^{(t)} \\sim q_t} \\left[ \\| f_{\\theta}(z_i^{(t)}, t, y) - z_i \\|^2 \\right]$$\n\n<span id=\"page-4-1\"></span><sup>3</sup>We swept over percentiles 50%, 70%, 85%, 95% and found 50% to be optimal in all experiments.\n\n<span id=\"page-5-0\"></span>![](_page_5_Picture_0.jpeg)\n\nFigure 4: Variations between two images by interpolating their CLIP image embedding and then decoding with a diffusion model. We fix the decoder seed across each row. The intermediate variations naturally blend the content and style from both input images.\n\n## 3 Image Manipulations\n\nOur approach allows us to encode any given image x into a bipartite latent representation (z<sup>i</sup> , x<sup>T</sup> ) that is sufficient for the decoder to produce an accurate reconstruction. The latent z<sup>i</sup> describes the aspects of the image that are recognized by CLIP, while the latent x<sup>T</sup> encodes all of the residual information necessary for the decoder to reconstruct x. The former is obtained by simply encoding the image with the CLIP image encoder. The latter is obtained by applying DDIM inversion (Appendix F in [\\[11\\]](#page-18-1)) to x using the decoder, while conditioning on z<sup>i</sup> . We describe three different kinds of manipulations that are enabled by this bipartite representation.\n\n## 3.1 Variations\n\nGiven an image x, we can produce related images that share the same essential content but vary in other apects, such as shape and orientation (Figure [3\\)](#page-4-0). To do this, we apply the decoder to the bipartite representation (z<sup>i</sup> , x<sup>T</sup> ) using DDIM with η > 0 for sampling. With η = 0, the decoder becomes deterministic and will reconstruct the given image x. Larger values of η introduce stochasticity into successive sampling steps, resulting in variations that are perceptually \"centered\" around the original image x. As η increases, these variations tell us what information was captured in the CLIP image embedding (and thus is preserved across samples), and what was lost (and thus changes across the samples).\n\n<span id=\"page-6-0\"></span>![](_page_6_Picture_0.jpeg)\n\na photo of a cat  $\\rightarrow$  an anime drawing of a super saiyan cat, artstation\n\n![](_page_6_Picture_2.jpeg)\n\na photo of a victorian house  $\\rightarrow$  a photo of a modern house\n\n![](_page_6_Picture_4.jpeg)\n\na photo of an adult lion  $\\rightarrow$  a photo of lion cub\n\n![](_page_6_Picture_6.jpeg)\n\na photo of a landscape in winter  $\\rightarrow$  a photo of a landscape in fall\n\nFigure 5: Text diffs applied to images by interpolating between their CLIP image embeddings and a normalised difference of the CLIP text embeddings produced from the two descriptions. We also perform DDIM inversion to perfectly reconstruct the input image in the first column, and fix the decoder DDIM noise across each row.\n\n#### 3.2 Interpolations\n\nIt is also possible to blend two images  $x_1$  and  $x_2$  for variations (Figure 4), traversing all of the concepts in CLIP's embedding space that occur between them. To do this, we rotate between their CLIP embeddings  $z_{i_1}$  and  $z_{i_2}$  using spherical interpolation, yielding intermediate CLIP representations  $z_{i_{\\theta}} = \\text{slerp}(z_{i_1}, z_{i_2}, \\theta)$  as  $\\theta$  is varied from 0 to 1. There are two options for producing the intermediate DDIM latents along the trajectory. The first option involves interpolating between their DDIM inverted latents  $x_{T_1}$  and  $x_{T_2}$  (by setting  $x_{T_{\\theta}} = \\text{slerp}(x_{T_1}, x_{T_2}, \\theta)$ ), which yields a single trajectory whose endpoints reconstruct  $x_1$  and  $x_2$ . The second option involves fixing the DDIM latent to a randomly-sampled value for all interpolates in the trajectory. This results in an infinite number of trajectories between  $x_1$  and  $x_2$ , though the endpoints of these trajectories will generally no longer coincide with the original images. We use this approach in Figure 4.\n\n#### 3.3 Text Diffs\n\nA key advantage of using CLIP compared to other models for image representations is that it embeds images and text to the same latent space, thus allowing us to apply language-guided image manipulations (i.e., text diffs), which we show in Figure 5. To modify the image to reflect a new text description y, we first obtain its CLIP text embedding  $z_t$ , as well as the CLIP text embedding  $z_{t_0}$  of a caption describing the current image<sup>4</sup>. We then compute a *text diff* vector  $z_d = \\text{norm}(z_t - z_{t_0})$  from these by taking their difference and\n\n<span id=\"page-6-1\"></span><sup>&</sup>lt;sup>4</sup>Instead of a description of the current image, we also experimented with using a dummy caption like \"a photo\" for the baseline, or removing it altogether. These also worked well.\n\n<span id=\"page-7-0\"></span>![](_page_7_Picture_0.jpeg)\n\nFigure 6: Variations of images featuring typographic attacks [\\[20\\]](#page-19-5) paired with the CLIP model's predicted probabilities across three labels. Surprisingly, the decoder still recovers Granny Smith apples even when the predicted probability for this label is near 0%. We also find that our CLIP model is slightly less susceptible to the \"pizza\" attack than the models investigated in [\\[20\\]](#page-19-5).\n\nnormalizing. Now, we can rotate between the image CLIP embedding z<sup>i</sup> and the text diff vector z<sup>d</sup> using spherical interpolation, yielding intermediate CLIP representations z<sup>θ</sup> = slerp(z<sup>i</sup> , zd, θ), where θ is increased linearly from 0 to a maximum value that is typically in [0.25, 0.50]. We produce the final outputs by decoding the interpolates zθ, fixing the base DDIM noise to x<sup>T</sup> throughout the entire trajectory.\n\n## <span id=\"page-7-1\"></span>4 Probing the CLIP Latent Space\n\nOur decoder model provides a unique opportunity to explore CLIP latent space by allowing us to directly visualize what the CLIP image encoder is seeing. As an example use case, we can revisit cases where CLIP makes incorrect predictions, such as typographic attacks [\\[20\\]](#page-19-5). In these adversarial images, a piece of text is overlayed on top of an object, which causes CLIP to predict the object described by the text rather than the object depicted in the image. This piece of text essentially hides the original object in terms of output probabilities. In Figure [6,](#page-7-0) we show an example of this attack from [\\[20\\]](#page-19-5), wherein an apple can be misclassified as an iPod. Surprisingly, we find that our decoder still generates pictures of apples with high probability even though the predicted probability of \"Granny Smith\" is near zero. Even more notable, the model never produces pictures of iPods, despite the very high relative predicted probability of this caption.\n\n<span id=\"page-8-0\"></span>![](_page_8_Picture_0.jpeg)\n\nFigure 7: Visualization of reconstructions of CLIP latents from progressively more PCA dimensions (20, 30, 40, 80, 120, 160, 200, 320 dimensions), with the original source image on the far right. The lower dimensions preserve coarse-grained semantic information, whereas the higher dimensions encode finer-grained details about the exact form of the objects in the scene.\n\nPCA reconstructions offer another tool for probing the structure of the CLIP latent space. In Figure [7,](#page-8-0) we take the CLIP image embeddings of a handful of source images and reconstruct them with progressively more PCA dimensions, and then visualize the reconstructed image embeddings using our decoder with DDIM on a fixed seed. This allows us to see what semantic information the different dimensions encode. We observe that the early PCA dimensions preserve coarse-grained semantic information such as what types of objects are in the scene, whereas the later PCA dimensions encode finer-grained detail such as the shapes and exact form of the objects. For example, in the first scene, the earlier dimensions seem to encode that there is food and perhaps a container present, whereas the later dimensions encode tomatoes and a bottle specifically. Figure [7](#page-8-0) also serves as a visualization of what the AR prior is modeling, since the AR prior is trained to explicitly predict these principal components in this order.\n\n## 5 Text-to-Image Generation\n\n#### 5.1 Importance of the Prior\n\nAlthough we train a prior to generate CLIP image embeddings from captions, the prior is not strictly necessary for caption-to-image generation. For instance, our decoder can condition on both CLIP image embeddings and captions, but the CLIP image embedding is dropped 5% of the time during training in order to enable classifier-free guidance. Therefore, at sampling time, we can condition on only the caption, although this underperforms a model trained fully in this way (this model is GLIDE, and we do a thorough comparison with GLIDE in Sections [5.2](#page-9-0) and [5.3\\)](#page-11-0). Another possibility is to feed the decoder the CLIP text embedding as if it were an image embedding, as previously observed [\\[61,](#page-21-5) [54\\]](#page-21-6). The first two rows of Figure [8](#page-9-1) depicts samples obtained in these two ways; the third row depicts samples obtained with a prior. Conditioning the decoder on just the caption is clearly worst, but conditioning on text embeddings zero-shot does produce reasonable results. Building on this observation, another approach would be to train the decoder to condition on CLIP text embeddings [\\[9\\]](#page-18-3) instead of CLIP image embeddings (although we would lose the capabilities mentioned in Section [4\\)](#page-7-1).\n\nTo quantify the effectiveness of these alternate approaches, we train two models: a small decoder conditioned on CLIP text embeddings, and a small unCLIP stack (diffusion prior and decoder). We then compare samples from the text-embedding decoder, samples from the unCLIP stack, and samples obtained from feeding text\n\n<span id=\"page-9-1\"></span>![](_page_9_Figure_0.jpeg)\n\nFigure 8: Samples using different conditioning signals for the *same* decoder. In the first row, we pass the text caption to the decoder, and pass a zero vector for the CLIP embedding. In the second row, we pass both the text caption and the CLIP text embedding of the caption. In the third row, we pass the text and a CLIP image embedding generated by an autoregressive prior for the given caption. Note that this decoder is only trained to do the text-to-image generation task (without the CLIP image representation) 5% of the time.\n\nembeddings to the unCLIP decoder zero-shot, sweeping across guidance scales for all models. We find that these approaches respectively score FIDs of 9.16, 7.99, and 16.55 on a test set, suggesting the unCLIP approach is best. We also run human evaluations comparing the first two settings, sweeping over sampling hyperparameters for each using our human evaluation proxy model (Appendix [A\\)](#page-22-1). We find that humans prefer the full unCLIP stack 57.0% ± 3.1% of the time for photorealism and 53.1% ± 3.1% of the time for caption similarity.\n\nGiven the importance of the prior, it is worth evaluating different approaches for training it. We compare both the AR and diffusion priors throughout our experiments. In all cases (Sections [5.2,](#page-9-0) [5.4,](#page-12-0) and [5.5\\)](#page-12-1), we find that the diffusion prior outperforms the AR prior for comparable model size and reduced training compute.\n\n## <span id=\"page-9-0\"></span>5.2 Human Evaluations\n\nWe observe in Figure [1](#page-1-0) that unCLIP is capable of synthesizing complex, realistic images. While we can compare sample quality to past models using FID, it is not always aligned with human judgment. To better gauge the generation capabilities of our system, we conduct systematic human evaluations comparing unCLIP to GLIDE for photorealism, caption similarity, and sample diversity.\n\nWe follow the protocol of [Ramesh et al.,](#page-20-5) [Nichol et al.](#page-20-6) [\\[40,](#page-20-5) [35\\]](#page-20-6) for the first two evaluations: for photorealism, users are presented with pairs of images and must choose which looks more photorealistic; for caption\n\n<span id=\"page-10-1\"></span>![](_page_10_Figure_0.jpeg)\n\nFigure 9: Samples when increasing guidance scale for both unCLIP and GLIDE, using the prompt, \"A green vase filled with red roses sitting on top of table.\" For unCLIP, we fix the latent vectors sampled from the prior, and only vary the guidance scale of the decoder. For both models, we fix the diffusion noise seed for each column. Samples from unCLIP improve in quality (more realistic lighting and shadows) but do not change in content as we increase guidance scale, preserving semantic diversity even at high decoder guidance scales.\n\n| unCLIP Prior | Photorealism | Caption Similarity | Diversity    |\n|--------------|--------------|--------------------|--------------|\n| AR           | 47.1% ± 3.1% | 41.1% ± 3.0%       | 62.6% ± 3.0% |\n| Diffusion    | 48.9% ± 3.1% | 45.3% ± 3.0%       | 70.5% ± 2.8% |\n\n<span id=\"page-10-0\"></span>Table 1: Human evaluations comparing unCLIP to GLIDE. We compare to both the AR and diffusion prior for unCLIP. Reported figures are 95% confidence intervals of the probability that the unCLIP model specified by the row beats GLIDE. Sampling hyperparameters for all models were swept to optimize an automated proxy for human photorealism evaluations.\n\nsimilarity, users are additionally prompted with a caption, and must choose which image better matches the caption. In both evaluations, there is a third \"Not sure\" option. For diversity, we propose a new evaluation protocol in which humans are presented with two 4 × 4 grids of samples and must choose which is more diverse (with a third option, \"Not sure\"). For this evaluation, we produce sample grids using 1,000 captions from the MS-COCO validation set, and always compare sample grids for the same caption. Before running human comparisons, we swept over sampling hyperparameters for each model using a CLIP linear probe trained to be a proxy for human photorealism evaluations (Appendix [A\\)](#page-22-1). These hyperparameters are fixed across all three types of evaluation.\n\nWe present our results in Table [1.](#page-10-0) In general, the diffusion prior performs better than the AR prior in pairwise comparisons against GLIDE. We find that humans still slightly prefer GLIDE to unCLIP in terms of photorealism, but the gap is very small. Even with similar photorealism, unCLIP is strongly preferred over GLIDE in terms of diversity, highlighting one of its benefits.\n\n<span id=\"page-11-1\"></span>![](_page_11_Figure_0.jpeg)\n\n<span id=\"page-11-2\"></span>Figure 10: When comparing unCLIP (with our best sampling settings) to various settings of guidance scale for GLIDE, unCLIP was preferred by human evaluators on at least one axis among photorealism, caption similarity, and diversity for each comparison. At the higher guidance scales used to generate photorealistic images, unCLIP yields greater diversity for comparable photorealism and caption similarity.\n\n![](_page_11_Figure_2.jpeg)\n\nFigure 11: FID versus guidance scale for unCLIP and GLIDE. For the unCLIP priors, we swept over sampling hyperparameters and fixed to the settings with the best minimum FID.\n\n#### <span id=\"page-11-0\"></span>5.3 Improved Diversity-Fidelity Trade-off with Guidance\n\nCompared to GLIDE, we qualitatively observe that unCLIP is able to generate more diverse images while leveraging the guidance technique to improve sample quality. To understand why, consider Figure 9 where we increase guidance scale for both GLIDE and unCLIP. For GLIDE, the semantics (camera angle, color, size) converge as we increase guidance scale, whereas for unCLIP the semantic information of the scene is frozen in the CLIP image embedding and therefore does not collapse when guiding the decoder.\n\nIn Section 5.2, we observed that unCLIP achieves similar photorealism as GLIDE while maintaining more diversity, but that its caption matching capabilities were slightly worse. It is natural to ask whether GLIDE's guidance scale can be lowered to obtain the same diversity level as unCLIP while maintaining better caption\n\n<span id=\"page-12-2\"></span>\n\n| Model                             | FID   | Zero-shot FID | Zero-shot FID (filt) |  |\n|-----------------------------------|-------|---------------|----------------------|--|\n| AttnGAN (Xu et al., 2017)         | 35.49 |               |                      |  |\n| DM-GAN (Zhu et al., 2019)         | 32.64 |               |                      |  |\n| DF-GAN (Tao et al., 2020)         | 21.42 |               |                      |  |\n| DM-GAN + CL (Ye et al., 2021)     | 20.79 |               |                      |  |\n| XMC-GAN (Zhang et al., 2021)      | 9.33  |               |                      |  |\n| LAFITE (Zhou et al., 2021)        | 8.12  |               |                      |  |\n| Make-A-Scene (Gafni et al., 2022) | 7.55  |               |                      |  |\n| DALL-E (Ramesh et al., 2021)      |       | ∼ 28          |                      |  |\n| LAFITE (Zhou et al., 2021)        |       | 26.94         |                      |  |\n| GLIDE (Nichol et al., 2021)       |       | 12.24         | 12.89                |  |\n| Make-A-Scene (Gafni et al., 2022) |       |               | 11.84                |  |\n| unCLIP (AR prior)                 |       | 10.63         | 11.08                |  |\n| unCLIP (Diffusion prior)          |       | 10.39         | 10.87                |  |\n\nTable 2: Comparison of FID on MS-COCO 256 × 256. We use guidance scale 1.25 for the decoder for both the AR and diffusion prior, and achieve the best results using the diffusion prior.\n\nmatching. In Figure [10,](#page-11-1) we conduct a more careful study of this question by performing human evaluations across several GLIDE guidance scales. We find that GLIDE at guidance scale 2.0 is very close to the photorealism and caption similarity of unCLIP, while still producing less diverse samples.\n\nFinally, in Figure [11](#page-11-2) we compute MS-COCO zero-shot FID [\\[23\\]](#page-19-7) while sweeping over guidance scale for both unCLIP and GLIDE, finding that guidance hurts the FID of unCLIP much less so than for GLIDE. In this evaluation, we fix the guidance scale of the unCLIP prior and only vary the guidance scale of the decoder. This is another indication that guidance hurts the diversity of GLIDE much more than unCLIP, since FID heavily penalizes non-diverse generations.\n\n#### <span id=\"page-12-0\"></span>5.4 Comparison on MS-COCO\n\nIn the text-conditional image generation literature, it has become standard practice to evaluate FID on the MS-COCO [\\[28\\]](#page-19-8) validation set. We present results on this benchmark in Table [2.](#page-12-2) Like GLIDE and DALL-E, unCLIP is not directly trained on the MS-COCO training set, but can still generalize to the validation set zero-shot. We find that, compared to these other zero-shot models, unCLIP achieves a new state-of-the-art FID of 10.39 when sampling with the diffusion prior. In Figure [12,](#page-13-0) we visually compare unCLIP to various recent text-conditional image generation models on several captions from MS-COCO. We find that, like the other methods, unCLIP produces realistic scenes that capture the text prompts.\n\n#### <span id=\"page-12-1\"></span>5.5 Aesthetic Quality Comparison\n\nWe additionally perform automated aesthetic quality evaluations comparing unCLIP to GLIDE. Our goal with this evaluation is to assess how well each model produces artistic illustrations and photographs. To this end, we generated 512 \"artistic\" captions using GPT-3 [\\[4\\]](#page-18-4) by prompting it with captions for existing artwork (both real and AI generated). Next, we trained a CLIP linear probe to predict human aesthetic judgments using the AVA dataset [\\[33\\]](#page-20-12) (Appendix [A\\)](#page-22-1). For each model and set of sampling hyperparameters, we produce four images for each prompt, and report the mean predicted aesthetic judgment over the full batch of 2048 images.\n\nIn Figure [13,](#page-14-0) we present results on our aesthetic quality evaluation. We find that guidance improves aesthetic quality for both GLIDE and unCLIP. For unCLIP, we only guide the decoder (we found that guiding the prior hurt results). We also plot the aesthetic quality against Recall[5](#page-12-3) , since guidance typically induces a trade-off\n\n<span id=\"page-12-3\"></span><sup>5</sup>Recall is computed with respect to the training dataset.\n\n<span id=\"page-13-0\"></span>![](_page_13_Figure_0.jpeg)\n\nFigure 12: Random image samples on MS-COCO prompts.\n\n<span id=\"page-14-0\"></span>![](_page_14_Figure_0.jpeg)\n\nFigure 13: Aesthetic quality evaluations comparing GLIDE and unCLIP using 512 auto-generated artistic prompts. We find that both models benefit from guidance, but unCLIP does not sacrifice recall for aesthetic quality.\n\nbetween fidelity and diversity. Interestingly, we find that guiding unCLIP does not decrease Recall while still improving aesthetic quality according to this metric.\n\n#### 6 Related Work\n\nSynthetic image generation is a well studied problem, and most popular techniques for unconditional image generation have also been applied to the text-conditional setting. Many previous works have trained GANs [21] on publicly available image captioning datasets to produce text-conditional image samples [56, 63, 49, 58, 57]. Other works have adapted the VQ-VAE approach [52] to text-conditional image generation by training autoregressive transformers on sequences of text tokens followed by image tokens [40, 12, 1]. Finally, some works have applied diffusion models to the problem, training either continuous [35] or discrete [22] diffusion models with auxiliary text encoders to handle textual input.\n\nPrevious works have leveraged hierarchical generative processes to create high-quality synthetic images. Razavi et al. [41] trains a multi-layer discrete autoencoder, allowing them to first sample coarse-grained latent codes and then use this as conditioning information when sampling higher-resolution latent codes. Child, Vahdat and Kautz [5, 50] generate images using VAEs with a hierarchy of latent codes that increase progressively with resolution. Concurrently with our work, Gafni et al. [17] conditions a generative image model on segmentation masks, allowing for a generative process that first samples a semantic map of an image and then conditions the generated image on this information.\n\nThe computational benefits of using diffusion to model a latent space has been noted by previous works. Preechakul et al. [38] propose an autoencoder framework where diffusion models are used to render latent variables as images, and a second diffusion model is used to generate these latents (similar to our diffusion prior). Vahdat et al. [51] use a score-based model for the latent space of a VAE, while Rombach et al. [42] use diffusion models on the latents obtained from a VQGAN [14] like autoencoder.\n\nSince its release, CLIP [39] has been used extensively to steer generative image models towards text prompts. Galatolo et al., Patashnik et al., Murdock, Gal et al. [19, 36, 32, 18] guide GANs using gradients from a CLIP model. For diffusion models, Dhariwal and Nichol [11] introduced classifier guidance as a way to use gradients from a classifier trained on noised images to steer the model towards higher quality generations. Nichol et al. [35] train a CLIP model on noised images and guide a text-conditional diffusion model, while Crowson, Crowson [7, 8] use an unnoised CLIP model to guide unconditional or class-conditional diffusion models. Ho and Salimans [24] introduced classifier-free guidance and showed that one can perform guidance\n\n<span id=\"page-15-1\"></span>![](_page_15_Figure_0.jpeg)\n\nFigure 14: Samples from unCLIP and GLIDE for the prompt \"a red cube on top of a blue cube\".\n\nimplictly from the predictions of the model with and without the conditioning information, thus removing the need for a classifier. [Nichol et al.](#page-20-6) [\\[35\\]](#page-20-6) showed classifier-free guidance works more favorably than CLIP guidance for text conditional image generation.\n\nSeveral previous works have trained generative image models that are directly conditioned on CLIP embeddings. [Zhou et al.](#page-21-5) [\\[61\\]](#page-21-5) condition GAN models on randomly perturbed CLIP image embeddings, finding that these models can generalize to CLIP text embeddings to produce text-conditional images. [Crowson](#page-18-3) [\\[9\\]](#page-18-3) trained diffusion models conditioned on CLIP text embeddings, allowing for direct text-conditional image generation. [Wang et al.](#page-21-6) [\\[54\\]](#page-21-6) train an autoregressive generative model conditioned on CLIP image embeddings, finding that it generalizes to CLIP text embeddings well enough to allow for text-conditional image synthesis.\n\n[Bordes et al.](#page-18-11) [\\[3\\]](#page-18-11) train diffusion models conditioned on image representations from contrastive models. While the diffusion models themselves cannot generate images unconditionally, the authors experimented with a simple approach for two-stage image generation by employing Kernel Density Estimation to sample image representations. By feeding these generated representations to the diffusion model, they can generate images end-to-end in a way similar to our proposed technique. However, our work differs from this in two ways: first, we use multimodal contrastive representations rather than image-only representations; second, we employ much more powerful generative models for the first stage of the generation hierarchy, and these generative models are conditioned on text.\n\n# <span id=\"page-15-0\"></span>7 Limitations and Risks\n\nAlthough conditioning image generation on CLIP embeddings improves diversity, this choice does come with certain limitations. In particular, unCLIP is worse at binding attributes to objects than a corresponding GLIDE model. In Figure [14,](#page-15-1) we find that unCLIP struggles more than GLIDE with a prompt where it must bind two separate objects (cubes) to two separate attributes (colors). We hypothesize that this occurs because the CLIP embedding itself does not explicitly bind attributes to objects, and find that reconstructions from the decoder often mix up attributes and objects, as shown in Figure [15.](#page-16-0) A similar and likely related issue is that unCLIP\n\n<span id=\"page-16-0\"></span>![](_page_16_Picture_0.jpeg)\n\nFigure 15: Reconstructions from the decoder for difficult binding problems. We find that the reconstructions mix up objects and attributes. In the first two examples, the model mixes up the color of two objects. In the rightmost example, the model does not reliably reconstruct the relative size of two objects.\n\n<span id=\"page-16-1\"></span>![](_page_16_Picture_2.jpeg)\n\nFigure 16: Samples from unCLIP for the prompt, \"A sign that says deep learning.\"\n\nstruggles at producing coherent text, as illustrated in Figure [16;](#page-16-1) it is possible that the CLIP embedding does not precisely encode spelling information of rendered text. This issue is likely made worse because the BPE encoding we use obscures the spelling of the words in a caption from the model, so the model needs to have independently seen each token written out in the training images in order to learn to render it.\n\nWe also note that our stack still has a hard time producing details in complex scenes (Figure [17\\)](#page-17-0). We hypothesize that this is a limitation of our decoder hierarchy producing an image at a base resolution of 64 × 64 and then upsampling it. Training our unCLIP decoder at a higher base resolution should be able to alleviate this, at the cost of additional training and inference compute.\n\nAs discussed in the GLIDE paper, image generation models carry risks related to deceptive and otherwise harmful content. unCLIP's performance improvements also raise the risk profile over GLIDE. As the technology matures, it leaves fewer traces and indicators that outputs are AI-generated, making it easier to mistake generated images for authentic ones and vice versa. More research is also needed on how the change in architecture changes how the model learns biases in training data.\n\n<span id=\"page-17-0\"></span>![](_page_17_Picture_0.jpeg)\n\n(a) A high quality photo of a dog playing in a green field next to a lake.\n\n![](_page_17_Picture_2.jpeg)\n\n(b) A high quality photo of Times Square.\n\nFigure 17: unCLIP samples show low levels of detail for some complex scenes.\n\nThe risks of these models should be assessed in relation to the particular deployment context, which includes training data, guardrails in place, the deployment space, and who will have access. A preliminary analysis of these issues in the context of the DALL·E 2 Preview platform (the first deployment of an unCLIP model), can be found in [Mishkin et al.](#page-19-14) [\\[30\\]](#page-19-14).\n\n## 8 Acknowledgements\n\nWe'd like to thank Jong Wook Kim, Hyeonwoo Noh, Alec Radford, Pranav Shyam, and Ilya Sutskever for helpful discussions and contributions to our work. We'd also like to thank Yunxin Jiao for creating several figures used in the paper. We are grateful to the Acceleration and Supercomputing teams at OpenAI for their work on software and hardware infrastructure this project used.\n\n# References\n\n- <span id=\"page-18-6\"></span>[1] Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, and Luke Zettlemoyer. CM3: A Causal Masked Multimodal Model of the Internet. *[arXiv:2201.07520](https://arxiv.org/abs/2201.07520)*, 2022.\n- <span id=\"page-18-14\"></span>[2] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models. *CoRR*, abs/2201.06503, 2022. URL [https:](https://arxiv.org/abs/2201.06503) [//arxiv.org/abs/2201.06503](https://arxiv.org/abs/2201.06503).\n- <span id=\"page-18-11\"></span>[3] Florian Bordes, Randall Balestriero, and Pascal Vincent. High Fidelity Visualization of What Your Self-Supervised Representation Knows About. *[arXiv:2112.09164](https://arxiv.org/abs/2112.09164)*, 2021.\n- <span id=\"page-18-4\"></span>[4] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. *[arXiv:2005.14165](https://arxiv.org/abs/2005.14165)*, 2020.\n- <span id=\"page-18-7\"></span>[5] Rewon Child. Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images. *[arXiv:2011.10650](https://arxiv.org/abs/2011.10650)*, 2021.\n- <span id=\"page-18-13\"></span>[6] Katherine Crowson. AVA Linear Probe. [https://twitter.com/RiversHaveWings/status/](https://twitter.com/RiversHaveWings/status/1472346186728173568?s=20&t=T-HRr3Gw5HRGjQaMDtRe3A) [1472346186728173568?s=20&t=T-HRr3Gw5HRGjQaMDtRe3A](https://twitter.com/RiversHaveWings/status/1472346186728173568?s=20&t=T-HRr3Gw5HRGjQaMDtRe3A), 2021.\n- <span id=\"page-18-9\"></span>[7] Katherine Crowson. CLIP guided diffusion HQ 256x256. [https://colab.research.google.com/](https://colab.research.google.com/drive/12a_Wrfi2_gwwAuN3VvMTwVMz9TfqctNj) [drive/12a\\\\_Wrfi2\\\\_gwwAuN3VvMTwVMz9TfqctNj](https://colab.research.google.com/drive/12a_Wrfi2_gwwAuN3VvMTwVMz9TfqctNj), 2021.\n- <span id=\"page-18-10\"></span>[8] Katherine Crowson. CLIP Guided Diffusion 512x512, Secondary Model Method. [https://twitter.](https://twitter.com/RiversHaveWings/status/1462859669454536711) [com/RiversHaveWings/status/1462859669454536711](https://twitter.com/RiversHaveWings/status/1462859669454536711), 2021.\n- <span id=\"page-18-3\"></span>[9] Katherine Crowson. v-diffusion. <https://github.com/crowsonkb/v-diffusion-pytorch>, 2021.\n- <span id=\"page-18-0\"></span>[10] Karan Desai and Justin Johnson. VirTex: Learning Visual Representations from Textual Annotations. *[arXiv:2006.06666](https://arxiv.org/abs/2006.06666)*, 2020.\n- <span id=\"page-18-1\"></span>[11] Prafulla Dhariwal and Alex Nichol. Diffusion Models Beat GANs on Image Synthesis. *[arXiv:2105.05233](https://arxiv.org/abs/2105.05233)*, 2021.\n- <span id=\"page-18-5\"></span>[12] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, and Jie Tang. CogView: Mastering Text-to-Image Generation via Transformers. *[arXiv:2105.13290](https://arxiv.org/abs/2105.13290)*, 2021.\n- <span id=\"page-18-12\"></span>[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. *[arXiv:2010.11929](https://arxiv.org/abs/2010.11929)*, 2020.\n- <span id=\"page-18-8\"></span>[14] Patrick Esser, Robin Rombach, and Björn Ommer. Taming Transformers for High-Resolution Image Synthesis. *[arXiv:2012.09841](https://arxiv.org/abs/2012.09841)*, 2020.\n- <span id=\"page-18-2\"></span>[15] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-Aware Minimization for Efficiently Improving Generalization. *[arXiv:2010.01412](https://arxiv.org/abs/2010.01412)*, 2020.\n\n- <span id=\"page-19-1\"></span>[16] Andreas Fürst, Elisabeth Rumetshofer, Viet Thuong Tran, Hubert Ramsauer, Fei Tang, Johannes Lehner, D P Kreil, Michael K Kopp, Günter Klambauer, Angela Bitto-Nemling, and Sepp Hochreiter. CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP, 2022. URL [https://openreview.](https://openreview.net/forum?id=qw674L9PfQE) [net/forum?id=qw674L9PfQE](https://openreview.net/forum?id=qw674L9PfQE).\n- <span id=\"page-19-6\"></span>[17] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors. *[arXiv:2203.13131](https://arxiv.org/abs/2203.13131)*, 2022.\n- <span id=\"page-19-13\"></span>[18] Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, and Daniel Cohen-Or. StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators. *[arXiv:2108.00946](https://arxiv.org/abs/2108.00946)*, 2021.\n- <span id=\"page-19-11\"></span>[19] Federico A. Galatolo, Mario G. C. A. Cimino, and Gigliola Vaglini. Generating images from caption and vice versa via CLIP-Guided Generative Latent Space Search. *[arXiv:2102.01645](https://arxiv.org/abs/2102.01645)*, 2021.\n- <span id=\"page-19-5\"></span>[20] Gabriel Goh, Nick Cammarata †, Chelsea Voss †, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, and Chris Olah. Multimodal Neurons in Artificial Neural Networks. *Distill*, 2021. doi: 10.23915/distill.00030. https://distill.pub/2021/multimodal-neurons.\n- <span id=\"page-19-9\"></span>[21] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Networks. *[arXiv:1406.2661](https://arxiv.org/abs/1406.2661)*, 2014.\n- <span id=\"page-19-10\"></span>[22] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector Quantized Diffusion Model for Text-to-Image Synthesis. *[arXiv:2111.14822](https://arxiv.org/abs/2111.14822)*, 2021.\n- <span id=\"page-19-7\"></span>[23] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. *Advances in Neural Information Processing Systems 30 (NIPS 2017)*, 2017.\n- <span id=\"page-19-4\"></span>[24] Jonathan Ho and Tim Salimans. Classifier-Free Diffusion Guidance. In *NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications*, 2021. URL [https://openreview.net/](https://openreview.net/forum?id=qw8AKxfYbI) [forum?id=qw8AKxfYbI](https://openreview.net/forum?id=qw8AKxfYbI).\n- <span id=\"page-19-2\"></span>[25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. *[arXiv:2006.11239](https://arxiv.org/abs/2006.11239)*, 2020.\n- <span id=\"page-19-3\"></span>[26] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded Diffusion Models for High Fidelity Image Generation. *[arXiv:2106.15282](https://arxiv.org/abs/2106.15282)*, 2021.\n- <span id=\"page-19-15\"></span>[27] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. *[arXiv:1412.6980](https://arxiv.org/abs/1412.6980)*, 2014.\n- <span id=\"page-19-8\"></span>[28] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. Microsoft COCO: Common Objects in Context. *[arXiv:1405.0312](https://arxiv.org/abs/1405.0312)*, 2014.\n- <span id=\"page-19-16\"></span>[29] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. *[arXiv:1711.05101](https://arxiv.org/abs/1711.05101)*, 2017.\n- <span id=\"page-19-14\"></span>[30] Pamela Mishkin, Lama Ahmad, Miles Brundage, Gretchen Krueger, and Girish Sastry. DALL·E 2 Preview - Risks and Limitations. 2022. URL [https://github.com/openai/dalle-2-preview/](https://github.com/openai/dalle-2-preview/blob/main/system-card.md) [blob/main/system-card.md](https://github.com/openai/dalle-2-preview/blob/main/system-card.md).\n- <span id=\"page-19-0\"></span>[31] Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. SLIP: Self-supervision meets Language-Image Pre-training. *[arXiv:2112.12750](https://arxiv.org/abs/2112.12750)*, 2021.\n- <span id=\"page-19-12\"></span>[32] Ryan Murdock. The Big Sleep. [https://twitter.com/advadnoun/status/](https://twitter.com/advadnoun/status/1351038053033406468) [1351038053033406468](https://twitter.com/advadnoun/status/1351038053033406468), 2021.\n\n- <span id=\"page-20-12\"></span>[33] Naila Murray, Luca Marchesotti, and Florent Perronnin. AVA: A large-scale database for aesthetic visual analysis. In *2012 IEEE Conference on Computer Vision and Pattern Recognition*, pages 2408–2415, 2012. doi: 10.1109/CVPR.2012.6247954.\n- <span id=\"page-20-7\"></span>[34] Alex Nichol and Prafulla Dhariwal. Improved Denoising Diffusion Probabilistic Models. *[arXiv:2102.09672](https://arxiv.org/abs/2102.09672)*, 2021.\n- <span id=\"page-20-6\"></span>[35] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. *[arXiv:2112.10741](https://arxiv.org/abs/2112.10741)*, 2021.\n- <span id=\"page-20-16\"></span>[36] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery. *[arXiv:2103.17249](https://arxiv.org/abs/2103.17249)*, 2021.\n- <span id=\"page-20-10\"></span>[37] Karl Pearson. LIII. On lines and planes of closest fit to systems of points in space, November 1901. URL <https://doi.org/10.1080/14786440109462720>.\n- <span id=\"page-20-15\"></span>[38] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn. Diffusion Autoencoders: Toward a Meaningful and Decodable Representation. *[arXiv:2111.15640](https://arxiv.org/abs/2111.15640)*, 2021.\n- <span id=\"page-20-1\"></span>[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision. *[arXiv:2103.00020](https://arxiv.org/abs/2103.00020)*, 2021.\n- <span id=\"page-20-5\"></span>[40] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-Shot Text-to-Image Generation. *[arXiv:2102.12092](https://arxiv.org/abs/2102.12092)*, 2021.\n- <span id=\"page-20-13\"></span>[41] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating Diverse High-Fidelity Images with VQ-VAE-2. *[arXiv:1906.00446](https://arxiv.org/abs/1906.00446)*, 2019.\n- <span id=\"page-20-9\"></span>[42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-Resolution Image Synthesis with Latent Diffusion Models. *[arXiv:2112.10752](https://arxiv.org/abs/2112.10752)*, 2021.\n- <span id=\"page-20-8\"></span>[43] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and Mohammad Norouzi. Image Super-Resolution via Iterative Refinement. *[arXiv:arXiv:2104.07636](https://arxiv.org/abs/arXiv:2104.07636)*, 2021.\n- <span id=\"page-20-0\"></span>[44] Mert Bulent Sariyildiz, Julien Perez, and Diane Larlus. Learning Visual Representations with Caption Annotations. *[arXiv:2008.01392](https://arxiv.org/abs/2008.01392)*, 2020.\n- <span id=\"page-20-2\"></span>[45] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt Keutzer. How Much Can CLIP Benefit Vision-and-Language Tasks? *[arXiv:2107.06383](https://arxiv.org/abs/2107.06383)*, 2021.\n- <span id=\"page-20-3\"></span>[46] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep Unsupervised Learning using Nonequilibrium Thermodynamics. *[arXiv:1503.03585](https://arxiv.org/abs/1503.03585)*, 2015.\n- <span id=\"page-20-17\"></span>[47] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising Diffusion Implicit Models. *[arXiv:2010.02502](https://arxiv.org/abs/2010.02502)*, 2020.\n- <span id=\"page-20-4\"></span>[48] Yang Song and Stefano Ermon. Improved Techniques for Training Score-Based Generative Models. *[arXiv:2006.09011](https://arxiv.org/abs/2006.09011)*, 2020.\n- <span id=\"page-20-11\"></span>[49] Ming Tao, Hao Tang, Songsong Wu, Nicu Sebe, Xiao-Yuan Jing, Fei Wu, and Bingkun Bao. DF-GAN: Deep Fusion Generative Adversarial Networks for Text-to-Image Synthesis. *[arXiv:2008.05865](https://arxiv.org/abs/2008.05865)*, 2020.\n- <span id=\"page-20-14\"></span>[50] Arash Vahdat and Jan Kautz. NVAE: A Deep Hierarchical Variational Autoencoder. *[arXiv:2007.03898](https://arxiv.org/abs/2007.03898)*, 2020.\n\n- <span id=\"page-21-12\"></span>[51] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based Generative Modeling in Latent Space. In *Neural Information Processing Systems (NeurIPS)*, 2021.\n- <span id=\"page-21-11\"></span>[52] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural Discrete Representation Learning. *[arXiv:1711.00937](https://arxiv.org/abs/1711.00937)*, 2017.\n- <span id=\"page-21-4\"></span>[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. *[arXiv:1706.03762](https://arxiv.org/abs/1706.03762)*, 2017.\n- <span id=\"page-21-6\"></span>[54] Zihao Wang, Wei Liu, Qian He, Xinglong Wu, and Zili Yi. CLIP-GEN: Language-Free Training of a Text-to-Image Generator with CLIP. *[arXiv:2203.00386](https://arxiv.org/abs/2203.00386)*, 2022.\n- <span id=\"page-21-2\"></span>[55] Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang. GAN Inversion: A Survey. *[arXiv:2101.05278](https://arxiv.org/abs/2101.05278)*, 2021.\n- <span id=\"page-21-7\"></span>[56] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks. *[arXiv:1711.10485](https://arxiv.org/abs/1711.10485)*, 2017.\n- <span id=\"page-21-9\"></span>[57] Hui Ye, Xiulong Yang, Martin Takac, Rajshekhar Sunderraman, and Shihao Ji. Improving Text-to-Image Synthesis Using Contrastive Learning. *[arXiv:2107.02423](https://arxiv.org/abs/2107.02423)*, 2021.\n- <span id=\"page-21-10\"></span>[58] Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang. Cross-Modal Contrastive Learning for Text-to-Image Generation. *[arXiv:2101.04702](https://arxiv.org/abs/2101.04702)*, 2021.\n- <span id=\"page-21-3\"></span>[59] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing a Practical Degradation Model for Deep Blind Image Super-Resolution. *2021 IEEE/CVF International Conference on Computer Vision (ICCV)*, Oct 2021. doi: 10.1109/iccv48922.2021.00475. URL [http://dx.doi.org/10.1109/](http://dx.doi.org/10.1109/ICCV48922.2021.00475) [ICCV48922.2021.00475](http://dx.doi.org/10.1109/ICCV48922.2021.00475).\n- <span id=\"page-21-0\"></span>[60] Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D. Manning, and Curtis P. Langlotz. Contrastive Learning of Medical Visual Representations from Paired Images and Text. *[arXiv:2010.00747](https://arxiv.org/abs/2010.00747)*, 2020.\n- <span id=\"page-21-5\"></span>[61] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong Sun. LAFITE: Towards Language-Free Training for Text-to-Image Generation. *[arXiv:2111.13792](https://arxiv.org/abs/2111.13792)*, 2021.\n- <span id=\"page-21-1\"></span>[62] Jun-Yan Zhu, Philipp Krähenbühl, Eli Shechtman, and Alexei A. Efros. Generative Visual Manipulation on the Natural Image Manifold. *[arXiv:1609.03552](https://arxiv.org/abs/1609.03552)*, 2016.\n- <span id=\"page-21-8\"></span>[63] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-to-Image Synthesis. *[arXiv:1904.01310](https://arxiv.org/abs/1904.01310)*, 2019.\n\n# <span id=\"page-22-1\"></span>A Linear Probes for Evaluations\n\nFor our evaluations, we leverage two new linear probes on top of a CLIP ViT-L/14 [\\[13\\]](#page-18-12) model. To automate aesthetic quality evaluations, we follow the procedure used by [Crowson](#page-18-13) [\\[6\\]](#page-18-13), training a linear regression model on images and mean ratings from the AVA dataset [\\[33\\]](#page-20-12). To reduce the cost of hyperparameter sweeps before conducting human evaluations, we train a logistic regression model to predict win probabilities between pairs of images. To train this model, we used 15,000 pairwise image comparisons gathered from all of our previous human evaluations. For each comparison i, we computed CLIP image embeddings x<sup>i</sup> and y<sup>i</sup> for the two images in the pair. We then trained a linear model f(x) such that 1/(1 + exp (f(xi) − f(yi))) approximates the probability that a human prefers the image for y<sup>i</sup> . This can be reduced to a logistic regression problem with inputs equal to y<sup>i</sup> − x<sup>i</sup> .\n\n# B Error Bars for Human Evaluation\n\nWhen computing error bars for human evaluations, we use the normal approximation interval with p = 0.95. We expect the normal approximation to be accurate for such a large sample size of n = 1000.\n\n## <span id=\"page-22-0\"></span>C Training Details\n\nThe unCLIP models used for the experiments in this paper were trained with the hyperparameters described below, unless otherwise noted. We additionally trained a production version of unCLIP using similarly sized models but with modified architectures and trained for longer; we include changes to accommodate product and safety requirements (e.g. inpainting, preventing unwanted memorization), and train on a larger dataset that is filtered for aesthetic quality and safety. We report model and training hyperparameters for the paper models in Table [3.](#page-23-0) All models were trained using Adam [\\[27\\]](#page-19-15) with corrected weight decay [\\[29\\]](#page-19-16) and momentum β<sup>1</sup> = 0.9.\n\nOur CLIP model uses a ViT-H/16 [\\[13\\]](#page-18-12) image encoder that consumes 256 × 256 resolution images, and has width 1280 with 32 Transformer [\\[53\\]](#page-21-4) blocks. The text encoder also follows the architecture described in [Radford et al.](#page-20-1) [\\[39\\]](#page-20-1): it is a Transformer [\\[53\\]](#page-21-4) with a causal attention mask, with width 1024 and 24 Transformer blocks. Both models are trained with learning rate 3 × 10<sup>−</sup><sup>4</sup> and SAM [\\[15\\]](#page-18-2) with ρ = 0.1, where the perturbations are applied independently by the replicas, each of which uses batch size 64. The remaining hyperparameters are the same as those reported in [Radford et al.](#page-20-1) [\\[39\\]](#page-20-1).\n\nWhen training the encoder, we sample from the CLIP [\\[39\\]](#page-20-1) and DALL-E [\\[40\\]](#page-20-5) datasets (approximately 650M images in total) with equal probability. When training the decoder, upsamplers, and prior, we use only the DALL-E dataset [\\[40\\]](#page-20-5) (approximately 250M images). Incorporating the noisier CLIP dataset while training the generative stack negatively impacted sample quality in our initial evaluations.\n\nOur decoder architecture is the 3.5 billion parameter GLIDE model, with the same architecture and diffusion hyperparameters as in [Nichol et al.](#page-20-6) [\\[35\\]](#page-20-6). We train with learned sigma and sample with 250 strided sampling steps as in [Nichol and Dhariwal](#page-20-7) [\\[34\\]](#page-20-7).\n\nWe use the ADMNet architecture [\\[11\\]](#page-18-1) for the upsamplers. In the first upsampling stage, we use a cosine noising schedule, 320 channels and a depth of 3 resblocks per resolution inside the ADMNet. We also apply gaussian blur (kernel size 3, sigma 0.6) as described in [Saharia et al.](#page-20-8) [\\[43\\]](#page-20-8). In the second upsampling stage, we use a linear noising schedule, 192 channels, a depth of 2 resblocks per resolution, and train with the BSR degradation from [Rombach et al.](#page-20-9) [\\[42\\]](#page-20-9). Neither upsampler uses attention. To reduce inference time, we use DDIM [\\[47\\]](#page-20-17) and manually tune the number of steps, with 27 steps for 256 × 256 model, and 15 steps for the 1024 × 1024 model.\n\nFor the AR prior, we use a Transformer text encoder with width 2048 and 24 blocks and a decoder with a causal attention mask, width 1664, and 24 blocks. For the diffusion prior, we use a Transformer with width 2048 and 24 blocks, and sample with Analytic DPM [\\[2\\]](#page-18-14) with 64 strided sampling steps. To reuse hyperparameters tuned for diffusion noise schedules on images from [Dhariwal and Nichol](#page-18-1) [\\[11\\]](#page-18-1), we scale the CLIP embedding inputs by 17.2 to match the empirical variance of RGB pixel values of ImageNet images scaled to [−1, 1].\n\n<span id=\"page-23-0\"></span>\n\n|                          | AR prior | Diffusion prior | 64           | 64 → 256  | 256 → 1024  |\n|--------------------------|----------|-----------------|--------------|-----------|-------------|\n| Diffusion steps          | -        | 1000            | 1000         | 1000      | 1000        |\n| Noise schedule           | -        | cosine          | cosine       | cosine    | linear      |\n| Sampling steps           | -        | 64              | 250          | 27        | 15          |\n| Sampling variance method | -        | analytic [2]    | learned [34] | DDIM [47] | DDIM [47]   |\n| Crop fraction            | -        | -               | -            | 0.25      | 0.25        |\n| Model size               | 1B       | 1B              | 3.5B         | 700M      | 300M        |\n| Channels                 | -        | -               | 512          | 320       | 192         |\n| Depth                    | -        | -               | 3            | 3         | 2           |\n| Channels multiple        | -        | -               | 1,2,3,4      | 1,2,3,4   | 1,1,2,2,4,4 |\n| Heads channels           | -        | -               | 64           | -         | -           |\n| Attention resolution     | -        | -               | 32,16,8      | -         | -           |\n| Text encoder context     | 256      | 256             | 256          | -         | -           |\n| Text encoder width       | 2048     | 2048            | 2048         | -         | -           |\n| Text encoder depth       | 24       | 24              | 24           | -         | -           |\n| Text encoder heads       | 32       | 32              | 32           | -         | -           |\n| Latent decoder context   | 384      | -               | -            | -         | -           |\n| Latent decoder width     | 1664     | -               | -            | -         | -           |\n| Latent decoder depth     | 24       | -               | -            | -         | -           |\n| Latent decoder heads     | 26       | -               | -            | -         | -           |\n| Dropout                  | -        | -               | 0.1          | 0.1       | -           |\n| Weight decay             | 4.0e-2   | 6.0e-2          | -            | -         | -           |\n| Batch size               | 4096     | 4096            | 2048         | 1024      | 512         |\n| Iterations               | 1M       | 600K            | 800K         | 1M        | 1M          |\n| Learning rate            | 1.6e-4   | 1.1e-4          | 1.2e-4       | 1.2e-4    | 1.0e-4      |\n| Adam β2                  | 0.91     | 0.96            | 0.999        | 0.999     | 0.999       |\n| Adam                     | 1.0e-10  | 1.0e-6          | 1.0e-8       | 1.0e-8    | 1.0e-8      |\n| EMA decay                | 0.999    | 0.9999          | 0.9999       | 0.9999    | 0.9999      |\n\nTable 3: Hyperparameters for the models\n\n# D Random samples\n\nIn Figures [18,](#page-24-0) [19](#page-25-0) and [20](#page-26-0) we show random samples from our production model for some of the prompts from Figure [1.](#page-1-0)\n\n<span id=\"page-24-0\"></span>![](_page_24_Picture_2.jpeg)\n\nFigure 18: Random samples from unCLIP for prompt \"Vibrant portrait painting of Salvador Dali with a robotic half face\"\n\n<span id=\"page-25-0\"></span>![](_page_25_Picture_0.jpeg)\n\nFigure 19: Random samples from unCLIP for prompt \"A close up of a handpalm with leaves growing from it.\"\n\n<span id=\"page-26-0\"></span>![](_page_26_Picture_0.jpeg)\n\nFigure 20: Random samples from unCLIP for prompt \"A teddybear on a skateboard in Times Square.\"",
  "chunks": [
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_0",
      "content": "# Hierarchical Text-Conditional Image Generation with CLIP Latents\n\nAditya Ramesh<sup>∗</sup> OpenAI aramesh@openai.com\n\nPrafulla Dhariwal<sup>∗</sup> OpenAI prafulla@openai.com\n\nAlex Nichol<sup>∗</sup> OpenAI alex@openai.com\n\nCasey Chu<sup>∗</sup> OpenAI casey@openai.com\n\nMark Chen OpenAI mark@openai.com\n\n# Abstract",
      "metadata": {
        "chunk_index": 0,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 318
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_1",
      "content": "Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion",
      "metadata": {
        "chunk_index": 1,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 816
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_2",
      "content": ". Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.",
      "metadata": {
        "chunk_index": 2,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 324
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_3",
      "content": "# 1 Introduction",
      "metadata": {
        "chunk_index": 3,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 16
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_4",
      "content": "Recent progress in computer vision has been driven by scaling models on large datasets of captioned images collected from the internet [\\[10,](#page-18-0) [44,](#page-20-0) [60,](#page-21-0) [39,](#page-20-1) [31,](#page-19-0) [16\\]](#page-19-1). Within this framework, CLIP [\\[39\\]](#page-20-1) has emerged as a successful representation learner for images. CLIP embeddings have a number of desirable properties: they are robust to image distribution shift, have impressive zero-shot capabilities, and have been fine-tuned to achieve state-of-the-art results on a wide variety of vision and language tasks [\\[45\\]](#page-20-2). Concurrently, diffusion models [\\[46,](#page-20-3) [48,](#page-20-4) [25\\]](#page-19-2) have emerged as a promising generative modeling framework, pushing the state-of-the-art on image and video generation tasks [\\[11,](#page-18-1) [26,](#page-19-3) [24\\]](#page-19-4)",
      "metadata": {
        "chunk_index": 4,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 897
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_5",
      "content": ". To achieve best results, diffusion models leverage a guidance technique [\\[11,](#page-18-1) [24\\]](#page-19-4) which improves sample fidelity (for images, photorealism) at the cost of sample diversity.",
      "metadata": {
        "chunk_index": 5,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 203
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_6",
      "content": "In this work, we combine these two approaches for the problem of text-conditional image generation. We first train a diffusion *decoder* to invert the CLIP image *encoder*. Our inverter is non-deterministic, and can produce multiple images corresponding to a given image embedding. The presence of an encoder and its approximate inverse (the decoder) allows for capabilities beyond text-to-image translation. As in GAN inversion [\\[62,](#page-21-1) [55\\]](#page-21-2), encoding and decoding an input image produces semantically similar output images (Figure [3\\)](#page-4-0). We can also interpolate between input images by inverting interpolations of their image embeddings (Figure [4\\)](#page-5-0). However, one notable advantage of using the CLIP latent space is the ability to semantically modify images by moving in the direction of any encoded text vector (Figure [5\\)](#page-6-0), whereas discovering these directions in GAN latent space involves\n\n<sup>∗</sup>Equal contribution",
      "metadata": {
        "chunk_index": 6,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 985
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_7",
      "content": "<sup>∗</sup>Equal contribution\n\n<span id=\"page-1-0\"></span>![](_page_1_Picture_0.jpeg)\n\nvibrant portrait painting of Salvador Dalí with a robotic half face a shiba inu wearing a beret and black turtleneck a close up of a handpalm with leaves growing from it\n\n![](_page_1_Picture_2.jpeg)\n\n![](_page_1_Picture_4.jpeg)\n\n![](_page_1_Picture_6.jpeg)\n\nan espresso machine that makes coffee from human souls, artstation panda mad scientist mixing sparkling chemicals, artstation a corgi's head depicted as an explosion of a nebula\n\n![](_page_1_Picture_8.jpeg)\n\n![](_page_1_Picture_10.jpeg)\n\n![](_page_1_Picture_12.jpeg)\n\n![](_page_1_Picture_14.jpeg)\n\na dolphin in an astronaut suit on saturn, artstation a propaganda poster depicting a cat dressed as french emperor napoleon holding a piece of cheese a teddy bear on a skateboard in times square\n\n![](_page_1_Picture_16.jpeg)\n\nFigure 1: Selected 1024 × 1024 samples from a production version of our model.\n\n![](_page_2_Picture_0.jpeg)",
      "metadata": {
        "chunk_index": 7,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 977
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_8",
      "content": "![](_page_1_Picture_16.jpeg)\n\nFigure 1: Selected 1024 × 1024 samples from a production version of our model.\n\n![](_page_2_Picture_0.jpeg)\n\nFigure 2: A high-level overview of unCLIP. Above the dotted line, we depict the CLIP training process, through which we learn a joint representation space for text and images. Below the dotted line, we depict our text-to-image generation process: a CLIP text embedding is first fed to an autoregressive or diffusion prior to produce an image embedding, and then this embedding is used to condition a diffusion decoder which produces a final image. Note that the CLIP model is frozen during training of the prior and decoder.\n\nluck and diligent manual examination. Furthermore, encoding and decoding images also provides us with a tool for observing which features of the image are recognized or disregarded by CLIP.",
      "metadata": {
        "chunk_index": 8,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 854
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_9",
      "content": "luck and diligent manual examination. Furthermore, encoding and decoding images also provides us with a tool for observing which features of the image are recognized or disregarded by CLIP.\n\nTo obtain a full generative model of images, we combine the CLIP image embedding *decoder* with a *prior* model, which generates possible CLIP image embeddings from a given text caption. We compare our text-to-image system with other systems such as DALL-E [\\[40\\]](#page-20-5) and GLIDE [\\[35\\]](#page-20-6), finding that our samples are comparable in quality to GLIDE, but with greater diversity in our generations. We also develop methods for training diffusion priors in latent space, and show that they achieve comparable performance to autoregressive priors, while being more compute-efficient. We refer to our full text-conditional image generation stack as *unCLIP*, since it generates images by inverting the CLIP image encoder.",
      "metadata": {
        "chunk_index": 9,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 928
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_10",
      "content": "## 2 Method\n\nOur training dataset consists of pairs (x, y) of images x and their corresponding captions y. Given an image x, let z<sup>i</sup> and z<sup>t</sup> be its CLIP image and text embeddings, respectively. We design our generative stack to produce images from captions using two components:\n\n- A *prior* P(z<sup>i</sup> |y) that produces CLIP image embeddings z<sup>i</sup> conditioned on captions y.\n- A *decoder* P(x|z<sup>i</sup> , y) that produces images x conditioned on CLIP image embeddings z<sup>i</sup> (and optionally text captions y).\n\nThe decoder allows us to invert images given their CLIP image embeddings, while the prior allows us to learn a generative model of the image embeddings themselves. Stacking these two components yields a generative model P(x|y) of images x given captions y:\n\n$$P(x|y) = P(x, z_i|y) = P(x|z_i, y)P(z_i|y).$$",
      "metadata": {
        "chunk_index": 10,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 860
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_11",
      "content": "$$P(x|y) = P(x, z_i|y) = P(x|z_i, y)P(z_i|y).$$\n\nThe first equality holds because z<sup>i</sup> is a deterministic function of x. The second equality holds because of the chain rule. Thus, we can sample from the true conditional distribution P(x|y) by first sampling z<sup>i</sup> using the prior, and then sampling x using the decoder. In the following sections, we describe our decoder and prior stacks. For training details and hyperparameters, refer to Appendix [C.](#page-22-0)",
      "metadata": {
        "chunk_index": 11,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 482
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_12",
      "content": "#### 2.1 Decoder\n\nWe use diffusion models [\\[25,](#page-19-2) [48\\]](#page-20-4) to produce images conditioned on CLIP image embeddings (and optionally text captions). Specifically, we modify the architecture described in [Nichol et al.](#page-20-6) [\\(2021\\)](#page-20-6) by projecting and adding CLIP embeddings to the existing timestep embedding, and by projecting CLIP embeddings into four extra tokens of context that are concatenated to the sequence of outputs from the GLIDE text encoder. We retained the text conditioning pathway present in the original GLIDE model, hypothesizing that it could allow the diffusion model to learn aspects of natural language that CLIP fails to capture (e.g. variable binding), but find that it offers little help in this regard (Section [7\\)](#page-15-0).",
      "metadata": {
        "chunk_index": 12,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 796
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_13",
      "content": "While we can sample from the conditional distribution of the decoder directly, past work using diffusion models shows using guidance on the conditioning information [\\[11,](#page-18-1) [24,](#page-19-4) [35\\]](#page-20-6) improves sample quality a lot. We enable classifier-free guidance [\\[24\\]](#page-19-4) by randomly setting the CLIP embeddings to zero (or a learned embedding) 10% of the time, and randomly dropping the text caption 50% of the time during training.",
      "metadata": {
        "chunk_index": 13,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 470
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_14",
      "content": "To generate high resolution images, we train two diffusion upsampler models [\\[34,](#page-20-7) [43\\]](#page-20-8): one to upsample images from 64×64 to 256×256 resolution, and another to further upsample those to 1024×1024 resolution. To improve the robustness of our upsamplers, we slightly corrupt the conditioning images during training. For the first upsampling stage, we use gaussian blur [\\[43\\]](#page-20-8), and for the second, we use a more diverse BSR degradation [\\[42,](#page-20-9) [59\\]](#page-21-3). To reduce training compute and improve numerical stability, we follow [Rombach et al.](#page-20-9) [\\[42\\]](#page-20-9) and train on random crops of images that are one-fourth the target size. We use only spatial convolutions in the model (i.e., no attention layers) and at inference time directly apply the model at the target resolution, observing that it readily generalizes to the higher resolution",
      "metadata": {
        "chunk_index": 14,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 917
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_15",
      "content": ". We found no benefit from conditioning the upsamplers on the caption, and use unconditional ADMNets [\\[11\\]](#page-18-1) with no guidance.",
      "metadata": {
        "chunk_index": 15,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 139
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_16",
      "content": "#### 2.2 Prior\n\nWhile a decoder can invert CLIP image embeddings z<sup>i</sup> to produce images x, we need a prior model that produces z<sup>i</sup> from captions y to enable image generations from text captions. We explore two different model classes for the prior model:\n\n- *Autoregressive (AR)* prior: the CLIP image embedding z<sup>i</sup> is converted into a sequence of discrete codes and predicted autoregressively conditioned on the caption y.\n- *Diffusion* prior: The continuous vector z<sup>i</sup> is directly modelled using a Gaussian diffusion model conditioned on the caption y.\n\nIn addition to the caption, we can condition the prior on the CLIP text embedding z<sup>t</sup> since it is a deterministic function of the caption. To improve sample quality we also enable sampling using classifier-free guidance for both the AR and diffusion prior, by randomly dropping this text conditioning information 10% of the time during training.",
      "metadata": {
        "chunk_index": 16,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 950
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_17",
      "content": "To train and sample from the AR prior more efficiently, we first reduce the dimensionality of the CLIP image embeddings z<sup>i</sup> by applying Principal Component Analysis (PCA) [\\[37\\]](#page-20-10). In particular, we find that the rank of the CLIP representation space is drastically reduced when training CLIP with SAM [\\[15\\]](#page-18-2) while slightly improving evaluation metrics. We are able to preserve nearly all of the information[2](#page-3-0) by retaining only 319 principal components out of the original 1,024. After applying PCA, we order the principal components by decreasing eigenvalue magnitude, quantize each of the 319 dimensions into 1,024 discrete buckets, and\n\n<span id=\"page-3-0\"></span><sup>2</sup> I.e., less than 1% average mean-squared error in reconstructing the image representations.\n\n<span id=\"page-4-0\"></span>![](_page_4_Picture_0.jpeg)",
      "metadata": {
        "chunk_index": 17,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 875
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_18",
      "content": "<span id=\"page-3-0\"></span><sup>2</sup> I.e., less than 1% average mean-squared error in reconstructing the image representations.\n\n<span id=\"page-4-0\"></span>![](_page_4_Picture_0.jpeg)\n\nFigure 3: Variations of an input image by encoding with CLIP and then decoding with a diffusion model. The variations preserve both semantic information like presence of a clock in the painting and the overlapping strokes in the logo, as well as stylistic elements like the surrealism in the painting and the color gradients in the logo, while varying the non-essential details.\n\npredict the resulting sequence using a Transformer [\\[53\\]](#page-21-4) model with a causal attention mask. This results in a threefold reduction in the number of tokens predicted during inference, and improves training stability.",
      "metadata": {
        "chunk_index": 18,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 798
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_19",
      "content": "We condition the AR prior on the text caption and the CLIP text embedding by encoding them as a prefix to the sequence. Additionally, we prepend a token indicating the (quantized) dot product between the text embedding and image embedding, z<sup>i</sup> · zt. This allows us to condition the model on a higher dot product, since higher text-image dot products correspond to captions which better describe the image. In practice, we find it beneficial to sample the dot product from the top half of the distribution.[3](#page-4-1)",
      "metadata": {
        "chunk_index": 19,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 529
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_20",
      "content": "For the diffusion prior, we train a decoder-only Transformer with a causal attention mask on a sequence consisting of, in order: the encoded text, the CLIP text embedding, an embedding for the diffusion timestep, the noised CLIP image embedding, and a final embedding whose output from the Transformer is used to predict the unnoised CLIP image embedding. We choose not to condition the diffusion prior on z<sup>i</sup> · z<sup>t</sup> like in the AR prior; instead, we improve quality during sampling time by generating two samples of z<sup>i</sup> and selecting the one with a higher dot product with zt. Instead of using the -prediction formulation from [Ho et al.](#page-19-2) [\\[25\\]](#page-19-2), we find it better to train our model to predict the unnoised z<sup>i</sup> directly, and use a mean-squared error loss on this prediction:\n\n$$L_{\\text{prior}} = \\mathbb{E}_{t \\sim [1, T], z_i^{(t)} \\sim q_t} \\left[ \\| f_{\\theta}(z_i^{(t)}, t, y) - z_i \\|^2 \\right]$$",
      "metadata": {
        "chunk_index": 20,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 969
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_21",
      "content": "$$L_{\\text{prior}} = \\mathbb{E}_{t \\sim [1, T], z_i^{(t)} \\sim q_t} \\left[ \\| f_{\\theta}(z_i^{(t)}, t, y) - z_i \\|^2 \\right]$$\n\n<span id=\"page-4-1\"></span><sup>3</sup>We swept over percentiles 50%, 70%, 85%, 95% and found 50% to be optimal in all experiments.\n\n<span id=\"page-5-0\"></span>![](_page_5_Picture_0.jpeg)\n\nFigure 4: Variations between two images by interpolating their CLIP image embedding and then decoding with a diffusion model. We fix the decoder seed across each row. The intermediate variations naturally blend the content and style from both input images.",
      "metadata": {
        "chunk_index": 21,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 573
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_22",
      "content": "## 3 Image Manipulations\n\nOur approach allows us to encode any given image x into a bipartite latent representation (z<sup>i</sup> , x<sup>T</sup> ) that is sufficient for the decoder to produce an accurate reconstruction. The latent z<sup>i</sup> describes the aspects of the image that are recognized by CLIP, while the latent x<sup>T</sup> encodes all of the residual information necessary for the decoder to reconstruct x. The former is obtained by simply encoding the image with the CLIP image encoder. The latter is obtained by applying DDIM inversion (Appendix F in [\\[11\\]](#page-18-1)) to x using the decoder, while conditioning on z<sup>i</sup> . We describe three different kinds of manipulations that are enabled by this bipartite representation.",
      "metadata": {
        "chunk_index": 22,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 758
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_23",
      "content": "## 3.1 Variations\n\nGiven an image x, we can produce related images that share the same essential content but vary in other apects, such as shape and orientation (Figure [3\\)](#page-4-0). To do this, we apply the decoder to the bipartite representation (z<sup>i</sup> , x<sup>T</sup> ) using DDIM with η > 0 for sampling. With η = 0, the decoder becomes deterministic and will reconstruct the given image x. Larger values of η introduce stochasticity into successive sampling steps, resulting in variations that are perceptually \"centered\" around the original image x. As η increases, these variations tell us what information was captured in the CLIP image embedding (and thus is preserved across samples), and what was lost (and thus changes across the samples).\n\n<span id=\"page-6-0\"></span>![](_page_6_Picture_0.jpeg)\n\na photo of a cat  $\\rightarrow$  an anime drawing of a super saiyan cat, artstation\n\n![](_page_6_Picture_2.jpeg)",
      "metadata": {
        "chunk_index": 23,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 933
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_24",
      "content": "<span id=\"page-6-0\"></span>![](_page_6_Picture_0.jpeg)\n\na photo of a cat  $\\rightarrow$  an anime drawing of a super saiyan cat, artstation\n\n![](_page_6_Picture_2.jpeg)\n\na photo of a victorian house  $\\rightarrow$  a photo of a modern house\n\n![](_page_6_Picture_4.jpeg)\n\na photo of an adult lion  $\\rightarrow$  a photo of lion cub\n\n![](_page_6_Picture_6.jpeg)\n\na photo of a landscape in winter  $\\rightarrow$  a photo of a landscape in fall\n\nFigure 5: Text diffs applied to images by interpolating between their CLIP image embeddings and a normalised difference of the CLIP text embeddings produced from the two descriptions. We also perform DDIM inversion to perfectly reconstruct the input image in the first column, and fix the decoder DDIM noise across each row.",
      "metadata": {
        "chunk_index": 24,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 767
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_25",
      "content": "#### 3.2 Interpolations",
      "metadata": {
        "chunk_index": 25,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 23
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_26",
      "content": "It is also possible to blend two images  $x_1$  and  $x_2$  for variations (Figure 4), traversing all of the concepts in CLIP's embedding space that occur between them. To do this, we rotate between their CLIP embeddings  $z_{i_1}$  and  $z_{i_2}$  using spherical interpolation, yielding intermediate CLIP representations  $z_{i_{\\theta}} = \\text{slerp}(z_{i_1}, z_{i_2}, \\theta)$  as  $\\theta$  is varied from 0 to 1. There are two options for producing the intermediate DDIM latents along the trajectory. The first option involves interpolating between their DDIM inverted latents  $x_{T_1}$  and  $x_{T_2}$  (by setting  $x_{T_{\\theta}} = \\text{slerp}(x_{T_1}, x_{T_2}, \\theta)$ ), which yields a single trajectory whose endpoints reconstruct  $x_1$  and  $x_2$ . The second option involves fixing the DDIM latent to a randomly-sampled value for all interpolates in the trajectory",
      "metadata": {
        "chunk_index": 26,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 884
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_27",
      "content": ". The second option involves fixing the DDIM latent to a randomly-sampled value for all interpolates in the trajectory. This results in an infinite number of trajectories between  $x_1$  and  $x_2$ , though the endpoints of these trajectories will generally no longer coincide with the original images. We use this approach in Figure 4.",
      "metadata": {
        "chunk_index": 27,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 336
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_28",
      "content": "#### 3.3 Text Diffs\n\nA key advantage of using CLIP compared to other models for image representations is that it embeds images and text to the same latent space, thus allowing us to apply language-guided image manipulations (i.e., text diffs), which we show in Figure 5. To modify the image to reflect a new text description y, we first obtain its CLIP text embedding  $z_t$ , as well as the CLIP text embedding  $z_{t_0}$  of a caption describing the current image<sup>4</sup>. We then compute a *text diff* vector  $z_d = \\text{norm}(z_t - z_{t_0})$  from these by taking their difference and\n\n<span id=\"page-6-1\"></span><sup>&</sup>lt;sup>4</sup>Instead of a description of the current image, we also experimented with using a dummy caption like \"a photo\" for the baseline, or removing it altogether. These also worked well.\n\n<span id=\"page-7-0\"></span>![](_page_7_Picture_0.jpeg)",
      "metadata": {
        "chunk_index": 28,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 883
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_29",
      "content": "<span id=\"page-7-0\"></span>![](_page_7_Picture_0.jpeg)\n\nFigure 6: Variations of images featuring typographic attacks [\\[20\\]](#page-19-5) paired with the CLIP model's predicted probabilities across three labels. Surprisingly, the decoder still recovers Granny Smith apples even when the predicted probability for this label is near 0%. We also find that our CLIP model is slightly less susceptible to the \"pizza\" attack than the models investigated in [\\[20\\]](#page-19-5).\n\nnormalizing. Now, we can rotate between the image CLIP embedding z<sup>i</sup> and the text diff vector z<sup>d</sup> using spherical interpolation, yielding intermediate CLIP representations z<sup>θ</sup> = slerp(z<sup>i</sup> , zd, θ), where θ is increased linearly from 0 to a maximum value that is typically in [0.25, 0.50]. We produce the final outputs by decoding the interpolates zθ, fixing the base DDIM noise to x<sup>T</sup> throughout the entire trajectory.",
      "metadata": {
        "chunk_index": 29,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 943
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_30",
      "content": "## <span id=\"page-7-1\"></span>4 Probing the CLIP Latent Space",
      "metadata": {
        "chunk_index": 30,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 61
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_31",
      "content": "Our decoder model provides a unique opportunity to explore CLIP latent space by allowing us to directly visualize what the CLIP image encoder is seeing. As an example use case, we can revisit cases where CLIP makes incorrect predictions, such as typographic attacks [\\[20\\]](#page-19-5). In these adversarial images, a piece of text is overlayed on top of an object, which causes CLIP to predict the object described by the text rather than the object depicted in the image. This piece of text essentially hides the original object in terms of output probabilities. In Figure [6,](#page-7-0) we show an example of this attack from [\\[20\\]](#page-19-5), wherein an apple can be misclassified as an iPod. Surprisingly, we find that our decoder still generates pictures of apples with high probability even though the predicted probability of \"Granny Smith\" is near zero",
      "metadata": {
        "chunk_index": 31,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 867
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_32",
      "content": ". Surprisingly, we find that our decoder still generates pictures of apples with high probability even though the predicted probability of \"Granny Smith\" is near zero. Even more notable, the model never produces pictures of iPods, despite the very high relative predicted probability of this caption.",
      "metadata": {
        "chunk_index": 32,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 300
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_33",
      "content": "<span id=\"page-8-0\"></span>![](_page_8_Picture_0.jpeg)\n\nFigure 7: Visualization of reconstructions of CLIP latents from progressively more PCA dimensions (20, 30, 40, 80, 120, 160, 200, 320 dimensions), with the original source image on the far right. The lower dimensions preserve coarse-grained semantic information, whereas the higher dimensions encode finer-grained details about the exact form of the objects in the scene.",
      "metadata": {
        "chunk_index": 33,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 427
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_34",
      "content": "PCA reconstructions offer another tool for probing the structure of the CLIP latent space. In Figure [7,](#page-8-0) we take the CLIP image embeddings of a handful of source images and reconstruct them with progressively more PCA dimensions, and then visualize the reconstructed image embeddings using our decoder with DDIM on a fixed seed. This allows us to see what semantic information the different dimensions encode. We observe that the early PCA dimensions preserve coarse-grained semantic information such as what types of objects are in the scene, whereas the later PCA dimensions encode finer-grained detail such as the shapes and exact form of the objects. For example, in the first scene, the earlier dimensions seem to encode that there is food and perhaps a container present, whereas the later dimensions encode tomatoes and a bottle specifically",
      "metadata": {
        "chunk_index": 34,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 860
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_35",
      "content": ". For example, in the first scene, the earlier dimensions seem to encode that there is food and perhaps a container present, whereas the later dimensions encode tomatoes and a bottle specifically. Figure [7](#page-8-0) also serves as a visualization of what the AR prior is modeling, since the AR prior is trained to explicitly predict these principal components in this order.",
      "metadata": {
        "chunk_index": 35,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 377
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_36",
      "content": "## 5 Text-to-Image Generation",
      "metadata": {
        "chunk_index": 36,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 29
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_37",
      "content": "#### 5.1 Importance of the Prior",
      "metadata": {
        "chunk_index": 37,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 32
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_38",
      "content": "Although we train a prior to generate CLIP image embeddings from captions, the prior is not strictly necessary for caption-to-image generation. For instance, our decoder can condition on both CLIP image embeddings and captions, but the CLIP image embedding is dropped 5% of the time during training in order to enable classifier-free guidance. Therefore, at sampling time, we can condition on only the caption, although this underperforms a model trained fully in this way (this model is GLIDE, and we do a thorough comparison with GLIDE in Sections [5.2](#page-9-0) and [5.3\\)](#page-11-0). Another possibility is to feed the decoder the CLIP text embedding as if it were an image embedding, as previously observed [\\[61,](#page-21-5) [54\\]](#page-21-6). The first two rows of Figure [8](#page-9-1) depicts samples obtained in these two ways; the third row depicts samples obtained with a prior",
      "metadata": {
        "chunk_index": 38,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 895
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_39",
      "content": ". The first two rows of Figure [8](#page-9-1) depicts samples obtained in these two ways; the third row depicts samples obtained with a prior. Conditioning the decoder on just the caption is clearly worst, but conditioning on text embeddings zero-shot does produce reasonable results. Building on this observation, another approach would be to train the decoder to condition on CLIP text embeddings [\\[9\\]](#page-18-3) instead of CLIP image embeddings (although we would lose the capabilities mentioned in Section [4\\)](#page-7-1).",
      "metadata": {
        "chunk_index": 39,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 531
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_40",
      "content": "To quantify the effectiveness of these alternate approaches, we train two models: a small decoder conditioned on CLIP text embeddings, and a small unCLIP stack (diffusion prior and decoder). We then compare samples from the text-embedding decoder, samples from the unCLIP stack, and samples obtained from feeding text\n\n<span id=\"page-9-1\"></span>![](_page_9_Figure_0.jpeg)\n\nFigure 8: Samples using different conditioning signals for the *same* decoder. In the first row, we pass the text caption to the decoder, and pass a zero vector for the CLIP embedding. In the second row, we pass both the text caption and the CLIP text embedding of the caption. In the third row, we pass the text and a CLIP image embedding generated by an autoregressive prior for the given caption. Note that this decoder is only trained to do the text-to-image generation task (without the CLIP image representation) 5% of the time.",
      "metadata": {
        "chunk_index": 40,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 908
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_41",
      "content": "embeddings to the unCLIP decoder zero-shot, sweeping across guidance scales for all models. We find that these approaches respectively score FIDs of 9.16, 7.99, and 16.55 on a test set, suggesting the unCLIP approach is best. We also run human evaluations comparing the first two settings, sweeping over sampling hyperparameters for each using our human evaluation proxy model (Appendix [A\\)](#page-22-1). We find that humans prefer the full unCLIP stack 57.0% ± 3.1% of the time for photorealism and 53.1% ± 3.1% of the time for caption similarity.\n\nGiven the importance of the prior, it is worth evaluating different approaches for training it. We compare both the AR and diffusion priors throughout our experiments. In all cases (Sections [5.2,](#page-9-0) [5.4,](#page-12-0) and [5.5\\)](#page-12-1), we find that the diffusion prior outperforms the AR prior for comparable model size and reduced training compute.",
      "metadata": {
        "chunk_index": 41,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 917
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_42",
      "content": "## <span id=\"page-9-0\"></span>5.2 Human Evaluations\n\nWe observe in Figure [1](#page-1-0) that unCLIP is capable of synthesizing complex, realistic images. While we can compare sample quality to past models using FID, it is not always aligned with human judgment. To better gauge the generation capabilities of our system, we conduct systematic human evaluations comparing unCLIP to GLIDE for photorealism, caption similarity, and sample diversity.\n\nWe follow the protocol of [Ramesh et al.,](#page-20-5) [Nichol et al.](#page-20-6) [\\[40,](#page-20-5) [35\\]](#page-20-6) for the first two evaluations: for photorealism, users are presented with pairs of images and must choose which looks more photorealistic; for caption\n\n<span id=\"page-10-1\"></span>![](_page_10_Figure_0.jpeg)",
      "metadata": {
        "chunk_index": 42,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 778
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_43",
      "content": "<span id=\"page-10-1\"></span>![](_page_10_Figure_0.jpeg)\n\nFigure 9: Samples when increasing guidance scale for both unCLIP and GLIDE, using the prompt, \"A green vase filled with red roses sitting on top of table.\" For unCLIP, we fix the latent vectors sampled from the prior, and only vary the guidance scale of the decoder. For both models, we fix the diffusion noise seed for each column. Samples from unCLIP improve in quality (more realistic lighting and shadows) but do not change in content as we increase guidance scale, preserving semantic diversity even at high decoder guidance scales.\n\n| unCLIP Prior | Photorealism | Caption Similarity | Diversity    |\n|--------------|--------------|--------------------|--------------|\n| AR           | 47.1% ± 3.1% | 41.1% ± 3.0%       | 62.6% ± 3.0% |\n| Diffusion    | 48.9% ± 3.1% | 45.3% ± 3.0%       | 70.5% ± 2.8% |",
      "metadata": {
        "chunk_index": 43,
        "content_type": "text",
        "has_table": true,
        "has_figure": true,
        "char_count": 867
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_44",
      "content": "<span id=\"page-10-0\"></span>Table 1: Human evaluations comparing unCLIP to GLIDE. We compare to both the AR and diffusion prior for unCLIP. Reported figures are 95% confidence intervals of the probability that the unCLIP model specified by the row beats GLIDE. Sampling hyperparameters for all models were swept to optimize an automated proxy for human photorealism evaluations.",
      "metadata": {
        "chunk_index": 44,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 378
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_45",
      "content": "similarity, users are additionally prompted with a caption, and must choose which image better matches the caption. In both evaluations, there is a third \"Not sure\" option. For diversity, we propose a new evaluation protocol in which humans are presented with two 4 × 4 grids of samples and must choose which is more diverse (with a third option, \"Not sure\"). For this evaluation, we produce sample grids using 1,000 captions from the MS-COCO validation set, and always compare sample grids for the same caption. Before running human comparisons, we swept over sampling hyperparameters for each model using a CLIP linear probe trained to be a proxy for human photorealism evaluations (Appendix [A\\)](#page-22-1). These hyperparameters are fixed across all three types of evaluation.",
      "metadata": {
        "chunk_index": 45,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 782
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_46",
      "content": "We present our results in Table [1.](#page-10-0) In general, the diffusion prior performs better than the AR prior in pairwise comparisons against GLIDE. We find that humans still slightly prefer GLIDE to unCLIP in terms of photorealism, but the gap is very small. Even with similar photorealism, unCLIP is strongly preferred over GLIDE in terms of diversity, highlighting one of its benefits.\n\n<span id=\"page-11-1\"></span>![](_page_11_Figure_0.jpeg)\n\n<span id=\"page-11-2\"></span>Figure 10: When comparing unCLIP (with our best sampling settings) to various settings of guidance scale for GLIDE, unCLIP was preferred by human evaluators on at least one axis among photorealism, caption similarity, and diversity for each comparison. At the higher guidance scales used to generate photorealistic images, unCLIP yields greater diversity for comparable photorealism and caption similarity.\n\n![](_page_11_Figure_2.jpeg)",
      "metadata": {
        "chunk_index": 46,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 915
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_47",
      "content": "![](_page_11_Figure_2.jpeg)\n\nFigure 11: FID versus guidance scale for unCLIP and GLIDE. For the unCLIP priors, we swept over sampling hyperparameters and fixed to the settings with the best minimum FID.",
      "metadata": {
        "chunk_index": 47,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 202
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_48",
      "content": "#### <span id=\"page-11-0\"></span>5.3 Improved Diversity-Fidelity Trade-off with Guidance\n\nCompared to GLIDE, we qualitatively observe that unCLIP is able to generate more diverse images while leveraging the guidance technique to improve sample quality. To understand why, consider Figure 9 where we increase guidance scale for both GLIDE and unCLIP. For GLIDE, the semantics (camera angle, color, size) converge as we increase guidance scale, whereas for unCLIP the semantic information of the scene is frozen in the CLIP image embedding and therefore does not collapse when guiding the decoder.\n\nIn Section 5.2, we observed that unCLIP achieves similar photorealism as GLIDE while maintaining more diversity, but that its caption matching capabilities were slightly worse. It is natural to ask whether GLIDE's guidance scale can be lowered to obtain the same diversity level as unCLIP while maintaining better caption\n\n<span id=\"page-12-2\"></span>",
      "metadata": {
        "chunk_index": 48,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 948
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_49",
      "content": "| Model                             | FID   | Zero-shot FID | Zero-shot FID (filt) |  |\n|-----------------------------------|-------|---------------|----------------------|--|\n| AttnGAN (Xu et al., 2017)         | 35.49 |               |                      |  |\n| DM-GAN (Zhu et al., 2019)         | 32.64 |               |                      |  |\n| DF-GAN (Tao et al., 2020)         | 21.42 |               |                      |  |\n| DM-GAN + CL (Ye et al., 2021)     | 20.79 |               |                      |  |\n| XMC-GAN (Zhang et al., 2021)      | 9.33  |               |                      |  |\n| LAFITE (Zhou et al., 2021)        | 8.12  |               |                      |  |\n| Make-A-Scene (Gafni et al., 2022) | 7.55  |               |                      |  |\n| DALL-E (Ramesh et al., 2021)      |       | ∼ 28          |                      |  |\n| LAFITE (Zhou et al., 2021)        |       | 26.94         |                      |  |",
      "metadata": {
        "chunk_index": 49,
        "content_type": "text",
        "has_table": true,
        "has_figure": false,
        "char_count": 967
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_50",
      "content": "| DALL-E (Ramesh et al., 2021)      |       | ∼ 28          |                      |  |\n| LAFITE (Zhou et al., 2021)        |       | 26.94         |                      |  |\n| GLIDE (Nichol et al., 2021)       |       | 12.24         | 12.89                |  |\n| Make-A-Scene (Gafni et al., 2022) |       |               | 11.84                |  |\n| unCLIP (AR prior)                 |       | 10.63         | 11.08                |  |\n| unCLIP (Diffusion prior)          |       | 10.39         | 10.87                |  |",
      "metadata": {
        "chunk_index": 50,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 527
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_51",
      "content": "Table 2: Comparison of FID on MS-COCO 256 × 256. We use guidance scale 1.25 for the decoder for both the AR and diffusion prior, and achieve the best results using the diffusion prior.\n\nmatching. In Figure [10,](#page-11-1) we conduct a more careful study of this question by performing human evaluations across several GLIDE guidance scales. We find that GLIDE at guidance scale 2.0 is very close to the photorealism and caption similarity of unCLIP, while still producing less diverse samples.\n\nFinally, in Figure [11](#page-11-2) we compute MS-COCO zero-shot FID [\\[23\\]](#page-19-7) while sweeping over guidance scale for both unCLIP and GLIDE, finding that guidance hurts the FID of unCLIP much less so than for GLIDE. In this evaluation, we fix the guidance scale of the unCLIP prior and only vary the guidance scale of the decoder. This is another indication that guidance hurts the diversity of GLIDE much more than unCLIP, since FID heavily penalizes non-diverse generations.",
      "metadata": {
        "chunk_index": 51,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 984
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_52",
      "content": "#### <span id=\"page-12-0\"></span>5.4 Comparison on MS-COCO\n\nIn the text-conditional image generation literature, it has become standard practice to evaluate FID on the MS-COCO [\\[28\\]](#page-19-8) validation set. We present results on this benchmark in Table [2.](#page-12-2) Like GLIDE and DALL-E, unCLIP is not directly trained on the MS-COCO training set, but can still generalize to the validation set zero-shot. We find that, compared to these other zero-shot models, unCLIP achieves a new state-of-the-art FID of 10.39 when sampling with the diffusion prior. In Figure [12,](#page-13-0) we visually compare unCLIP to various recent text-conditional image generation models on several captions from MS-COCO. We find that, like the other methods, unCLIP produces realistic scenes that capture the text prompts.",
      "metadata": {
        "chunk_index": 52,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 814
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_53",
      "content": "#### <span id=\"page-12-1\"></span>5.5 Aesthetic Quality Comparison\n\nWe additionally perform automated aesthetic quality evaluations comparing unCLIP to GLIDE. Our goal with this evaluation is to assess how well each model produces artistic illustrations and photographs. To this end, we generated 512 \"artistic\" captions using GPT-3 [\\[4\\]](#page-18-4) by prompting it with captions for existing artwork (both real and AI generated). Next, we trained a CLIP linear probe to predict human aesthetic judgments using the AVA dataset [\\[33\\]](#page-20-12) (Appendix [A\\)](#page-22-1). For each model and set of sampling hyperparameters, we produce four images for each prompt, and report the mean predicted aesthetic judgment over the full batch of 2048 images.",
      "metadata": {
        "chunk_index": 53,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 756
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_54",
      "content": "In Figure [13,](#page-14-0) we present results on our aesthetic quality evaluation. We find that guidance improves aesthetic quality for both GLIDE and unCLIP. For unCLIP, we only guide the decoder (we found that guiding the prior hurt results). We also plot the aesthetic quality against Recall[5](#page-12-3) , since guidance typically induces a trade-off\n\n<span id=\"page-12-3\"></span><sup>5</sup>Recall is computed with respect to the training dataset.\n\n<span id=\"page-13-0\"></span>![](_page_13_Figure_0.jpeg)\n\nFigure 12: Random image samples on MS-COCO prompts.\n\n<span id=\"page-14-0\"></span>![](_page_14_Figure_0.jpeg)\n\nFigure 13: Aesthetic quality evaluations comparing GLIDE and unCLIP using 512 auto-generated artistic prompts. We find that both models benefit from guidance, but unCLIP does not sacrifice recall for aesthetic quality.",
      "metadata": {
        "chunk_index": 54,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 842
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_55",
      "content": "between fidelity and diversity. Interestingly, we find that guiding unCLIP does not decrease Recall while still improving aesthetic quality according to this metric.",
      "metadata": {
        "chunk_index": 55,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 165
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_56",
      "content": "#### 6 Related Work\n\nSynthetic image generation is a well studied problem, and most popular techniques for unconditional image generation have also been applied to the text-conditional setting. Many previous works have trained GANs [21] on publicly available image captioning datasets to produce text-conditional image samples [56, 63, 49, 58, 57]. Other works have adapted the VQ-VAE approach [52] to text-conditional image generation by training autoregressive transformers on sequences of text tokens followed by image tokens [40, 12, 1]. Finally, some works have applied diffusion models to the problem, training either continuous [35] or discrete [22] diffusion models with auxiliary text encoders to handle textual input.",
      "metadata": {
        "chunk_index": 56,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 727
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_57",
      "content": "Previous works have leveraged hierarchical generative processes to create high-quality synthetic images. Razavi et al. [41] trains a multi-layer discrete autoencoder, allowing them to first sample coarse-grained latent codes and then use this as conditioning information when sampling higher-resolution latent codes. Child, Vahdat and Kautz [5, 50] generate images using VAEs with a hierarchy of latent codes that increase progressively with resolution. Concurrently with our work, Gafni et al. [17] conditions a generative image model on segmentation masks, allowing for a generative process that first samples a semantic map of an image and then conditions the generated image on this information.",
      "metadata": {
        "chunk_index": 57,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 699
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_58",
      "content": "The computational benefits of using diffusion to model a latent space has been noted by previous works. Preechakul et al. [38] propose an autoencoder framework where diffusion models are used to render latent variables as images, and a second diffusion model is used to generate these latents (similar to our diffusion prior). Vahdat et al. [51] use a score-based model for the latent space of a VAE, while Rombach et al. [42] use diffusion models on the latents obtained from a VQGAN [14] like autoencoder.",
      "metadata": {
        "chunk_index": 58,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 507
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_59",
      "content": "Since its release, CLIP [39] has been used extensively to steer generative image models towards text prompts. Galatolo et al., Patashnik et al., Murdock, Gal et al. [19, 36, 32, 18] guide GANs using gradients from a CLIP model. For diffusion models, Dhariwal and Nichol [11] introduced classifier guidance as a way to use gradients from a classifier trained on noised images to steer the model towards higher quality generations. Nichol et al. [35] train a CLIP model on noised images and guide a text-conditional diffusion model, while Crowson, Crowson [7, 8] use an unnoised CLIP model to guide unconditional or class-conditional diffusion models. Ho and Salimans [24] introduced classifier-free guidance and showed that one can perform guidance\n\n<span id=\"page-15-1\"></span>![](_page_15_Figure_0.jpeg)\n\nFigure 14: Samples from unCLIP and GLIDE for the prompt \"a red cube on top of a blue cube\".",
      "metadata": {
        "chunk_index": 59,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 897
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_60",
      "content": "<span id=\"page-15-1\"></span>![](_page_15_Figure_0.jpeg)\n\nFigure 14: Samples from unCLIP and GLIDE for the prompt \"a red cube on top of a blue cube\".\n\nimplictly from the predictions of the model with and without the conditioning information, thus removing the need for a classifier. [Nichol et al.](#page-20-6) [\\[35\\]](#page-20-6) showed classifier-free guidance works more favorably than CLIP guidance for text conditional image generation.",
      "metadata": {
        "chunk_index": 60,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 441
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_61",
      "content": "Several previous works have trained generative image models that are directly conditioned on CLIP embeddings. [Zhou et al.](#page-21-5) [\\[61\\]](#page-21-5) condition GAN models on randomly perturbed CLIP image embeddings, finding that these models can generalize to CLIP text embeddings to produce text-conditional images. [Crowson](#page-18-3) [\\[9\\]](#page-18-3) trained diffusion models conditioned on CLIP text embeddings, allowing for direct text-conditional image generation. [Wang et al.](#page-21-6) [\\[54\\]](#page-21-6) train an autoregressive generative model conditioned on CLIP image embeddings, finding that it generalizes to CLIP text embeddings well enough to allow for text-conditional image synthesis.",
      "metadata": {
        "chunk_index": 61,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 719
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_62",
      "content": "[Bordes et al.](#page-18-11) [\\[3\\]](#page-18-11) train diffusion models conditioned on image representations from contrastive models. While the diffusion models themselves cannot generate images unconditionally, the authors experimented with a simple approach for two-stage image generation by employing Kernel Density Estimation to sample image representations. By feeding these generated representations to the diffusion model, they can generate images end-to-end in a way similar to our proposed technique. However, our work differs from this in two ways: first, we use multimodal contrastive representations rather than image-only representations; second, we employ much more powerful generative models for the first stage of the generation hierarchy, and these generative models are conditioned on text.\n\n# <span id=\"page-15-0\"></span>7 Limitations and Risks",
      "metadata": {
        "chunk_index": 62,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 864
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_63",
      "content": "# <span id=\"page-15-0\"></span>7 Limitations and Risks\n\nAlthough conditioning image generation on CLIP embeddings improves diversity, this choice does come with certain limitations. In particular, unCLIP is worse at binding attributes to objects than a corresponding GLIDE model. In Figure [14,](#page-15-1) we find that unCLIP struggles more than GLIDE with a prompt where it must bind two separate objects (cubes) to two separate attributes (colors). We hypothesize that this occurs because the CLIP embedding itself does not explicitly bind attributes to objects, and find that reconstructions from the decoder often mix up attributes and objects, as shown in Figure [15.](#page-16-0) A similar and likely related issue is that unCLIP\n\n<span id=\"page-16-0\"></span>![](_page_16_Picture_0.jpeg)",
      "metadata": {
        "chunk_index": 63,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 794
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_64",
      "content": "<span id=\"page-16-0\"></span>![](_page_16_Picture_0.jpeg)\n\nFigure 15: Reconstructions from the decoder for difficult binding problems. We find that the reconstructions mix up objects and attributes. In the first two examples, the model mixes up the color of two objects. In the rightmost example, the model does not reliably reconstruct the relative size of two objects.\n\n<span id=\"page-16-1\"></span>![](_page_16_Picture_2.jpeg)\n\nFigure 16: Samples from unCLIP for the prompt, \"A sign that says deep learning.\"\n\nstruggles at producing coherent text, as illustrated in Figure [16;](#page-16-1) it is possible that the CLIP embedding does not precisely encode spelling information of rendered text. This issue is likely made worse because the BPE encoding we use obscures the spelling of the words in a caption from the model, so the model needs to have independently seen each token written out in the training images in order to learn to render it.",
      "metadata": {
        "chunk_index": 64,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 947
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_65",
      "content": "We also note that our stack still has a hard time producing details in complex scenes (Figure [17\\)](#page-17-0). We hypothesize that this is a limitation of our decoder hierarchy producing an image at a base resolution of 64 × 64 and then upsampling it. Training our unCLIP decoder at a higher base resolution should be able to alleviate this, at the cost of additional training and inference compute.\n\nAs discussed in the GLIDE paper, image generation models carry risks related to deceptive and otherwise harmful content. unCLIP's performance improvements also raise the risk profile over GLIDE. As the technology matures, it leaves fewer traces and indicators that outputs are AI-generated, making it easier to mistake generated images for authentic ones and vice versa. More research is also needed on how the change in architecture changes how the model learns biases in training data.\n\n<span id=\"page-17-0\"></span>![](_page_17_Picture_0.jpeg)",
      "metadata": {
        "chunk_index": 65,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 949
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_66",
      "content": "<span id=\"page-17-0\"></span>![](_page_17_Picture_0.jpeg)\n\n(a) A high quality photo of a dog playing in a green field next to a lake.\n\n![](_page_17_Picture_2.jpeg)\n\n(b) A high quality photo of Times Square.\n\nFigure 17: unCLIP samples show low levels of detail for some complex scenes.\n\nThe risks of these models should be assessed in relation to the particular deployment context, which includes training data, guardrails in place, the deployment space, and who will have access. A preliminary analysis of these issues in the context of the DALL·E 2 Preview platform (the first deployment of an unCLIP model), can be found in [Mishkin et al.](#page-19-14) [\\[30\\]](#page-19-14).",
      "metadata": {
        "chunk_index": 66,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 677
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_67",
      "content": "## 8 Acknowledgements\n\nWe'd like to thank Jong Wook Kim, Hyeonwoo Noh, Alec Radford, Pranav Shyam, and Ilya Sutskever for helpful discussions and contributions to our work. We'd also like to thank Yunxin Jiao for creating several figures used in the paper. We are grateful to the Acceleration and Supercomputing teams at OpenAI for their work on software and hardware infrastructure this project used.\n\n# References",
      "metadata": {
        "chunk_index": 67,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 415
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_68",
      "content": "- <span id=\"page-18-6\"></span>[1] Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, and Luke Zettlemoyer. CM3: A Causal Masked Multimodal Model of the Internet. *[arXiv:2201.07520](https://arxiv.org/abs/2201.07520)*, 2022.\n- <span id=\"page-18-14\"></span>[2] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models. *CoRR*, abs/2201.06503, 2022. URL [https:](https://arxiv.org/abs/2201.06503) [//arxiv.org/abs/2201.06503](https://arxiv.org/abs/2201.06503).\n- <span id=\"page-18-11\"></span>[3] Florian Bordes, Randall Balestriero, and Pascal Vincent. High Fidelity Visualization of What Your Self-Supervised Representation Knows About. *[arXiv:2112.09164](https://arxiv.org/abs/2112.09164)*, 2021.",
      "metadata": {
        "chunk_index": 68,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 876
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_69",
      "content": "- <span id=\"page-18-4\"></span>[4] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. *[arXiv:2005.14165](https://arxiv.org/abs/2005.14165)*, 2020.\n- <span id=\"page-18-7\"></span>[5] Rewon Child. Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images. *[arXiv:2011.10650](https://arxiv.org/abs/2011.10650)*, 2021.",
      "metadata": {
        "chunk_index": 69,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 809
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_70",
      "content": "- <span id=\"page-18-7\"></span>[5] Rewon Child. Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images. *[arXiv:2011.10650](https://arxiv.org/abs/2011.10650)*, 2021.\n- <span id=\"page-18-13\"></span>[6] Katherine Crowson. AVA Linear Probe. [https://twitter.com/RiversHaveWings/status/](https://twitter.com/RiversHaveWings/status/1472346186728173568?s=20&t=T-HRr3Gw5HRGjQaMDtRe3A) [1472346186728173568?s=20&t=T-HRr3Gw5HRGjQaMDtRe3A](https://twitter.com/RiversHaveWings/status/1472346186728173568?s=20&t=T-HRr3Gw5HRGjQaMDtRe3A), 2021.\n- <span id=\"page-18-9\"></span>[7] Katherine Crowson. CLIP guided diffusion HQ 256x256. [https://colab.research.google.com/](https://colab.research.google.com/drive/12a_Wrfi2_gwwAuN3VvMTwVMz9TfqctNj) [drive/12a\\\\_Wrfi2\\\\_gwwAuN3VvMTwVMz9TfqctNj](https://colab.research.google.com/drive/12a_Wrfi2_gwwAuN3VvMTwVMz9TfqctNj), 2021.",
      "metadata": {
        "chunk_index": 70,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 883
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_71",
      "content": "- <span id=\"page-18-10\"></span>[8] Katherine Crowson. CLIP Guided Diffusion 512x512, Secondary Model Method. [https://twitter.](https://twitter.com/RiversHaveWings/status/1462859669454536711) [com/RiversHaveWings/status/1462859669454536711](https://twitter.com/RiversHaveWings/status/1462859669454536711), 2021.\n- <span id=\"page-18-3\"></span>[9] Katherine Crowson. v-diffusion. <https://github.com/crowsonkb/v-diffusion-pytorch>, 2021.\n- <span id=\"page-18-0\"></span>[10] Karan Desai and Justin Johnson. VirTex: Learning Visual Representations from Textual Annotations. *[arXiv:2006.06666](https://arxiv.org/abs/2006.06666)*, 2020.\n- <span id=\"page-18-1\"></span>[11] Prafulla Dhariwal and Alex Nichol. Diffusion Models Beat GANs on Image Synthesis. *[arXiv:2105.05233](https://arxiv.org/abs/2105.05233)*, 2021.",
      "metadata": {
        "chunk_index": 71,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 809
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_72",
      "content": "- <span id=\"page-18-1\"></span>[11] Prafulla Dhariwal and Alex Nichol. Diffusion Models Beat GANs on Image Synthesis. *[arXiv:2105.05233](https://arxiv.org/abs/2105.05233)*, 2021.\n- <span id=\"page-18-5\"></span>[12] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, and Jie Tang. CogView: Mastering Text-to-Image Generation via Transformers. *[arXiv:2105.13290](https://arxiv.org/abs/2105.13290)*, 2021.\n- <span id=\"page-18-12\"></span>[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. *[arXiv:2010.11929](https://arxiv.org/abs/2010.11929)*, 2020.",
      "metadata": {
        "chunk_index": 72,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 849
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_73",
      "content": "- <span id=\"page-18-8\"></span>[14] Patrick Esser, Robin Rombach, and Björn Ommer. Taming Transformers for High-Resolution Image Synthesis. *[arXiv:2012.09841](https://arxiv.org/abs/2012.09841)*, 2020.\n- <span id=\"page-18-2\"></span>[15] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-Aware Minimization for Efficiently Improving Generalization. *[arXiv:2010.01412](https://arxiv.org/abs/2010.01412)*, 2020.",
      "metadata": {
        "chunk_index": 73,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 435
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_74",
      "content": "- <span id=\"page-19-1\"></span>[16] Andreas Fürst, Elisabeth Rumetshofer, Viet Thuong Tran, Hubert Ramsauer, Fei Tang, Johannes Lehner, D P Kreil, Michael K Kopp, Günter Klambauer, Angela Bitto-Nemling, and Sepp Hochreiter. CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP, 2022. URL [https://openreview.](https://openreview.net/forum?id=qw674L9PfQE) [net/forum?id=qw674L9PfQE](https://openreview.net/forum?id=qw674L9PfQE).\n- <span id=\"page-19-6\"></span>[17] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors. *[arXiv:2203.13131](https://arxiv.org/abs/2203.13131)*, 2022.\n- <span id=\"page-19-13\"></span>[18] Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, and Daniel Cohen-Or. StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators. *[arXiv:2108.00946](https://arxiv.org/abs/2108.00946)*, 2021.",
      "metadata": {
        "chunk_index": 74,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 925
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_75",
      "content": "- <span id=\"page-19-11\"></span>[19] Federico A. Galatolo, Mario G. C. A. Cimino, and Gigliola Vaglini. Generating images from caption and vice versa via CLIP-Guided Generative Latent Space Search. *[arXiv:2102.01645](https://arxiv.org/abs/2102.01645)*, 2021.\n- <span id=\"page-19-5\"></span>[20] Gabriel Goh, Nick Cammarata †, Chelsea Voss †, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, and Chris Olah. Multimodal Neurons in Artificial Neural Networks. *Distill*, 2021. doi: 10.23915/distill.00030. https://distill.pub/2021/multimodal-neurons.\n- <span id=\"page-19-9\"></span>[21] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Networks. *[arXiv:1406.2661](https://arxiv.org/abs/1406.2661)*, 2014.",
      "metadata": {
        "chunk_index": 75,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 817
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_76",
      "content": "- <span id=\"page-19-10\"></span>[22] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector Quantized Diffusion Model for Text-to-Image Synthesis. *[arXiv:2111.14822](https://arxiv.org/abs/2111.14822)*, 2021.\n- <span id=\"page-19-7\"></span>[23] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. *Advances in Neural Information Processing Systems 30 (NIPS 2017)*, 2017.\n- <span id=\"page-19-4\"></span>[24] Jonathan Ho and Tim Salimans. Classifier-Free Diffusion Guidance. In *NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications*, 2021. URL [https://openreview.net/](https://openreview.net/forum?id=qw8AKxfYbI) [forum?id=qw8AKxfYbI](https://openreview.net/forum?id=qw8AKxfYbI).",
      "metadata": {
        "chunk_index": 76,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 869
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_77",
      "content": "- <span id=\"page-19-2\"></span>[25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. *[arXiv:2006.11239](https://arxiv.org/abs/2006.11239)*, 2020.\n- <span id=\"page-19-3\"></span>[26] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded Diffusion Models for High Fidelity Image Generation. *[arXiv:2106.15282](https://arxiv.org/abs/2106.15282)*, 2021.\n- <span id=\"page-19-15\"></span>[27] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. *[arXiv:1412.6980](https://arxiv.org/abs/1412.6980)*, 2014.\n- <span id=\"page-19-8\"></span>[28] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. Microsoft COCO: Common Objects in Context. *[arXiv:1405.0312](https://arxiv.org/abs/1405.0312)*, 2014.",
      "metadata": {
        "chunk_index": 77,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 904
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_78",
      "content": "- <span id=\"page-19-16\"></span>[29] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. *[arXiv:1711.05101](https://arxiv.org/abs/1711.05101)*, 2017.\n- <span id=\"page-19-14\"></span>[30] Pamela Mishkin, Lama Ahmad, Miles Brundage, Gretchen Krueger, and Girish Sastry. DALL·E 2 Preview - Risks and Limitations. 2022. URL [https://github.com/openai/dalle-2-preview/](https://github.com/openai/dalle-2-preview/blob/main/system-card.md) [blob/main/system-card.md](https://github.com/openai/dalle-2-preview/blob/main/system-card.md).\n- <span id=\"page-19-0\"></span>[31] Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. SLIP: Self-supervision meets Language-Image Pre-training. *[arXiv:2112.12750](https://arxiv.org/abs/2112.12750)*, 2021.",
      "metadata": {
        "chunk_index": 78,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 765
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_79",
      "content": "- <span id=\"page-19-12\"></span>[32] Ryan Murdock. The Big Sleep. [https://twitter.com/advadnoun/status/](https://twitter.com/advadnoun/status/1351038053033406468) [1351038053033406468](https://twitter.com/advadnoun/status/1351038053033406468), 2021.",
      "metadata": {
        "chunk_index": 79,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 249
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_80",
      "content": "- <span id=\"page-20-12\"></span>[33] Naila Murray, Luca Marchesotti, and Florent Perronnin. AVA: A large-scale database for aesthetic visual analysis. In *2012 IEEE Conference on Computer Vision and Pattern Recognition*, pages 2408–2415, 2012. doi: 10.1109/CVPR.2012.6247954.\n- <span id=\"page-20-7\"></span>[34] Alex Nichol and Prafulla Dhariwal. Improved Denoising Diffusion Probabilistic Models. *[arXiv:2102.09672](https://arxiv.org/abs/2102.09672)*, 2021.\n- <span id=\"page-20-6\"></span>[35] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. *[arXiv:2112.10741](https://arxiv.org/abs/2112.10741)*, 2021.",
      "metadata": {
        "chunk_index": 80,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 768
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_81",
      "content": "- <span id=\"page-20-16\"></span>[36] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery. *[arXiv:2103.17249](https://arxiv.org/abs/2103.17249)*, 2021.\n- <span id=\"page-20-10\"></span>[37] Karl Pearson. LIII. On lines and planes of closest fit to systems of points in space, November 1901. URL <https://doi.org/10.1080/14786440109462720>.\n- <span id=\"page-20-15\"></span>[38] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn. Diffusion Autoencoders: Toward a Meaningful and Decodable Representation. *[arXiv:2111.15640](https://arxiv.org/abs/2111.15640)*, 2021.",
      "metadata": {
        "chunk_index": 81,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 677
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_82",
      "content": "- <span id=\"page-20-1\"></span>[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision. *[arXiv:2103.00020](https://arxiv.org/abs/2103.00020)*, 2021.\n- <span id=\"page-20-5\"></span>[40] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-Shot Text-to-Image Generation. *[arXiv:2102.12092](https://arxiv.org/abs/2102.12092)*, 2021.\n- <span id=\"page-20-13\"></span>[41] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating Diverse High-Fidelity Images with VQ-VAE-2. *[arXiv:1906.00446](https://arxiv.org/abs/1906.00446)*, 2019.",
      "metadata": {
        "chunk_index": 82,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 805
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_83",
      "content": "- <span id=\"page-20-9\"></span>[42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-Resolution Image Synthesis with Latent Diffusion Models. *[arXiv:2112.10752](https://arxiv.org/abs/2112.10752)*, 2021.\n- <span id=\"page-20-8\"></span>[43] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and Mohammad Norouzi. Image Super-Resolution via Iterative Refinement. *[arXiv:arXiv:2104.07636](https://arxiv.org/abs/arXiv:2104.07636)*, 2021.\n- <span id=\"page-20-0\"></span>[44] Mert Bulent Sariyildiz, Julien Perez, and Diane Larlus. Learning Visual Representations with Caption Annotations. *[arXiv:2008.01392](https://arxiv.org/abs/2008.01392)*, 2020.\n- <span id=\"page-20-2\"></span>[45] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt Keutzer. How Much Can CLIP Benefit Vision-and-Language Tasks? *[arXiv:2107.06383](https://arxiv.org/abs/2107.06383)*, 2021.",
      "metadata": {
        "chunk_index": 83,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 969
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_84",
      "content": "- <span id=\"page-20-3\"></span>[46] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep Unsupervised Learning using Nonequilibrium Thermodynamics. *[arXiv:1503.03585](https://arxiv.org/abs/1503.03585)*, 2015.\n- <span id=\"page-20-17\"></span>[47] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising Diffusion Implicit Models. *[arXiv:2010.02502](https://arxiv.org/abs/2010.02502)*, 2020.\n- <span id=\"page-20-4\"></span>[48] Yang Song and Stefano Ermon. Improved Techniques for Training Score-Based Generative Models. *[arXiv:2006.09011](https://arxiv.org/abs/2006.09011)*, 2020.\n- <span id=\"page-20-11\"></span>[49] Ming Tao, Hao Tang, Songsong Wu, Nicu Sebe, Xiao-Yuan Jing, Fei Wu, and Bingkun Bao. DF-GAN: Deep Fusion Generative Adversarial Networks for Text-to-Image Synthesis. *[arXiv:2008.05865](https://arxiv.org/abs/2008.05865)*, 2020.",
      "metadata": {
        "chunk_index": 84,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 875
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_85",
      "content": "- <span id=\"page-20-14\"></span>[50] Arash Vahdat and Jan Kautz. NVAE: A Deep Hierarchical Variational Autoencoder. *[arXiv:2007.03898](https://arxiv.org/abs/2007.03898)*, 2020.",
      "metadata": {
        "chunk_index": 85,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 176
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_86",
      "content": "- <span id=\"page-21-12\"></span>[51] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based Generative Modeling in Latent Space. In *Neural Information Processing Systems (NeurIPS)*, 2021.\n- <span id=\"page-21-11\"></span>[52] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural Discrete Representation Learning. *[arXiv:1711.00937](https://arxiv.org/abs/1711.00937)*, 2017.\n- <span id=\"page-21-4\"></span>[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. *[arXiv:1706.03762](https://arxiv.org/abs/1706.03762)*, 2017.\n- <span id=\"page-21-6\"></span>[54] Zihao Wang, Wei Liu, Qian He, Xinglong Wu, and Zili Yi. CLIP-GEN: Language-Free Training of a Text-to-Image Generator with CLIP. *[arXiv:2203.00386](https://arxiv.org/abs/2203.00386)*, 2022.",
      "metadata": {
        "chunk_index": 86,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 861
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_87",
      "content": "- <span id=\"page-21-2\"></span>[55] Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang. GAN Inversion: A Survey. *[arXiv:2101.05278](https://arxiv.org/abs/2101.05278)*, 2021.\n- <span id=\"page-21-7\"></span>[56] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks. *[arXiv:1711.10485](https://arxiv.org/abs/1711.10485)*, 2017.\n- <span id=\"page-21-9\"></span>[57] Hui Ye, Xiulong Yang, Martin Takac, Rajshekhar Sunderraman, and Shihao Ji. Improving Text-to-Image Synthesis Using Contrastive Learning. *[arXiv:2107.02423](https://arxiv.org/abs/2107.02423)*, 2021.\n- <span id=\"page-21-10\"></span>[58] Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang. Cross-Modal Contrastive Learning for Text-to-Image Generation. *[arXiv:2101.04702](https://arxiv.org/abs/2101.04702)*, 2021.",
      "metadata": {
        "chunk_index": 87,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 957
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_88",
      "content": "- <span id=\"page-21-3\"></span>[59] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing a Practical Degradation Model for Deep Blind Image Super-Resolution. *2021 IEEE/CVF International Conference on Computer Vision (ICCV)*, Oct 2021. doi: 10.1109/iccv48922.2021.00475. URL [http://dx.doi.org/10.1109/](http://dx.doi.org/10.1109/ICCV48922.2021.00475) [ICCV48922.2021.00475](http://dx.doi.org/10.1109/ICCV48922.2021.00475).\n- <span id=\"page-21-0\"></span>[60] Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D. Manning, and Curtis P. Langlotz. Contrastive Learning of Medical Visual Representations from Paired Images and Text. *[arXiv:2010.00747](https://arxiv.org/abs/2010.00747)*, 2020.\n- <span id=\"page-21-5\"></span>[61] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong Sun. LAFITE: Towards Language-Free Training for Text-to-Image Generation. *[arXiv:2111.13792](https://arxiv.org/abs/2111.13792)*, 2021.",
      "metadata": {
        "chunk_index": 88,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 990
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_89",
      "content": "- <span id=\"page-21-1\"></span>[62] Jun-Yan Zhu, Philipp Krähenbühl, Eli Shechtman, and Alexei A. Efros. Generative Visual Manipulation on the Natural Image Manifold. *[arXiv:1609.03552](https://arxiv.org/abs/1609.03552)*, 2016.\n- <span id=\"page-21-8\"></span>[63] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-to-Image Synthesis. *[arXiv:1904.01310](https://arxiv.org/abs/1904.01310)*, 2019.",
      "metadata": {
        "chunk_index": 89,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 456
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_90",
      "content": "# <span id=\"page-22-1\"></span>A Linear Probes for Evaluations",
      "metadata": {
        "chunk_index": 90,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 61
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_91",
      "content": "For our evaluations, we leverage two new linear probes on top of a CLIP ViT-L/14 [\\[13\\]](#page-18-12) model. To automate aesthetic quality evaluations, we follow the procedure used by [Crowson](#page-18-13) [\\[6\\]](#page-18-13), training a linear regression model on images and mean ratings from the AVA dataset [\\[33\\]](#page-20-12). To reduce the cost of hyperparameter sweeps before conducting human evaluations, we train a logistic regression model to predict win probabilities between pairs of images. To train this model, we used 15,000 pairwise image comparisons gathered from all of our previous human evaluations. For each comparison i, we computed CLIP image embeddings x<sup>i</sup> and y<sup>i</sup> for the two images in the pair. We then trained a linear model f(x) such that 1/(1 + exp (f(xi) − f(yi))) approximates the probability that a human prefers the image for y<sup>i</sup>",
      "metadata": {
        "chunk_index": 91,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 896
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_92",
      "content": ". We then trained a linear model f(x) such that 1/(1 + exp (f(xi) − f(yi))) approximates the probability that a human prefers the image for y<sup>i</sup> . This can be reduced to a logistic regression problem with inputs equal to y<sup>i</sup> − x<sup>i</sup> .",
      "metadata": {
        "chunk_index": 92,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 261
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_93",
      "content": "# B Error Bars for Human Evaluation\n\nWhen computing error bars for human evaluations, we use the normal approximation interval with p = 0.95. We expect the normal approximation to be accurate for such a large sample size of n = 1000.",
      "metadata": {
        "chunk_index": 93,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 233
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_94",
      "content": "## <span id=\"page-22-0\"></span>C Training Details\n\nThe unCLIP models used for the experiments in this paper were trained with the hyperparameters described below, unless otherwise noted. We additionally trained a production version of unCLIP using similarly sized models but with modified architectures and trained for longer; we include changes to accommodate product and safety requirements (e.g. inpainting, preventing unwanted memorization), and train on a larger dataset that is filtered for aesthetic quality and safety. We report model and training hyperparameters for the paper models in Table [3.](#page-23-0) All models were trained using Adam [\\[27\\]](#page-19-15) with corrected weight decay [\\[29\\]](#page-19-16) and momentum β<sup>1</sup> = 0.9.",
      "metadata": {
        "chunk_index": 94,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 759
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_95",
      "content": "Our CLIP model uses a ViT-H/16 [\\[13\\]](#page-18-12) image encoder that consumes 256 × 256 resolution images, and has width 1280 with 32 Transformer [\\[53\\]](#page-21-4) blocks. The text encoder also follows the architecture described in [Radford et al.](#page-20-1) [\\[39\\]](#page-20-1): it is a Transformer [\\[53\\]](#page-21-4) with a causal attention mask, with width 1024 and 24 Transformer blocks. Both models are trained with learning rate 3 × 10<sup>−</sup><sup>4</sup> and SAM [\\[15\\]](#page-18-2) with ρ = 0.1, where the perturbations are applied independently by the replicas, each of which uses batch size 64. The remaining hyperparameters are the same as those reported in [Radford et al.](#page-20-1) [\\[39\\]](#page-20-1).",
      "metadata": {
        "chunk_index": 95,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 735
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_96",
      "content": "When training the encoder, we sample from the CLIP [\\[39\\]](#page-20-1) and DALL-E [\\[40\\]](#page-20-5) datasets (approximately 650M images in total) with equal probability. When training the decoder, upsamplers, and prior, we use only the DALL-E dataset [\\[40\\]](#page-20-5) (approximately 250M images). Incorporating the noisier CLIP dataset while training the generative stack negatively impacted sample quality in our initial evaluations.\n\nOur decoder architecture is the 3.5 billion parameter GLIDE model, with the same architecture and diffusion hyperparameters as in [Nichol et al.](#page-20-6) [\\[35\\]](#page-20-6). We train with learned sigma and sample with 250 strided sampling steps as in [Nichol and Dhariwal](#page-20-7) [\\[34\\]](#page-20-7).",
      "metadata": {
        "chunk_index": 96,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 756
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_97",
      "content": "We use the ADMNet architecture [\\[11\\]](#page-18-1) for the upsamplers. In the first upsampling stage, we use a cosine noising schedule, 320 channels and a depth of 3 resblocks per resolution inside the ADMNet. We also apply gaussian blur (kernel size 3, sigma 0.6) as described in [Saharia et al.](#page-20-8) [\\[43\\]](#page-20-8). In the second upsampling stage, we use a linear noising schedule, 192 channels, a depth of 2 resblocks per resolution, and train with the BSR degradation from [Rombach et al.](#page-20-9) [\\[42\\]](#page-20-9). Neither upsampler uses attention. To reduce inference time, we use DDIM [\\[47\\]](#page-20-17) and manually tune the number of steps, with 27 steps for 256 × 256 model, and 15 steps for the 1024 × 1024 model.",
      "metadata": {
        "chunk_index": 97,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 750
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_98",
      "content": "For the AR prior, we use a Transformer text encoder with width 2048 and 24 blocks and a decoder with a causal attention mask, width 1664, and 24 blocks. For the diffusion prior, we use a Transformer with width 2048 and 24 blocks, and sample with Analytic DPM [\\[2\\]](#page-18-14) with 64 strided sampling steps. To reuse hyperparameters tuned for diffusion noise schedules on images from [Dhariwal and Nichol](#page-18-1) [\\[11\\]](#page-18-1), we scale the CLIP embedding inputs by 17.2 to match the empirical variance of RGB pixel values of ImageNet images scaled to [−1, 1].\n\n<span id=\"page-23-0\"></span>",
      "metadata": {
        "chunk_index": 98,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 606
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_99",
      "content": "|                          | AR prior | Diffusion prior | 64           | 64 → 256  | 256 → 1024  |\n|--------------------------|----------|-----------------|--------------|-----------|-------------|\n| Diffusion steps          | -        | 1000            | 1000         | 1000      | 1000        |\n| Noise schedule           | -        | cosine          | cosine       | cosine    | linear      |\n| Sampling steps           | -        | 64              | 250          | 27        | 15          |\n| Sampling variance method | -        | analytic [2]    | learned [34] | DDIM [47] | DDIM [47]   |\n| Crop fraction            | -        | -               | -            | 0.25      | 0.25        |\n| Model size               | 1B       | 1B              | 3.5B         | 700M      | 300M        |\n| Channels                 | -        | -               | 512          | 320       | 192         |\n| Depth                    | -        | -               | 3            | 3         | 2           |",
      "metadata": {
        "chunk_index": 99,
        "content_type": "text",
        "has_table": true,
        "has_figure": false,
        "char_count": 989
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_100",
      "content": "| Channels                 | -        | -               | 512          | 320       | 192         |\n| Depth                    | -        | -               | 3            | 3         | 2           |\n| Channels multiple        | -        | -               | 1,2,3,4      | 1,2,3,4   | 1,1,2,2,4,4 |\n| Heads channels           | -        | -               | 64           | -         | -           |\n| Attention resolution     | -        | -               | 32,16,8      | -         | -           |\n| Text encoder context     | 256      | 256             | 256          | -         | -           |\n| Text encoder width       | 2048     | 2048            | 2048         | -         | -           |\n| Text encoder depth       | 24       | 24              | 24           | -         | -           |\n| Text encoder heads       | 32       | 32              | 32           | -         | -           |\n| Latent decoder context   | 384      | -               | -            | -         | -           |",
      "metadata": {
        "chunk_index": 100,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 989
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_101",
      "content": "| Text encoder heads       | 32       | 32              | 32           | -         | -           |\n| Latent decoder context   | 384      | -               | -            | -         | -           |\n| Latent decoder width     | 1664     | -               | -            | -         | -           |\n| Latent decoder depth     | 24       | -               | -            | -         | -           |\n| Latent decoder heads     | 26       | -               | -            | -         | -           |\n| Dropout                  | -        | -               | 0.1          | 0.1       | -           |\n| Weight decay             | 4.0e-2   | 6.0e-2          | -            | -         | -           |\n| Batch size               | 4096     | 4096            | 2048         | 1024      | 512         |\n| Iterations               | 1M       | 600K            | 800K         | 1M        | 1M          |\n| Learning rate            | 1.6e-4   | 1.1e-4          | 1.2e-4       | 1.2e-4    | 1.0e-4      |",
      "metadata": {
        "chunk_index": 101,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 989
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_102",
      "content": "| Iterations               | 1M       | 600K            | 800K         | 1M        | 1M          |\n| Learning rate            | 1.6e-4   | 1.1e-4          | 1.2e-4       | 1.2e-4    | 1.0e-4      |\n| Adam β2                  | 0.91     | 0.96            | 0.999        | 0.999     | 0.999       |\n| Adam                     | 1.0e-10  | 1.0e-6          | 1.0e-8       | 1.0e-8    | 1.0e-8      |\n| EMA decay                | 0.999    | 0.9999          | 0.9999       | 0.9999    | 0.9999      |",
      "metadata": {
        "chunk_index": 102,
        "content_type": "text",
        "has_table": false,
        "has_figure": false,
        "char_count": 494
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_chunk_103",
      "content": "Table 3: Hyperparameters for the models\n\n# D Random samples\n\nIn Figures [18,](#page-24-0) [19](#page-25-0) and [20](#page-26-0) we show random samples from our production model for some of the prompts from Figure [1.](#page-1-0)\n\n<span id=\"page-24-0\"></span>![](_page_24_Picture_2.jpeg)\n\nFigure 18: Random samples from unCLIP for prompt \"Vibrant portrait painting of Salvador Dali with a robotic half face\"\n\n<span id=\"page-25-0\"></span>![](_page_25_Picture_0.jpeg)\n\nFigure 19: Random samples from unCLIP for prompt \"A close up of a handpalm with leaves growing from it.\"\n\n<span id=\"page-26-0\"></span>![](_page_26_Picture_0.jpeg)\n\nFigure 20: Random samples from unCLIP for prompt \"A teddybear on a skateboard in Times Square.\"",
      "metadata": {
        "chunk_index": 103,
        "content_type": "text",
        "has_table": false,
        "has_figure": true,
        "char_count": 725
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_table_1",
      "content": "| unCLIP Prior | Photorealism | Caption Similarity | Diversity    |\n|--------------|--------------|--------------------|--------------|\n| AR           | 47.1% ± 3.1% | 41.1% ± 3.0%       | 62.6% ± 3.0% |\n| Diffusion    | 48.9% ± 3.1% | 45.3% ± 3.0%       | 70.5% ± 2.8% |\n",
      "metadata": {
        "content_type": "table",
        "table_id": "table_1",
        "headers": [
          "unCLIP Prior",
          "Photorealism",
          "Caption Similarity",
          "Diversity"
        ],
        "row_count": 2
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_table_2",
      "content": "| Model                             | FID   | Zero-shot FID | Zero-shot FID (filt) |  |\n|-----------------------------------|-------|---------------|----------------------|--|\n| AttnGAN (Xu et al., 2017)         | 35.49 |               |                      |  |\n| DM-GAN (Zhu et al., 2019)         | 32.64 |               |                      |  |\n| DF-GAN (Tao et al., 2020)         | 21.42 |               |                      |  |\n| DM-GAN + CL (Ye et al., 2021)     | 20.79 |               |                      |  |\n| XMC-GAN (Zhang et al., 2021)      | 9.33  |               |                      |  |\n| LAFITE (Zhou et al., 2021)        | 8.12  |               |                      |  |\n| Make-A-Scene (Gafni et al., 2022) | 7.55  |               |                      |  |\n| DALL-E (Ramesh et al., 2021)      |       | ∼ 28          |                      |  |\n| LAFITE (Zhou et al., 2021)        |       | 26.94         |                      |  |\n| GLIDE (Nichol et al., 2021)       |       | 12.24         | 12.89                |  |\n| Make-A-Scene (Gafni et al., 2022) |       |               | 11.84                |  |\n| unCLIP (AR prior)                 |       | 10.63         | 11.08                |  |\n| unCLIP (Diffusion prior)          |       | 10.39         | 10.87                |  |\n",
      "metadata": {
        "content_type": "table",
        "table_id": "table_2",
        "headers": [
          "Model",
          "FID",
          "Zero-shot FID",
          "Zero-shot FID (filt)",
          ""
        ],
        "row_count": 13
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_table_3",
      "content": "|                          | AR prior | Diffusion prior | 64           | 64 → 256  | 256 → 1024  |\n|--------------------------|----------|-----------------|--------------|-----------|-------------|\n| Diffusion steps          | -        | 1000            | 1000         | 1000      | 1000        |\n| Noise schedule           | -        | cosine          | cosine       | cosine    | linear      |\n| Sampling steps           | -        | 64              | 250          | 27        | 15          |\n| Sampling variance method | -        | analytic [2]    | learned [34] | DDIM [47] | DDIM [47]   |\n| Crop fraction            | -        | -               | -            | 0.25      | 0.25        |\n| Model size               | 1B       | 1B              | 3.5B         | 700M      | 300M        |\n| Channels                 | -        | -               | 512          | 320       | 192         |\n| Depth                    | -        | -               | 3            | 3         | 2           |\n| Channels multiple        | -        | -               | 1,2,3,4      | 1,2,3,4   | 1,1,2,2,4,4 |\n| Heads channels           | -        | -               | 64           | -         | -           |\n| Attention resolution     | -        | -               | 32,16,8      | -         | -           |\n| Text encoder context     | 256      | 256             | 256          | -         | -           |\n| Text encoder width       | 2048     | 2048            | 2048         | -         | -           |\n| Text encoder depth       | 24       | 24              | 24           | -         | -           |\n| Text encoder heads       | 32       | 32              | 32           | -         | -           |\n| Latent decoder context   | 384      | -               | -            | -         | -           |\n| Latent decoder width     | 1664     | -               | -            | -         | -           |\n| Latent decoder depth     | 24       | -               | -            | -         | -           |\n| Latent decoder heads     | 26       | -               | -            | -         | -           |\n| Dropout                  | -        | -               | 0.1          | 0.1       | -           |\n| Weight decay             | 4.0e-2   | 6.0e-2          | -            | -         | -           |\n| Batch size               | 4096     | 4096            | 2048         | 1024      | 512         |\n| Iterations               | 1M       | 600K            | 800K         | 1M        | 1M          |\n| Learning rate            | 1.6e-4   | 1.1e-4          | 1.2e-4       | 1.2e-4    | 1.0e-4      |\n| Adam β2                  | 0.91     | 0.96            | 0.999        | 0.999     | 0.999       |\n| Adam                     | 1.0e-10  | 1.0e-6          | 1.0e-8       | 1.0e-8    | 1.0e-8      |\n| EMA decay                | 0.999    | 0.9999          | 0.9999       | 0.9999    | 0.9999      |\n",
      "metadata": {
        "content_type": "table",
        "table_id": "table_3",
        "headers": [
          "",
          "AR prior",
          "Diffusion prior",
          "64",
          "64 → 256",
          "256 → 1024"
        ],
        "row_count": 27
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_1",
      "content": "<span id=\"page-1-0\"></span>\n\nFigure: vibrant portrait painting of Salvador Dalí with a robotic half face a shiba inu wearing a beret and black turtleneck a close up of a handpalm with leaves growing from it\n\nVisual Description: I'm unable to interpret or analyze the content of images like academic figures, graphs, or data visualizations directly. However, if you provide details about the axes labels, data trends, or key findings, I can help you describe or summarize that information!",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_1",
        "caption": "vibrant portrait painting of Salvador Dalí with a robotic half face a shiba inu wearing a beret and black turtleneck a close up of a handpalm with leaves growing from it",
        "image_key": "_page_1_Picture_0.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_2",
      "content": "Figure: ![](_page_1_Picture_4.jpeg)\n\nVisual Description: I'm unable to view the academic figure, but I can offer guidance on how to describe one. \n\nTypically, you would start with the axes labels, detailing what each axis represents (e.g., \"Time\" on the x-axis and \"Sales\" on the y-axis). Then, note any observable data trends, such as increasing or decreasing patterns. Finally, conclude with the key findings, emphasizing any significant results or correlations displayed in the figure.\n\nIf you provide specific details about the figure, I can help craft a more tailored description!",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_2",
        "caption": "![](_page_1_Picture_4.jpeg)",
        "image_key": "_page_1_Picture_2.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_3",
      "content": "Figure: an espresso machine that makes coffee from human souls, artstation panda mad scientist mixing sparkling chemicals, artstation a corgi's head depicted as an explosion of a nebula\n\nVisual Description: The figure depicts an espresso machine with anthropomorphic features, emphasizing a whimsical yet unsettling theme. While there are no conventional axes or data trends, the visual representation suggests a metaphorical exploration of consuming human essence through coffee. Key findings may include commentary on the fusion of life and consumption, and the surrealistic portrayal calls attention to the ethics of creation and desires within the realm of artistry and science.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_3",
        "caption": "an espresso machine that makes coffee from human souls, artstation panda mad scientist mixing sparkling chemicals, artstation a corgi's head depicted as an explosion of a nebula",
        "image_key": "_page_1_Picture_6.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_4",
      "content": "Figure: ![](_page_1_Picture_10.jpeg)\n\nVisual Description: The figure likely illustrates a whimsical representation of a panda in a laboratory setting, engaging with various beakers and performing experiments. There appears to be a focus on colorful liquids and a dynamic atmosphere, suggesting an exploration of chemical reactions or scientific processes. While specific axes labels and data trends are not applicable, the overall theme emphasizes curiosity and innovation in a playful manner within a scientific context.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_4",
        "caption": "![](_page_1_Picture_10.jpeg)",
        "image_key": "_page_1_Picture_8.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_5",
      "content": "Figure: ![](_page_1_Picture_14.jpeg)\n\nVisual Description: I'm unable to describe the academic figure as requested, as the image you provided seems to be an artistic representation rather than an academic figure with data trends and axes labels. If you need assistance with a specific academic figure or context, please provide more detail or describe the content!",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_5",
        "caption": "![](_page_1_Picture_14.jpeg)",
        "image_key": "_page_1_Picture_12.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_6",
      "content": "Figure: Selected 1024 × 1024 samples from a production version of our model.\n\nVisual Description: The figure depicts a teddy bear riding a skateboard in a busy urban setting, contrasting playful innocence with a bustling backdrop of pedestrians and bright signage. Although there are no visible axes or specific data trends, the scene conveys themes of juxtaposition and urban life, highlighting the blend of childlike imagery with a vibrant city environment. Key findings could emphasize the surreal and humorous elements of the work, suggesting a commentary on societal dynamics and the intersection of childhood and adult experiences.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_6",
        "caption": "Selected 1024 × 1024 samples from a production version of our model.",
        "image_key": "_page_1_Picture_16.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_7",
      "content": "Figure: A high-level overview of unCLIP. Above the dotted line, we depict the CLIP training process, through which we learn a joint representation space for text and images. Below the dotted line, we depict our text-to-image generation process: a CLIP text embedding is first fed to an autoregressive or diffusion prior to produce an image embedding, and then this embedding is used to condition a diffusion decoder which produces a final image. Note that the CLIP model is frozen during training of the prior and decoder.\n\nVisual Description: The figure provides an overview of the unCLIP framework, illustrating the processes involved in text-to-image generation. The axes aren't explicitly labeled, but the structure divides the CLIP training process (above the dotted line) from the image generation workflow (below the dotted line). Key findings include that a CLIP text embedding, generated from a textual description (\"a corgi playing a flame throwing trumpet\"), is utilized to condition an autoregressive or diffusion model, eventually leading to image generation through a frozen decoder. The visual highlights the interconnectedness of text and image representations within the framework.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_7",
        "caption": "A high-level overview of unCLIP. Above the dotted line, we depict the CLIP training process, through which we learn a joint representation space for text and images. Below the dotted line, we depict our text-to-image generation process: a CLIP text embedding is first fed to an autoregressive or diffusion prior to produce an image embedding, and then this embedding is used to condition a diffusion decoder which produces a final image. Note that the CLIP model is frozen during training of the prior and decoder.",
        "image_key": "_page_2_Picture_0.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_8",
      "content": "<span id=\"page-4-0\"></span>\n\nFigure: Variations of an input image by encoding with CLIP and then decoding with a diffusion model. The variations preserve both semantic information like presence of a clock in the painting and the overlapping strokes in the logo, as well as stylistic elements like the surrealism in the painting and the color gradients in the logo, while varying the non-essential details.\n\nVisual Description: The academic figure consists of two main sections, showcasing variations derived from an input image encoded with CLIP and decoded using a diffusion model. The left section features multiple renditions of a surrealist painting, maintaining essential semantic elements like clocks and stylistic components such as color gradients and brush strokes, while altering non-essential details. The right section presents variations of a logo, emphasizing similar preservation of styling and structure, with distinct visual transformations across the grid while ensuring the core design elements are intact.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_8",
        "caption": "Variations of an input image by encoding with CLIP and then decoding with a diffusion model. The variations preserve both semantic information like presence of a clock in the painting and the overlapping strokes in the logo, as well as stylistic elements like the surrealism in the painting and the color gradients in the logo, while varying the non-essential details.",
        "image_key": "_page_4_Picture_0.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_9",
      "content": "<span id=\"page-5-0\"></span>\n\nFigure: Variations between two images by interpolating their CLIP image embedding and then decoding with a diffusion model. We fix the decoder seed across each row. The intermediate variations naturally blend the content and style from both input images.\n\nVisual Description: The academic figure illustrates the interpolation of image embeddings from two distinct inputs, demonstrating how the variations blend both content and style across multiple interpolations. The top row consists of variations between \"Starry Night\" and images of corgi dogs, showcasing a gradual shift in artistic style while merging elements from each image. The bottom row features a series of pottery forms, transitioning smoothly between different designs, emphasizing the flexibility of the diffusion model in generating cohesive and visually appealing blends from the provided images. Each column represents a specific interpolation step, visually narrating the transformation process.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_9",
        "caption": "Variations between two images by interpolating their CLIP image embedding and then decoding with a diffusion model. We fix the decoder seed across each row. The intermediate variations naturally blend the content and style from both input images.",
        "image_key": "_page_5_Picture_0.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_10",
      "content": "<span id=\"page-6-0\"></span>\n\nFigure: a photo of a cat  $\\rightarrow$  an anime drawing of a super saiyan cat, artstation\n\nVisual Description: The figure presents a series of images depicting the transformation of a cat from a realistic photograph to an anime representation styled as a super saiyan cat. The horizontal axis represents the progression of artistic styles, ranging from a naturalistic depiction to an exaggerated, anime-inspired version, while the vertical axis reflects the intensity of the character's features, such as facial expression and energy levels. Key findings highlight that as the style shifts toward the anime interpretation, there is a marked increase in the cat's expressive traits and visual dynamism, showcasing a trend toward more vibrant and bold artistic choices.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_10",
        "caption": "a photo of a cat  $\\rightarrow$  an anime drawing of a super saiyan cat, artstation",
        "image_key": "_page_6_Picture_0.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_11",
      "content": "Figure: a photo of a victorian house  $\\rightarrow$  a photo of a modern house\n\nVisual Description: The figure presents a visual comparison of architectural styles, with the left side showcasing a Victorian house and the progression towards modern house designs on the right. The x-axis likely represents the timeline of architectural evolution, while the y-axis could indicate design complexity or style sophistication. Key findings suggest a trend of simplification and minimalism in design, transitioning from the intricate details of Victorian architecture to the sleek lines and open spaces of modern homes.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_11",
        "caption": "a photo of a victorian house  $\\rightarrow$  a photo of a modern house",
        "image_key": "_page_6_Picture_2.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_12",
      "content": "Figure: a photo of an adult lion  $\\rightarrow$  a photo of lion cub\n\nVisual Description: The figure presents a visual representation showing the developmental stages of lions from adult to cub. The x-axis indicates age progression from an adult lion to a lion cub, while the y-axis reflects physical characteristics such as facial features or size. Key findings suggest a gradual transition in morphology, with notable changes in facial structure and size, emphasizing the distinct differences between adult lions and their younger counterparts.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_12",
        "caption": "a photo of an adult lion  $\\rightarrow$  a photo of lion cub",
        "image_key": "_page_6_Picture_4.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_13",
      "content": "Figure: a photo of a landscape in winter  $\\rightarrow$  a photo of a landscape in fall\n\nVisual Description: The figure illustrates a transition from a winter landscape on the left to a fall landscape on the right, showcasing the gradual change in foliage and color. The x-axis represents time or seasonal progression, while the y-axis could represent visual intensity or color saturation. Key findings indicate that as the seasons progress from winter to fall, there is a notable increase in color variety and vibrancy, highlighting the dynamic nature of the landscape throughout different times of the year.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_13",
        "caption": "a photo of a landscape in winter  $\\rightarrow$  a photo of a landscape in fall",
        "image_key": "_page_6_Picture_6.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_14",
      "content": "<span id=\"page-7-0\"></span>\n\nFigure: Variations of images featuring typographic attacks [\\[20\\]](#page-19-5) paired with the CLIP model's predicted probabilities across three labels. Surprisingly, the decoder still recovers Granny Smith apples even when the predicted probability for this label is near 0%. We also find that our CLIP model is slightly less susceptible to the \"pizza\" attack than the models investigated in [\\[20\\]](#page-19-5).\n\nVisual Description: The figure displays variations of images depicting Granny Smith apples labeled with typographic attacks, with predicted probabilities for three categories: \"Granny Smith,\" \"iPod,\" and \"Pizza.\" The x-axis represents the different images, while the y-axis indicates the model's predicted probabilities for each label. Key findings highlight that the model maintains a prediction of Granny Smith apples even at a low probability of 0.02%, demonstrating resilience to the typographic attacks, particularly noting that it is less susceptible to the \"pizza\" attack compared to previous models.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_14",
        "caption": "Variations of images featuring typographic attacks [\\[20\\]](#page-19-5) paired with the CLIP model's predicted probabilities across three labels. Surprisingly, the decoder still recovers Granny Smith apples even when the predicted probability for this label is near 0%. We also find that our CLIP model is slightly less susceptible to the \"pizza\" attack than the models investigated in [\\[20\\]](#page-19-5).",
        "image_key": "_page_7_Picture_0.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_15",
      "content": "<span id=\"page-8-0\"></span>\n\nFigure: Visualization of reconstructions of CLIP latents from progressively more PCA dimensions (20, 30, 40, 80, 120, 160, 200, 320 dimensions), with the original source image on the far right. The lower dimensions preserve coarse-grained semantic information, whereas the higher dimensions encode finer-grained details about the exact form of the objects in the scene.\n\nVisual Description: The figure presents a visualization of CLIP latents reconstructed from various PCA dimensions, ranging from 20 to 320, with each column representing a different dimensional reconstruction aligned to the original image on the far right. The axes likely denote the PCA dimensions on the horizontal axis and the reconstructed clarity or detail on the vertical axis. The key trend observed suggests that lower PCA dimensions capture broader semantic content, while higher dimensions increasingly depict finer details of the objects and scenes, indicating a trade-off between abstraction and detail in image representation.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_15",
        "caption": "Visualization of reconstructions of CLIP latents from progressively more PCA dimensions (20, 30, 40, 80, 120, 160, 200, 320 dimensions), with the original source image on the far right. The lower dimensions preserve coarse-grained semantic information, whereas the higher dimensions encode finer-grained details about the exact form of the objects in the scene.",
        "image_key": "_page_8_Picture_0.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_16",
      "content": "<span id=\"page-9-1\"></span>\n\nFigure: Samples using different conditioning signals for the *same* decoder. In the first row, we pass the text caption to the decoder, and pass a zero vector for the CLIP embedding. In the second row, we pass both the text caption and the CLIP text embedding of the caption. In the third row, we pass the text and a CLIP image embedding generated by an autoregressive prior for the given caption. Note that this decoder is only trained to do the text-to-image generation task (without the CLIP image representation) 5% of the time.\n\nVisual Description: The academic figure presents three rows comparing the outcomes of varying conditioning signals fed into a text-to-image decoder. The first row uses only text captions, while the second introduces a CLIP text embedding, and the third incorporates a CLIP image embedding generated through an autoregressive prior. The trends indicate that incorporating CLIP embeddings enhances the decoder's performance in accurately generating images that align with the textual descriptions, showcasing the importance of multimodal inputs in text-to-image generation tasks.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_16",
        "caption": "Samples using different conditioning signals for the *same* decoder. In the first row, we pass the text caption to the decoder, and pass a zero vector for the CLIP embedding. In the second row, we pass both the text caption and the CLIP text embedding of the caption. In the third row, we pass the text and a CLIP image embedding generated by an autoregressive prior for the given caption. Note that this decoder is only trained to do the text-to-image generation task (without the CLIP image representation) 5% of the time.",
        "image_key": "_page_9_Figure_0.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_17",
      "content": "<span id=\"page-10-1\"></span>\n\nFigure: Samples when increasing guidance scale for both unCLIP and GLIDE, using the prompt, \"A green vase filled with red roses sitting on top of table.\" For unCLIP, we fix the latent vectors sampled from the prior, and only vary the guidance scale of the decoder. For both models, we fix the diffusion noise seed for each column. Samples from unCLIP improve in quality (more realistic lighting and shadows) but do not change in content as we increase guidance scale, preserving semantic diversity even at high decoder guidance scales.\n\nVisual Description: The figure presents samples generated by unCLIP and GLIDE models, evaluated at increasing guidance scales from 1.0 to 4.0 for the prompt \"A green vase filled with red roses sitting on top of a table.\" The vertical axis indicates the guidance scale, while the horizontal axis compares outputs from unCLIP (left) and GLIDE (right) for each scale. As the guidance scale increases, unCLIP samples show improved quality in terms of lighting and shadows without altering the content, indicating a preservation of semantic diversity, whereas GLIDE samples exhibit more variation in their visual representations across scales.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_17",
        "caption": "Samples when increasing guidance scale for both unCLIP and GLIDE, using the prompt, \"A green vase filled with red roses sitting on top of table.\" For unCLIP, we fix the latent vectors sampled from the prior, and only vary the guidance scale of the decoder. For both models, we fix the diffusion noise seed for each column. Samples from unCLIP improve in quality (more realistic lighting and shadows) but do not change in content as we increase guidance scale, preserving semantic diversity even at high decoder guidance scales.",
        "image_key": "_page_10_Figure_0.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_18",
      "content": "<span id=\"page-11-1\"></span>\n\nFigure: <span id=\"page-11-2\"></span>Figure 10: When comparing unCLIP (with our best sampling settings) to various settings of guidance scale for GLIDE, unCLIP was preferred by human evaluators on at least one axis among photorealism, caption similarity, and diversity for each comparison. At the higher guidance scales used to generate photorealistic images, unCLIP yields greater diversity for comparable photorealism and caption similarity.\n\nVisual Description: The figure compares the performance of unCLIP and GLIDE across different guidance scales regarding photorealism, caption similarity, and diversity. The x-axis represents the GLIDE guidance scale ranging from 1.0 to 3.0, while the y-axis indicates the frequency with which unCLIP was preferred over GLIDE, expressed as a percentage. Key findings show that while unCLIP generally outperformed GLIDE, particularly in terms of diversity, preferences varied with guidance scale, indicating that higher guidance scales yielded better diversity while maintaining comparable photorealism and caption similarity.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_18",
        "caption": "<span id=\"page-11-2\"></span>Figure 10: When comparing unCLIP (with our best sampling settings) to various settings of guidance scale for GLIDE, unCLIP was preferred by human evaluators on at least one axis among photorealism, caption similarity, and diversity for each comparison. At the higher guidance scales used to generate photorealistic images, unCLIP yields greater diversity for comparable photorealism and caption similarity.",
        "image_key": "_page_11_Figure_0.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_19",
      "content": "Figure: FID versus guidance scale for unCLIP and GLIDE. For the unCLIP priors, we swept over sampling hyperparameters and fixed to the settings with the best minimum FID.\n\nVisual Description: The figure displays the relationship between the guidance scale and the FID (Fréchet Inception Distance) scores for different models: GLIDE, unCLIP (AR), and unCLIP (Diffusion). The x-axis represents the guidance scale, ranging from 1 to 4, while the y-axis shows the MS-COCO FID, indicating quality of generated images. GLIDE shows a significant increase in FID scores as the guidance scale increases, peaking at around 19, whereas both unCLIP (AR) and unCLIP (Diffusion) maintain relatively stable FID scores near 12, suggesting different performance characteristics between the models as the guidance scale varies.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_19",
        "caption": "FID versus guidance scale for unCLIP and GLIDE. For the unCLIP priors, we swept over sampling hyperparameters and fixed to the settings with the best minimum FID.",
        "image_key": "_page_11_Figure_2.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_20",
      "content": "<span id=\"page-13-0\"></span>\n\nFigure: Random image samples on MS-COCO prompts.\n\nVisual Description: The figure showcases various generated images corresponding to prompts from the MS-COCO dataset, arranged in a grid format. The axes are implicitly defined by the two categories: \"Real image\" on the left and five different models (DALL-E, GLIDE, Make-A-Scene, unCLIP, unCLIP (post.)) on the top. Key findings indicate that while all models generate images that visually match the prompts, there is variation in the level of detail and realism, particularly in complex scenes such as those featuring animals or interior spaces. The consistency of generated images across models suggests a shared understanding of common visual elements, though nuanced differences highlight areas for improvement in AI-generated imagery.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_20",
        "caption": "Random image samples on MS-COCO prompts.",
        "image_key": "_page_13_Figure_0.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_21",
      "content": "<span id=\"page-14-0\"></span>\n\nFigure: Aesthetic quality evaluations comparing GLIDE and unCLIP using 512 auto-generated artistic prompts. We find that both models benefit from guidance, but unCLIP does not sacrifice recall for aesthetic quality.\n\nVisual Description: The figure presents two plots comparing aesthetic quality evaluations of the GLIDE and unCLIP models based on various guidance scales. The left plot displays mean AVA predictions against the guidance scale, with GLIDE achieving consistently higher scores, especially at higher guidance values, while both unCLIP variants also improve with guidance but at a slower rate. The right plot illustrates the relationship between mean AVA predictions and recall, showing that even as aesthetic quality increases for GLIDE, its recall decreases slightly, while unCLIP maintains recall levels without sacrificing aesthetic performance.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_21",
        "caption": "Aesthetic quality evaluations comparing GLIDE and unCLIP using 512 auto-generated artistic prompts. We find that both models benefit from guidance, but unCLIP does not sacrifice recall for aesthetic quality.",
        "image_key": "_page_14_Figure_0.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_22",
      "content": "<span id=\"page-15-1\"></span>\n\nFigure: Samples from unCLIP and GLIDE for the prompt \"a red cube on top of a blue cube\".\n\nVisual Description: The figure presents a comparative analysis of samples generated by unCLIP (left) and GLIDE (right) based on the prompt \"a red cube on top of a blue cube.\" Each grid features various configurations of red and blue cube shapes, demonstrating the diverse interpretations of the prompt by both models. Key findings include differences in detail and complexity, with unCLIP showcasing a wider variety of cubic formations and GLIDE emphasizing smoother designs with distinct features like polka dots.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_22",
        "caption": "Samples from unCLIP and GLIDE for the prompt \"a red cube on top of a blue cube\".",
        "image_key": "_page_15_Figure_0.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_23",
      "content": "<span id=\"page-16-0\"></span>\n\nFigure: Reconstructions from the decoder for difficult binding problems. We find that the reconstructions mix up objects and attributes. In the first two examples, the model mixes up the color of two objects. In the rightmost example, the model does not reliably reconstruct the relative size of two objects.\n\nVisual Description: The figure illustrates the performance of a decoder in reconstructing objects and their attributes under challenging conditions. The horizontal axis likely represents different object attributes (such as color and size), while the vertical axis could depict the accuracy of reconstructions or instances of mix-up. Key findings include the model's tendency to confuse the colors of two objects in the first examples and its inability to accurately represent the relative sizes in the rightmost example, highlighting limitations in decoding complex visual information.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_23",
        "caption": "Reconstructions from the decoder for difficult binding problems. We find that the reconstructions mix up objects and attributes. In the first two examples, the model mixes up the color of two objects. In the rightmost example, the model does not reliably reconstruct the relative size of two objects.",
        "image_key": "_page_16_Picture_0.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_24",
      "content": "<span id=\"page-16-1\"></span>\n\nFigure: Samples from unCLIP for the prompt, \"A sign that says deep learning.\"\n\nVisual Description: The figure displays four images of signs featuring variations on the phrase \"deep.\" The x-axis could represent different interpretations or presentations of the word \"deep,\" while the y-axis might indicate clarity or visibility of the text. The data trends show a playful engagement with the term, highlighting how it can be misinterpreted or creatively represented, ultimately reinforcing the concept of \"deep learning\" through visual wordplay. Key findings reveal a thematic exploration of language and perception, emphasizing the multifaceted nature of communication in the context of deep learning.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_24",
        "caption": "Samples from unCLIP for the prompt, \"A sign that says deep learning.\"",
        "image_key": "_page_16_Picture_2.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_25",
      "content": "<span id=\"page-17-0\"></span>\n\nFigure: (a) A high quality photo of a dog playing in a green field next to a lake.\n\nVisual Description: The image you provided does not contain academic data or figures that can be described in terms of axes, data trends, or key findings. Instead, it is a high-quality photo of two dogs in outdoor settings, one playing by a lake and the other in a green field. For a proper analysis or description related to academic data, a different type of visual representation would be required.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_25",
        "caption": "(a) A high quality photo of a dog playing in a green field next to a lake.",
        "image_key": "_page_17_Picture_0.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_26",
      "content": "Figure: (b) A high quality photo of Times Square.\n\nVisual Description: The figure features two contrasting images of Times Square, one taken during the day and another at night. While the daytime image showcases bright blue skies and a bustling atmosphere, the nighttime scene is dominated by vibrant neon lights and illuminated billboards, highlighting the area's dynamic character. Key findings indicate that the area transforms dramatically from day to night, with increased visual complexity and energy in the evening, reflecting the cultural and commercial vibrancy of Times Square.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_26",
        "caption": "(b) A high quality photo of Times Square.",
        "image_key": "_page_17_Picture_2.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_27",
      "content": "<span id=\"page-24-0\"></span>\n\nFigure: Random samples from unCLIP for prompt \"Vibrant portrait painting of Salvador Dali with a robotic half face\"\n\nVisual Description: The academic figure presents a 4x4 grid of vibrant, artistic interpretations based on a prompt featuring Salvador Dali with a robotic half-face. Each portrait displays a blend of surrealism and technology, showcasing variations in color intensity and style across the images. Key findings suggest a diverse range of artistic expressions derived from a single prompt, reflecting the algorithm's ability to merge classic surrealist elements with modern themes.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_27",
        "caption": "Random samples from unCLIP for prompt \"Vibrant portrait painting of Salvador Dali with a robotic half face\"",
        "image_key": "_page_24_Picture_2.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_28",
      "content": "<span id=\"page-25-0\"></span>\n\nFigure: Random samples from unCLIP for prompt \"A close up of a handpalm with leaves growing from it.\"\n\nVisual Description: The academic figure presents a grid of images generated in response to the prompt \"A close up of a handpalm with leaves growing from it,\" showcasing diverse representations of hands intertwined with various types of leaves. Each image highlights unique hand positions and leaf varieties, indicating a rich interplay between human and natural elements. A key finding from this visual exploration is the variety in hand sizes, textures, and types of foliage, suggesting a broad interpretation of the prompt and the creative potential of unCLIP's generative capabilities.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_28",
        "caption": "Random samples from unCLIP for prompt \"A close up of a handpalm with leaves growing from it.\"",
        "image_key": "_page_25_Picture_0.jpeg",
        "has_vision_description": true
      }
    },
    {
      "id": "Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf_figure_29",
      "content": "<span id=\"page-26-0\"></span>\n\nFigure: Random samples from unCLIP for prompt \"A teddybear on a skateboard in Times Square.\"\n\nVisual Description: The figure presents a grid of images depicting various teddy bears on skateboards in Times Square, showcasing the creativity of the unCLIP model in generating playful scenarios. The x-axis represents the different poses and actions of the teddy bears—ranging from stationary to dynamic skating poses—while the y-axis captures the diversity of bear designs and outfits. Key findings suggest that the unCLIP model effectively generates visually appealing and contextually relevant images that blend whimsical elements with a recognizable urban backdrop, as evidenced by the varied expressions and activities of the teddy bears across the sample.",
      "metadata": {
        "content_type": "figure",
        "figure_id": "figure_29",
        "caption": "Random samples from unCLIP for prompt \"A teddybear on a skateboard in Times Square.\"",
        "image_key": "_page_26_Picture_0.jpeg",
        "has_vision_description": true
      }
    }
  ],
  "tables": [
    {
      "table_id": "table_1",
      "headers": [
        "unCLIP Prior",
        "Photorealism",
        "Caption Similarity",
        "Diversity"
      ],
      "rows": [
        [
          "AR",
          "47.1% ± 3.1%",
          "41.1% ± 3.0%",
          "62.6% ± 3.0%"
        ],
        [
          "Diffusion",
          "48.9% ± 3.1%",
          "45.3% ± 3.0%",
          "70.5% ± 2.8%"
        ]
      ],
      "markdown": "| unCLIP Prior | Photorealism | Caption Similarity | Diversity    |\n|--------------|--------------|--------------------|--------------|\n| AR           | 47.1% ± 3.1% | 41.1% ± 3.0%       | 62.6% ± 3.0% |\n| Diffusion    | 48.9% ± 3.1% | 45.3% ± 3.0%       | 70.5% ± 2.8% |\n"
    },
    {
      "table_id": "table_2",
      "headers": [
        "Model",
        "FID",
        "Zero-shot FID",
        "Zero-shot FID (filt)",
        ""
      ],
      "rows": [
        [
          "AttnGAN (Xu et al., 2017)",
          "35.49",
          "",
          "",
          ""
        ],
        [
          "DM-GAN (Zhu et al., 2019)",
          "32.64",
          "",
          "",
          ""
        ],
        [
          "DF-GAN (Tao et al., 2020)",
          "21.42",
          "",
          "",
          ""
        ],
        [
          "DM-GAN + CL (Ye et al., 2021)",
          "20.79",
          "",
          "",
          ""
        ],
        [
          "XMC-GAN (Zhang et al., 2021)",
          "9.33",
          "",
          "",
          ""
        ],
        [
          "LAFITE (Zhou et al., 2021)",
          "8.12",
          "",
          "",
          ""
        ],
        [
          "Make-A-Scene (Gafni et al., 2022)",
          "7.55",
          "",
          "",
          ""
        ],
        [
          "DALL-E (Ramesh et al., 2021)",
          "",
          "∼ 28",
          "",
          ""
        ],
        [
          "LAFITE (Zhou et al., 2021)",
          "",
          "26.94",
          "",
          ""
        ],
        [
          "GLIDE (Nichol et al., 2021)",
          "",
          "12.24",
          "12.89",
          ""
        ],
        [
          "Make-A-Scene (Gafni et al., 2022)",
          "",
          "",
          "11.84",
          ""
        ],
        [
          "unCLIP (AR prior)",
          "",
          "10.63",
          "11.08",
          ""
        ],
        [
          "unCLIP (Diffusion prior)",
          "",
          "10.39",
          "10.87",
          ""
        ]
      ],
      "markdown": "| Model                             | FID   | Zero-shot FID | Zero-shot FID (filt) |  |\n|-----------------------------------|-------|---------------|----------------------|--|\n| AttnGAN (Xu et al., 2017)         | 35.49 |               |                      |  |\n| DM-GAN (Zhu et al., 2019)         | 32.64 |               |                      |  |\n| DF-GAN (Tao et al., 2020)         | 21.42 |               |                      |  |\n| DM-GAN + CL (Ye et al., 2021)     | 20.79 |               |                      |  |\n| XMC-GAN (Zhang et al., 2021)      | 9.33  |               |                      |  |\n| LAFITE (Zhou et al., 2021)        | 8.12  |               |                      |  |\n| Make-A-Scene (Gafni et al., 2022) | 7.55  |               |                      |  |\n| DALL-E (Ramesh et al., 2021)      |       | ∼ 28          |                      |  |\n| LAFITE (Zhou et al., 2021)        |       | 26.94         |                      |  |\n| GLIDE (Nichol et al., 2021)       |       | 12.24         | 12.89                |  |\n| Make-A-Scene (Gafni et al., 2022) |       |               | 11.84                |  |\n| unCLIP (AR prior)                 |       | 10.63         | 11.08                |  |\n| unCLIP (Diffusion prior)          |       | 10.39         | 10.87                |  |\n"
    },
    {
      "table_id": "table_3",
      "headers": [
        "",
        "AR prior",
        "Diffusion prior",
        "64",
        "64 → 256",
        "256 → 1024"
      ],
      "rows": [
        [
          "Diffusion steps",
          "-",
          "1000",
          "1000",
          "1000",
          "1000"
        ],
        [
          "Noise schedule",
          "-",
          "cosine",
          "cosine",
          "cosine",
          "linear"
        ],
        [
          "Sampling steps",
          "-",
          "64",
          "250",
          "27",
          "15"
        ],
        [
          "Sampling variance method",
          "-",
          "analytic [2]",
          "learned [34]",
          "DDIM [47]",
          "DDIM [47]"
        ],
        [
          "Crop fraction",
          "-",
          "-",
          "-",
          "0.25",
          "0.25"
        ],
        [
          "Model size",
          "1B",
          "1B",
          "3.5B",
          "700M",
          "300M"
        ],
        [
          "Channels",
          "-",
          "-",
          "512",
          "320",
          "192"
        ],
        [
          "Depth",
          "-",
          "-",
          "3",
          "3",
          "2"
        ],
        [
          "Channels multiple",
          "-",
          "-",
          "1,2,3,4",
          "1,2,3,4",
          "1,1,2,2,4,4"
        ],
        [
          "Heads channels",
          "-",
          "-",
          "64",
          "-",
          "-"
        ],
        [
          "Attention resolution",
          "-",
          "-",
          "32,16,8",
          "-",
          "-"
        ],
        [
          "Text encoder context",
          "256",
          "256",
          "256",
          "-",
          "-"
        ],
        [
          "Text encoder width",
          "2048",
          "2048",
          "2048",
          "-",
          "-"
        ],
        [
          "Text encoder depth",
          "24",
          "24",
          "24",
          "-",
          "-"
        ],
        [
          "Text encoder heads",
          "32",
          "32",
          "32",
          "-",
          "-"
        ],
        [
          "Latent decoder context",
          "384",
          "-",
          "-",
          "-",
          "-"
        ],
        [
          "Latent decoder width",
          "1664",
          "-",
          "-",
          "-",
          "-"
        ],
        [
          "Latent decoder depth",
          "24",
          "-",
          "-",
          "-",
          "-"
        ],
        [
          "Latent decoder heads",
          "26",
          "-",
          "-",
          "-",
          "-"
        ],
        [
          "Dropout",
          "-",
          "-",
          "0.1",
          "0.1",
          "-"
        ],
        [
          "Weight decay",
          "4.0e-2",
          "6.0e-2",
          "-",
          "-",
          "-"
        ],
        [
          "Batch size",
          "4096",
          "4096",
          "2048",
          "1024",
          "512"
        ],
        [
          "Iterations",
          "1M",
          "600K",
          "800K",
          "1M",
          "1M"
        ],
        [
          "Learning rate",
          "1.6e-4",
          "1.1e-4",
          "1.2e-4",
          "1.2e-4",
          "1.0e-4"
        ],
        [
          "Adam β2",
          "0.91",
          "0.96",
          "0.999",
          "0.999",
          "0.999"
        ],
        [
          "Adam",
          "1.0e-10",
          "1.0e-6",
          "1.0e-8",
          "1.0e-8",
          "1.0e-8"
        ],
        [
          "EMA decay",
          "0.999",
          "0.9999",
          "0.9999",
          "0.9999",
          "0.9999"
        ]
      ],
      "markdown": "|                          | AR prior | Diffusion prior | 64           | 64 → 256  | 256 → 1024  |\n|--------------------------|----------|-----------------|--------------|-----------|-------------|\n| Diffusion steps          | -        | 1000            | 1000         | 1000      | 1000        |\n| Noise schedule           | -        | cosine          | cosine       | cosine    | linear      |\n| Sampling steps           | -        | 64              | 250          | 27        | 15          |\n| Sampling variance method | -        | analytic [2]    | learned [34] | DDIM [47] | DDIM [47]   |\n| Crop fraction            | -        | -               | -            | 0.25      | 0.25        |\n| Model size               | 1B       | 1B              | 3.5B         | 700M      | 300M        |\n| Channels                 | -        | -               | 512          | 320       | 192         |\n| Depth                    | -        | -               | 3            | 3         | 2           |\n| Channels multiple        | -        | -               | 1,2,3,4      | 1,2,3,4   | 1,1,2,2,4,4 |\n| Heads channels           | -        | -               | 64           | -         | -           |\n| Attention resolution     | -        | -               | 32,16,8      | -         | -           |\n| Text encoder context     | 256      | 256             | 256          | -         | -           |\n| Text encoder width       | 2048     | 2048            | 2048         | -         | -           |\n| Text encoder depth       | 24       | 24              | 24           | -         | -           |\n| Text encoder heads       | 32       | 32              | 32           | -         | -           |\n| Latent decoder context   | 384      | -               | -            | -         | -           |\n| Latent decoder width     | 1664     | -               | -            | -         | -           |\n| Latent decoder depth     | 24       | -               | -            | -         | -           |\n| Latent decoder heads     | 26       | -               | -            | -         | -           |\n| Dropout                  | -        | -               | 0.1          | 0.1       | -           |\n| Weight decay             | 4.0e-2   | 6.0e-2          | -            | -         | -           |\n| Batch size               | 4096     | 4096            | 2048         | 1024      | 512         |\n| Iterations               | 1M       | 600K            | 800K         | 1M        | 1M          |\n| Learning rate            | 1.6e-4   | 1.1e-4          | 1.2e-4       | 1.2e-4    | 1.0e-4      |\n| Adam β2                  | 0.91     | 0.96            | 0.999        | 0.999     | 0.999       |\n| Adam                     | 1.0e-10  | 1.0e-6          | 1.0e-8       | 1.0e-8    | 1.0e-8      |\n| EMA decay                | 0.999    | 0.9999          | 0.9999       | 0.9999    | 0.9999      |\n"
    }
  ],
  "figures": [
    {
      "figure_id": "figure_1",
      "image_key": "_page_1_Picture_0.jpeg",
      "caption": "vibrant portrait painting of Salvador Dalí with a robotic half face a shiba inu wearing a beret and black turtleneck a close up of a handpalm with leaves growing from it",
      "alt_text": "",
      "context_before": "<span id=\"page-1-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_2",
      "image_key": "_page_1_Picture_2.jpeg",
      "caption": "![](_page_1_Picture_4.jpeg)",
      "alt_text": "",
      "context_before": "",
      "context_after": ""
    },
    {
      "figure_id": "figure_3",
      "image_key": "_page_1_Picture_6.jpeg",
      "caption": "an espresso machine that makes coffee from human souls, artstation panda mad scientist mixing sparkling chemicals, artstation a corgi's head depicted as an explosion of a nebula",
      "alt_text": "",
      "context_before": "",
      "context_after": ""
    },
    {
      "figure_id": "figure_4",
      "image_key": "_page_1_Picture_8.jpeg",
      "caption": "![](_page_1_Picture_10.jpeg)",
      "alt_text": "",
      "context_before": "",
      "context_after": ""
    },
    {
      "figure_id": "figure_5",
      "image_key": "_page_1_Picture_12.jpeg",
      "caption": "![](_page_1_Picture_14.jpeg)",
      "alt_text": "",
      "context_before": "",
      "context_after": ""
    },
    {
      "figure_id": "figure_6",
      "image_key": "_page_1_Picture_16.jpeg",
      "caption": "Selected 1024 × 1024 samples from a production version of our model.",
      "alt_text": "",
      "context_before": "",
      "context_after": ""
    },
    {
      "figure_id": "figure_7",
      "image_key": "_page_2_Picture_0.jpeg",
      "caption": "A high-level overview of unCLIP. Above the dotted line, we depict the CLIP training process, through which we learn a joint representation space for text and images. Below the dotted line, we depict our text-to-image generation process: a CLIP text embedding is first fed to an autoregressive or diffusion prior to produce an image embedding, and then this embedding is used to condition a diffusion decoder which produces a final image. Note that the CLIP model is frozen during training of the prior and decoder.",
      "alt_text": "",
      "context_before": "",
      "context_after": ""
    },
    {
      "figure_id": "figure_8",
      "image_key": "_page_4_Picture_0.jpeg",
      "caption": "Variations of an input image by encoding with CLIP and then decoding with a diffusion model. The variations preserve both semantic information like presence of a clock in the painting and the overlapping strokes in the logo, as well as stylistic elements like the surrealism in the painting and the color gradients in the logo, while varying the non-essential details.",
      "alt_text": "",
      "context_before": "<span id=\"page-4-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_9",
      "image_key": "_page_5_Picture_0.jpeg",
      "caption": "Variations between two images by interpolating their CLIP image embedding and then decoding with a diffusion model. We fix the decoder seed across each row. The intermediate variations naturally blend the content and style from both input images.",
      "alt_text": "",
      "context_before": "<span id=\"page-5-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_10",
      "image_key": "_page_6_Picture_0.jpeg",
      "caption": "a photo of a cat  $\\rightarrow$  an anime drawing of a super saiyan cat, artstation",
      "alt_text": "",
      "context_before": "<span id=\"page-6-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_11",
      "image_key": "_page_6_Picture_2.jpeg",
      "caption": "a photo of a victorian house  $\\rightarrow$  a photo of a modern house",
      "alt_text": "",
      "context_before": "",
      "context_after": ""
    },
    {
      "figure_id": "figure_12",
      "image_key": "_page_6_Picture_4.jpeg",
      "caption": "a photo of an adult lion  $\\rightarrow$  a photo of lion cub",
      "alt_text": "",
      "context_before": "",
      "context_after": ""
    },
    {
      "figure_id": "figure_13",
      "image_key": "_page_6_Picture_6.jpeg",
      "caption": "a photo of a landscape in winter  $\\rightarrow$  a photo of a landscape in fall",
      "alt_text": "",
      "context_before": "",
      "context_after": ""
    },
    {
      "figure_id": "figure_14",
      "image_key": "_page_7_Picture_0.jpeg",
      "caption": "Variations of images featuring typographic attacks [\\[20\\]](#page-19-5) paired with the CLIP model's predicted probabilities across three labels. Surprisingly, the decoder still recovers Granny Smith apples even when the predicted probability for this label is near 0%. We also find that our CLIP model is slightly less susceptible to the \"pizza\" attack than the models investigated in [\\[20\\]](#page-19-5).",
      "alt_text": "",
      "context_before": "<span id=\"page-7-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_15",
      "image_key": "_page_8_Picture_0.jpeg",
      "caption": "Visualization of reconstructions of CLIP latents from progressively more PCA dimensions (20, 30, 40, 80, 120, 160, 200, 320 dimensions), with the original source image on the far right. The lower dimensions preserve coarse-grained semantic information, whereas the higher dimensions encode finer-grained details about the exact form of the objects in the scene.",
      "alt_text": "",
      "context_before": "<span id=\"page-8-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_16",
      "image_key": "_page_9_Figure_0.jpeg",
      "caption": "Samples using different conditioning signals for the *same* decoder. In the first row, we pass the text caption to the decoder, and pass a zero vector for the CLIP embedding. In the second row, we pass both the text caption and the CLIP text embedding of the caption. In the third row, we pass the text and a CLIP image embedding generated by an autoregressive prior for the given caption. Note that this decoder is only trained to do the text-to-image generation task (without the CLIP image representation) 5% of the time.",
      "alt_text": "",
      "context_before": "<span id=\"page-9-1\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_17",
      "image_key": "_page_10_Figure_0.jpeg",
      "caption": "Samples when increasing guidance scale for both unCLIP and GLIDE, using the prompt, \"A green vase filled with red roses sitting on top of table.\" For unCLIP, we fix the latent vectors sampled from the prior, and only vary the guidance scale of the decoder. For both models, we fix the diffusion noise seed for each column. Samples from unCLIP improve in quality (more realistic lighting and shadows) but do not change in content as we increase guidance scale, preserving semantic diversity even at high decoder guidance scales.",
      "alt_text": "",
      "context_before": "<span id=\"page-10-1\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_18",
      "image_key": "_page_11_Figure_0.jpeg",
      "caption": "<span id=\"page-11-2\"></span>Figure 10: When comparing unCLIP (with our best sampling settings) to various settings of guidance scale for GLIDE, unCLIP was preferred by human evaluators on at least one axis among photorealism, caption similarity, and diversity for each comparison. At the higher guidance scales used to generate photorealistic images, unCLIP yields greater diversity for comparable photorealism and caption similarity.",
      "alt_text": "",
      "context_before": "<span id=\"page-11-1\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_19",
      "image_key": "_page_11_Figure_2.jpeg",
      "caption": "FID versus guidance scale for unCLIP and GLIDE. For the unCLIP priors, we swept over sampling hyperparameters and fixed to the settings with the best minimum FID.",
      "alt_text": "",
      "context_before": "",
      "context_after": ""
    },
    {
      "figure_id": "figure_20",
      "image_key": "_page_13_Figure_0.jpeg",
      "caption": "Random image samples on MS-COCO prompts.",
      "alt_text": "",
      "context_before": "<span id=\"page-13-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_21",
      "image_key": "_page_14_Figure_0.jpeg",
      "caption": "Aesthetic quality evaluations comparing GLIDE and unCLIP using 512 auto-generated artistic prompts. We find that both models benefit from guidance, but unCLIP does not sacrifice recall for aesthetic quality.",
      "alt_text": "",
      "context_before": "<span id=\"page-14-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_22",
      "image_key": "_page_15_Figure_0.jpeg",
      "caption": "Samples from unCLIP and GLIDE for the prompt \"a red cube on top of a blue cube\".",
      "alt_text": "",
      "context_before": "<span id=\"page-15-1\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_23",
      "image_key": "_page_16_Picture_0.jpeg",
      "caption": "Reconstructions from the decoder for difficult binding problems. We find that the reconstructions mix up objects and attributes. In the first two examples, the model mixes up the color of two objects. In the rightmost example, the model does not reliably reconstruct the relative size of two objects.",
      "alt_text": "",
      "context_before": "<span id=\"page-16-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_24",
      "image_key": "_page_16_Picture_2.jpeg",
      "caption": "Samples from unCLIP for the prompt, \"A sign that says deep learning.\"",
      "alt_text": "",
      "context_before": "<span id=\"page-16-1\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_25",
      "image_key": "_page_17_Picture_0.jpeg",
      "caption": "(a) A high quality photo of a dog playing in a green field next to a lake.",
      "alt_text": "",
      "context_before": "<span id=\"page-17-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_26",
      "image_key": "_page_17_Picture_2.jpeg",
      "caption": "(b) A high quality photo of Times Square.",
      "alt_text": "",
      "context_before": "",
      "context_after": ""
    },
    {
      "figure_id": "figure_27",
      "image_key": "_page_24_Picture_2.jpeg",
      "caption": "Random samples from unCLIP for prompt \"Vibrant portrait painting of Salvador Dali with a robotic half face\"",
      "alt_text": "",
      "context_before": "<span id=\"page-24-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_28",
      "image_key": "_page_25_Picture_0.jpeg",
      "caption": "Random samples from unCLIP for prompt \"A close up of a handpalm with leaves growing from it.\"",
      "alt_text": "",
      "context_before": "<span id=\"page-25-0\"></span>",
      "context_after": ""
    },
    {
      "figure_id": "figure_29",
      "image_key": "_page_26_Picture_0.jpeg",
      "caption": "Random samples from unCLIP for prompt \"A teddybear on a skateboard in Times Square.\"",
      "alt_text": "",
      "context_before": "<span id=\"page-26-0\"></span>",
      "context_after": ""
    }
  ],
  "stats": {
    "total_chunks": 136,
    "text_chunks": 104,
    "tables_extracted": 3,
    "images_extracted": 32,
    "figure_chunks_created": 29,
    "figures_with_vision": 29,
    "markdown_chars": 67717,
    "processing_time_seconds": 271.9
  }
}