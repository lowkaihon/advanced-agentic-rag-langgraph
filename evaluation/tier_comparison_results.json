{
  "timestamp": "2025-11-18T22:07:55.519612",
  "test_type": "tier_comparison",
  "tiers_evaluated": [
    "budget",
    "balanced",
    "premium"
  ],
  "results": {
    "budget": {
      "retrieval_metrics": {
        "recall_at_k": 0.35,
        "precision_at_k": 0.2,
        "f1_at_k": 0.25158730158730164,
        "hit_rate": 0.8,
        "mrr": 0.7,
        "ndcg_at_5": 0.38384377887622695
      },
      "generation_metrics": {
        "avg_groundedness": 0.8,
        "avg_confidence": 0.76,
        "hallucination_rate": 0.2,
        "avg_semantic_similarity": 0.6799999999999999,
        "avg_factual_accuracy": 0.76,
        "avg_completeness": 0.54,
        "avg_answer_relevance": 0.7069314678115126
      },
      "per_difficulty_breakdown": {
        "easy": {
          "recall_at_k": 0.25,
          "precision_at_k": 0.1,
          "f1_at_k": 0.14285714285714288,
          "hit_rate": 0.5,
          "mrr": 0.5,
          "ndcg_at_5": 0.23463936301137822
        },
        "medium": {
          "recall_at_k": 0.5,
          "precision_at_k": 0.30000000000000004,
          "f1_at_k": 0.375,
          "hit_rate": 1.0,
          "mrr": 1.0,
          "ndcg_at_5": 0.5848494622628649
        },
        "hard": {
          "recall_at_k": 0.25,
          "precision_at_k": 0.2,
          "f1_at_k": 0.22222222222222224,
          "hit_rate": 1.0,
          "mrr": 0.5,
          "ndcg_at_5": 0.2802412438326486
        }
      },
      "per_query_type_breakdown": {
        "factual": {
          "recall_at_k": 0.25,
          "precision_at_k": 0.1,
          "f1_at_k": 0.14285714285714288,
          "hit_rate": 0.5,
          "mrr": 0.5,
          "ndcg_at_5": 0.23463936301137822
        },
        "conceptual": {
          "recall_at_k": 0.5,
          "precision_at_k": 0.30000000000000004,
          "f1_at_k": 0.375,
          "hit_rate": 1.0,
          "mrr": 1.0,
          "ndcg_at_5": 0.5848494622628649
        },
        "procedural": {
          "recall_at_k": 0.25,
          "precision_at_k": 0.2,
          "f1_at_k": 0.22222222222222224,
          "hit_rate": 1.0,
          "mrr": 0.5,
          "ndcg_at_5": 0.2802412438326486
        }
      },
      "per_example_results": [
        {
          "example_id": "attention_001",
          "question": "How many attention heads are used in the base Transformer model?",
          "difficulty": "easy",
          "query_type": "factual",
          "retrieval_metrics": {
            "recall_at_k": 0.5,
            "precision_at_k": 0.2,
            "f1_at_k": 0.28571428571428575,
            "hit_rate": 1.0,
            "mrr": 1.0,
            "ndcg_at_5": 0.46927872602275644
          },
          "groundedness_score": 1.0,
          "confidence_score": 0.9,
          "has_hallucination": false,
          "final_answer": "The base Transformer model uses 8 attention heads. This is stated in the retrieved context, which mentions, \"we employ h = 8 parallel attention layers, or heads\" (from the document titled \"Attention Is All You Need.pdf\").",
          "ground_truth_answer": "The base Transformer model uses 8 attention heads (h=8). This is specified in the model configuration where the base model has dmodel=512 and uses 8 parallel attention heads, with each head operating on dk=dv=64 dimensions.",
          "semantic_similarity": 0.8,
          "factual_accuracy": 0.8,
          "completeness": 0.5,
          "answer_relevance_score": 0.8437318815456241,
          "is_answer_relevant": true,
          "relevance_category": "medium"
        },
        {
          "example_id": "attention_002",
          "question": "What is the dimensionality of the model (dmodel) in the base Transformer?",
          "difficulty": "easy",
          "query_type": "factual",
          "retrieval_metrics": {
            "recall_at_k": 0.0,
            "precision_at_k": 0.0,
            "f1_at_k": 0.0,
            "hit_rate": 0.0,
            "mrr": 0.0,
            "ndcg_at_5": 0.0
          },
          "groundedness_score": 0.0,
          "confidence_score": 0.9,
          "has_hallucination": true,
          "final_answer": "The dimensionality of the model (dmodel) in the base Transformer is 512, as stated in the retrieved documents.",
          "ground_truth_answer": "The base Transformer model uses dmodel=512. This is the dimensionality of the input and output of all sub-layers in the model, including the attention layers and feed-forward networks.",
          "semantic_similarity": 0.9,
          "factual_accuracy": 1.0,
          "completeness": 0.7,
          "answer_relevance_score": 0.9081291580784143,
          "is_answer_relevant": true,
          "relevance_category": "high"
        },
        {
          "example_id": "attention_003",
          "question": "What is self-attention and how does it differ from traditional attention?",
          "difficulty": "medium",
          "query_type": "conceptual",
          "retrieval_metrics": {
            "recall_at_k": 0.3333333333333333,
            "precision_at_k": 0.2,
            "f1_at_k": 0.25,
            "hit_rate": 1.0,
            "mrr": 1.0,
            "ndcg_at_5": 0.3393978490514598
          },
          "groundedness_score": 1.0,
          "confidence_score": 0.75,
          "has_hallucination": false,
          "final_answer": "Self-attention, sometimes referred to as intra-attention, is an attention mechanism that relates different positions within a single sequence to compute a representation of that sequence. According to the retrieved context, self-attention has been successfully applied in various tasks such as reading comprehension, abstractive summarization, and textual entailment.\n\nThe key difference between self-attention and traditional attention lies in their focus. Traditional attention often refers to mechanisms that relate different sequences, such as aligning the input and output sequences in tasks like machine translation. In contrast, self-attention focuses on relating different positions of the same sequence, enabling each position to consider the entire sequence for context and representation.\n\nThe provided context does not contain enough information to delve deeper into the specific differences or to provide additional examples of traditional attention mechanisms.",
          "ground_truth_answer": "Self-attention is an attention mechanism where the queries, keys, and values all come from the same source sequence, allowing the model to relate different positions within a single sequence. Unlike traditional attention mechanisms that attend from one sequence to another (like encoder-decoder attention), self-attention computes relationships between all positions in the same sequence, enabling the model to capture dependencies regardless of their distance in the sequence.",
          "semantic_similarity": 0.8,
          "factual_accuracy": 1.0,
          "completeness": 0.7,
          "answer_relevance_score": 0.7781600375590524,
          "is_answer_relevant": true,
          "relevance_category": "medium"
        },
        {
          "example_id": "attention_004",
          "question": "How does multi-head attention work and what are its benefits?",
          "difficulty": "medium",
          "query_type": "conceptual",
          "retrieval_metrics": {
            "recall_at_k": 0.6666666666666666,
            "precision_at_k": 0.4,
            "f1_at_k": 0.5,
            "hit_rate": 1.0,
            "mrr": 1.0,
            "ndcg_at_5": 0.8303010754742701
          },
          "groundedness_score": 1.0,
          "confidence_score": 0.85,
          "has_hallucination": false,
          "final_answer": "Multi-head attention works by allowing the model to jointly attend to information from different representation subspaces at various positions. This is achieved through multiple attention heads, where each head computes its own set of key, value, and query projections. The outputs of these heads are then concatenated and projected to form the final output values. The mathematical formulation is given by:\n\n\\[ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W_O \\]\n\nwhere each head is computed as:\n\n\\[ \\text{head}_i = \\text{Attention}(Q W_Q^i, K W_K^i, V W_V^i) \\]\n\nThis allows for each attention head to capture different aspects of the input data, thus enhancing the model's ability to learn complex relationships.\n\nThe benefits of multi-head attention include:\n1. **Diverse Representation**: Each head can focus on different parts of the input, allowing the model to capture various features and relationships in the data.\n2. **Reduced Dimensionality**: By splitting the attention into multiple heads with reduced dimensionality, the total computational cost remains similar to single-head attention with full dimensionality, making it efficient.\n3. **Improved Performance**: The parallel processing of multiple attention heads can lead to better performance in tasks such as natural language processing and image recognition, as it allows the model to consider a broader context and dependencies within the data.\n\nThis information is derived from the provided context in \"Attention Is All You Need.pdf\" which details the mechanisms and applications of multi-head attention in the Transformer model.",
          "ground_truth_answer": "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Instead of performing a single attention function, multi-head attention projects the queries, keys, and values h times with different learned linear projections. Each head performs attention independently, and the outputs are concatenated and linearly transformed. This mechanism allows the model to capture different types of relationships and attend to different parts of the representation space simultaneously, improving the model's ability to focus on different aspects of the input.",
          "semantic_similarity": 0.9,
          "factual_accuracy": 1.0,
          "completeness": 0.8,
          "answer_relevance_score": 0.7781412502535419,
          "is_answer_relevant": true,
          "relevance_category": "medium"
        },
        {
          "example_id": "attention_005",
          "question": "Explain the complete forward pass through the Transformer encoder, including all sub-layers and their connections.",
          "difficulty": "hard",
          "query_type": "procedural",
          "retrieval_metrics": {
            "recall_at_k": 0.25,
            "precision_at_k": 0.2,
            "f1_at_k": 0.22222222222222224,
            "hit_rate": 1.0,
            "mrr": 0.5,
            "ndcg_at_5": 0.2802412438326486
          },
          "groundedness_score": 1.0,
          "confidence_score": 0.4,
          "has_hallucination": false,
          "final_answer": "The provided context does not contain enough information to answer this question completely.",
          "ground_truth_answer": "The Transformer encoder consists of N=6 identical layers. Each layer has two sub-layers: (1) a multi-head self-attention mechanism, and (2) a position-wise fully connected feed-forward network. A residual connection is applied around each sub-layer, followed by layer normalization. The forward pass works as follows: First, the input embeddings are added with positional encodings. Then, for each of the 6 encoder layers: the input passes through multi-head self-attention (allowing positions to attend to all positions), the output is added to the input via residual connection and normalized, then it passes through a feed-forward network (two linear transformations with ReLU activation), and again a residual connection and normalization are applied. The output of the final encoder layer is then passed to the decoder.",
          "semantic_similarity": 0.0,
          "factual_accuracy": 0.0,
          "completeness": 0.0,
          "answer_relevance_score": 0.22649501162093053,
          "is_answer_relevant": false,
          "relevance_category": "low"
        }
      ],
      "total_examples": 5,
      "successful_evaluations": 5,
      "tier": "budget",
      "tier_config": {
        "name": "Budget",
        "daily_cost": 1200,
        "quality_narrative": "70-75%",
        "models": "All GPT-4o-mini",
        "targets": {
          "f1_at_k": [
            0.2,
            0.3
          ],
          "groundedness": [
            0.85,
            0.95
          ],
          "confidence": [
            0.65,
            0.8
          ]
        }
      },
      "execution_time_seconds": 334.7906904220581,
      "examples_evaluated": 5,
      "avg_time_per_example": 66.95813808441162
    },
    "balanced": {
      "retrieval_metrics": {
        "recall_at_k": 0.3333333333333333,
        "precision_at_k": 0.2,
        "f1_at_k": 0.24603174603174605,
        "hit_rate": 0.6,
        "mrr": 0.5,
        "ndcg_at_5": 0.35236985161219214
      },
      "generation_metrics": {
        "avg_groundedness": 0.9333333333333332,
        "avg_confidence": 0.65,
        "hallucination_rate": 0.2,
        "avg_semantic_similarity": 0.52,
        "avg_factual_accuracy": 0.5599999999999999,
        "avg_completeness": 0.4,
        "avg_answer_relevance": 0.56063856542086
      },
      "per_difficulty_breakdown": {
        "easy": {
          "recall_at_k": 0.25,
          "precision_at_k": 0.1,
          "f1_at_k": 0.14285714285714288,
          "hit_rate": 0.5,
          "mrr": 0.25,
          "ndcg_at_5": 0.1480409554829326
        },
        "medium": {
          "recall_at_k": 0.3333333333333333,
          "precision_at_k": 0.2,
          "f1_at_k": 0.25,
          "hit_rate": 0.5,
          "mrr": 0.5,
          "ndcg_at_5": 0.41515053773713506
        },
        "hard": {
          "recall_at_k": 0.5,
          "precision_at_k": 0.4,
          "f1_at_k": 0.4444444444444445,
          "hit_rate": 1.0,
          "mrr": 1.0,
          "ndcg_at_5": 0.6354662716208254
        }
      },
      "per_query_type_breakdown": {
        "factual": {
          "recall_at_k": 0.25,
          "precision_at_k": 0.1,
          "f1_at_k": 0.14285714285714288,
          "hit_rate": 0.5,
          "mrr": 0.25,
          "ndcg_at_5": 0.1480409554829326
        },
        "conceptual": {
          "recall_at_k": 0.3333333333333333,
          "precision_at_k": 0.2,
          "f1_at_k": 0.25,
          "hit_rate": 0.5,
          "mrr": 0.5,
          "ndcg_at_5": 0.41515053773713506
        },
        "procedural": {
          "recall_at_k": 0.5,
          "precision_at_k": 0.4,
          "f1_at_k": 0.4444444444444445,
          "hit_rate": 1.0,
          "mrr": 1.0,
          "ndcg_at_5": 0.6354662716208254
        }
      },
      "per_example_results": [
        {
          "example_id": "attention_001",
          "question": "How many attention heads are used in the base Transformer model?",
          "difficulty": "easy",
          "query_type": "factual",
          "retrieval_metrics": {
            "recall_at_k": 0.5,
            "precision_at_k": 0.2,
            "f1_at_k": 0.28571428571428575,
            "hit_rate": 1.0,
            "mrr": 0.5,
            "ndcg_at_5": 0.2960819109658652
          },
          "groundedness_score": 1.0,
          "confidence_score": 0.95,
          "has_hallucination": false,
          "final_answer": "The base Transformer model uses 8 parallel attention heads. This is stated in the context: \"In this work we employ h = 8 parallel attention layers, or heads.\"",
          "ground_truth_answer": "The base Transformer model uses 8 attention heads (h=8). This is specified in the model configuration where the base model has dmodel=512 and uses 8 parallel attention heads, with each head operating on dk=dv=64 dimensions.",
          "semantic_similarity": 0.8,
          "factual_accuracy": 0.8,
          "completeness": 0.5,
          "answer_relevance_score": 0.8284804933694732,
          "is_answer_relevant": true,
          "relevance_category": "medium"
        },
        {
          "example_id": "attention_002",
          "question": "What is the dimensionality of the model (dmodel) in the base Transformer?",
          "difficulty": "easy",
          "query_type": "factual",
          "retrieval_metrics": {
            "recall_at_k": 0.0,
            "precision_at_k": 0.0,
            "f1_at_k": 0.0,
            "hit_rate": 0.0,
            "mrr": 0.0,
            "ndcg_at_5": 0.0
          },
          "groundedness_score": 0.6666666666666666,
          "confidence_score": 0.95,
          "has_hallucination": true,
          "final_answer": "The dimensionality of the model (dmodel) in the base Transformer is 512. This is stated in the retrieved context, specifically in the section detailing the encoder architecture.",
          "ground_truth_answer": "The base Transformer model uses dmodel=512. This is the dimensionality of the input and output of all sub-layers in the model, including the attention layers and feed-forward networks.",
          "semantic_similarity": 0.9,
          "factual_accuracy": 1.0,
          "completeness": 0.7,
          "answer_relevance_score": 0.8986848131922185,
          "is_answer_relevant": true,
          "relevance_category": "high"
        },
        {
          "example_id": "attention_003",
          "question": "What is self-attention and how does it differ from traditional attention?",
          "difficulty": "medium",
          "query_type": "conceptual",
          "retrieval_metrics": {
            "recall_at_k": 0.0,
            "precision_at_k": 0.0,
            "f1_at_k": 0.0,
            "hit_rate": 0.0,
            "mrr": 0.0,
            "ndcg_at_5": 0.0
          },
          "groundedness_score": 1.0,
          "confidence_score": 0.3,
          "has_hallucination": false,
          "final_answer": "The provided context does not contain enough information to answer this question.",
          "ground_truth_answer": "Self-attention is an attention mechanism where the queries, keys, and values all come from the same source sequence, allowing the model to relate different positions within a single sequence. Unlike traditional attention mechanisms that attend from one sequence to another (like encoder-decoder attention), self-attention computes relationships between all positions in the same sequence, enabling the model to capture dependencies regardless of their distance in the sequence.",
          "semantic_similarity": 0.0,
          "factual_accuracy": 0.0,
          "completeness": 0.0,
          "answer_relevance_score": 0.13476752099864375,
          "is_answer_relevant": false,
          "relevance_category": "low"
        },
        {
          "example_id": "attention_004",
          "question": "How does multi-head attention work and what are its benefits?",
          "difficulty": "medium",
          "query_type": "conceptual",
          "retrieval_metrics": {
            "recall_at_k": 0.6666666666666666,
            "precision_at_k": 0.4,
            "f1_at_k": 0.5,
            "hit_rate": 1.0,
            "mrr": 1.0,
            "ndcg_at_5": 0.8303010754742701
          },
          "groundedness_score": 1.0,
          "confidence_score": 0.85,
          "has_hallucination": false,
          "final_answer": "Multi-head attention works by allowing the model to jointly attend to information from different representation subspaces at various positions. It combines multiple attention heads, where each head processes the input queries, keys, and values through separate linear projections and computes attention scores. The outputs from these heads are then concatenated and projected again to produce the final values. Specifically, the formula for multi-head attention is:\n\n\\[ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) W_O \\]\n\nwhere each head is calculated as:\n\n\\[ \\text{head}_i = \\text{Attention}(QW_{Q_i}, K W_{K_i}, V W_{V_i}) \\]\n\nThe benefits of multi-head attention include the ability to focus on different parts of the input sequence simultaneously and the reduced dimensionality of each head, which maintains a computational cost similar to that of single-head attention with full dimensionality. This architecture allows for more parallelization in computations and improves the model's ability to learn various aspects of the data across different heads (Document 1).\n\nIn the context of the Transformer model, multi-head attention is utilized in three main ways: encoder-decoder attention, self-attention in the encoder, and self-attention in the decoder, enabling the model to efficiently draw global dependencies between input and output sequences (Document 2).",
          "ground_truth_answer": "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Instead of performing a single attention function, multi-head attention projects the queries, keys, and values h times with different learned linear projections. Each head performs attention independently, and the outputs are concatenated and linearly transformed. This mechanism allows the model to capture different types of relationships and attend to different parts of the representation space simultaneously, improving the model's ability to focus on different aspects of the input.",
          "semantic_similarity": 0.9,
          "factual_accuracy": 1.0,
          "completeness": 0.8,
          "answer_relevance_score": 0.7601389977331005,
          "is_answer_relevant": true,
          "relevance_category": "medium"
        },
        {
          "example_id": "attention_005",
          "question": "Explain the complete forward pass through the Transformer encoder, including all sub-layers and their connections.",
          "difficulty": "hard",
          "query_type": "procedural",
          "retrieval_metrics": {
            "recall_at_k": 0.5,
            "precision_at_k": 0.4,
            "f1_at_k": 0.4444444444444445,
            "hit_rate": 1.0,
            "mrr": 1.0,
            "ndcg_at_5": 0.6354662716208254
          },
          "groundedness_score": 1.0,
          "confidence_score": 0.2,
          "has_hallucination": false,
          "final_answer": "The provided context does not contain enough information to answer this question.",
          "ground_truth_answer": "The Transformer encoder consists of N=6 identical layers. Each layer has two sub-layers: (1) a multi-head self-attention mechanism, and (2) a position-wise fully connected feed-forward network. A residual connection is applied around each sub-layer, followed by layer normalization. The forward pass works as follows: First, the input embeddings are added with positional encodings. Then, for each of the 6 encoder layers: the input passes through multi-head self-attention (allowing positions to attend to all positions), the output is added to the input via residual connection and normalized, then it passes through a feed-forward network (two linear transformations with ReLU activation), and again a residual connection and normalization are applied. The output of the final encoder layer is then passed to the decoder.",
          "semantic_similarity": 0.0,
          "factual_accuracy": 0.0,
          "completeness": 0.0,
          "answer_relevance_score": 0.18112100181086396,
          "is_answer_relevant": false,
          "relevance_category": "low"
        }
      ],
      "total_examples": 5,
      "successful_evaluations": 5,
      "tier": "balanced",
      "tier_config": {
        "name": "Balanced",
        "daily_cost": 1800,
        "quality_narrative": "78-80%",
        "models": "Hybrid GPT-4o-mini + GPT-5-mini",
        "targets": {
          "f1_at_k": [
            0.28,
            0.38
          ],
          "groundedness": [
            0.88,
            0.98
          ],
          "confidence": [
            0.72,
            0.87
          ]
        }
      },
      "execution_time_seconds": 588.3102285861969,
      "examples_evaluated": 5,
      "avg_time_per_example": 117.66204571723938
    },
    "premium": {
      "retrieval_metrics": {
        "recall_at_k": 0.5666666666666667,
        "precision_at_k": 0.32,
        "f1_at_k": 0.40317460317460324,
        "hit_rate": 0.8,
        "mrr": 0.6666666666666667,
        "ndcg_at_5": 0.5488947962602511
      },
      "generation_metrics": {
        "avg_groundedness": 0.9364055299539171,
        "avg_confidence": 0.9119999999999999,
        "hallucination_rate": 0.2,
        "avg_semantic_similarity": 0.8,
        "avg_factual_accuracy": 1.0,
        "avg_completeness": 0.6599999999999999,
        "avg_answer_relevance": 0.7713264447863665
      },
      "per_difficulty_breakdown": {
        "easy": {
          "recall_at_k": 0.5,
          "precision_at_k": 0.2,
          "f1_at_k": 0.28571428571428575,
          "hit_rate": 0.5,
          "mrr": 0.5,
          "ndcg_at_5": 0.38621987288678894
        },
        "medium": {
          "recall_at_k": 0.6666666666666666,
          "precision_at_k": 0.4,
          "f1_at_k": 0.5,
          "hit_rate": 1.0,
          "mrr": 1.0,
          "ndcg_at_5": 0.7793269476109768
        },
        "hard": {
          "recall_at_k": 0.5,
          "precision_at_k": 0.4,
          "f1_at_k": 0.4444444444444445,
          "hit_rate": 1.0,
          "mrr": 0.3333333333333333,
          "ndcg_at_5": 0.41338034030572435
        }
      },
      "per_query_type_breakdown": {
        "factual": {
          "recall_at_k": 0.5,
          "precision_at_k": 0.2,
          "f1_at_k": 0.28571428571428575,
          "hit_rate": 0.5,
          "mrr": 0.5,
          "ndcg_at_5": 0.38621987288678894
        },
        "conceptual": {
          "recall_at_k": 0.6666666666666666,
          "precision_at_k": 0.4,
          "f1_at_k": 0.5,
          "hit_rate": 1.0,
          "mrr": 1.0,
          "ndcg_at_5": 0.7793269476109768
        },
        "procedural": {
          "recall_at_k": 0.5,
          "precision_at_k": 0.4,
          "f1_at_k": 0.4444444444444445,
          "hit_rate": 1.0,
          "mrr": 0.3333333333333333,
          "ndcg_at_5": 0.41338034030572435
        }
      },
      "per_example_results": [
        {
          "example_id": "attention_001",
          "question": "How many attention heads are used in the base Transformer model?",
          "difficulty": "easy",
          "query_type": "factual",
          "retrieval_metrics": {
            "recall_at_k": 1.0,
            "precision_at_k": 0.4,
            "f1_at_k": 0.5714285714285715,
            "hit_rate": 1.0,
            "mrr": 1.0,
            "ndcg_at_5": 0.7724397457735779
          },
          "groundedness_score": 1.0,
          "confidence_score": 0.98,
          "has_hallucination": false,
          "final_answer": "The base Transformer model uses **8 attention heads**.",
          "ground_truth_answer": "The base Transformer model uses 8 attention heads (h=8). This is specified in the model configuration where the base model has dmodel=512 and uses 8 parallel attention heads, with each head operating on dk=dv=64 dimensions.",
          "semantic_similarity": 0.5,
          "factual_accuracy": 1.0,
          "completeness": 0.2,
          "answer_relevance_score": 0.8737449399302465,
          "is_answer_relevant": true,
          "relevance_category": "high"
        },
        {
          "example_id": "attention_002",
          "question": "What is the dimensionality of the model (dmodel) in the base Transformer?",
          "difficulty": "easy",
          "query_type": "factual",
          "retrieval_metrics": {
            "recall_at_k": 0.0,
            "precision_at_k": 0.0,
            "f1_at_k": 0.0,
            "hit_rate": 0.0,
            "mrr": 0.0,
            "ndcg_at_5": 0.0
          },
          "groundedness_score": 1.0,
          "confidence_score": 0.98,
          "has_hallucination": false,
          "final_answer": "The dimensionality of the model (dmodel) in the base Transformer is **512**.",
          "ground_truth_answer": "The base Transformer model uses dmodel=512. This is the dimensionality of the input and output of all sub-layers in the model, including the attention layers and feed-forward networks.",
          "semantic_similarity": 0.8,
          "factual_accuracy": 1.0,
          "completeness": 0.5,
          "answer_relevance_score": 0.9031849550426746,
          "is_answer_relevant": true,
          "relevance_category": "high"
        },
        {
          "example_id": "attention_003",
          "question": "What is self-attention and how does it differ from traditional attention?",
          "difficulty": "medium",
          "query_type": "conceptual",
          "retrieval_metrics": {
            "recall_at_k": 0.6666666666666666,
            "precision_at_k": 0.4,
            "f1_at_k": 0.5,
            "hit_rate": 1.0,
            "mrr": 1.0,
            "ndcg_at_5": 0.7283528197476834
          },
          "groundedness_score": 0.7142857142857143,
          "confidence_score": 0.9,
          "has_hallucination": true,
          "final_answer": "An attention function maps a **query** and a set of **key\u2013value pairs** to an **output**, where all of these are vectors. The output is a weighted sum of the values, with weights given by a compatibility function between the query and each key. A specific form, **Scaled Dot-Product Attention**, computes:\n\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right)V\n\\]\n\nwhere \\(Q\\) are queries, \\(K\\) are keys, \\(V\\) are values, and \\(d_k\\) is the key dimension.\n\n**Self-attention** is a special case of this attention mechanism where **the queries, keys, and values all come from the same place**:\n\n- In the encoder: \u201cIn a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\u201d\n- In the decoder: similarly, \u201cself-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.\u201d\n\nThis contrasts with **encoder\u2013decoder (traditional) attention**, where:\n\n- \u201cthe queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.\u201d\n\nSo, the key difference is:\n\n- **Traditional (encoder\u2013decoder) attention**: queries and key\u2013value pairs come from **different sources** (decoder vs encoder).\n- **Self-attention**: queries, keys, and values come from **the same sequence representation**, allowing each position to attend to other positions within that same sequence.",
          "ground_truth_answer": "Self-attention is an attention mechanism where the queries, keys, and values all come from the same source sequence, allowing the model to relate different positions within a single sequence. Unlike traditional attention mechanisms that attend from one sequence to another (like encoder-decoder attention), self-attention computes relationships between all positions in the same sequence, enabling the model to capture dependencies regardless of their distance in the sequence.",
          "semantic_similarity": 0.9,
          "factual_accuracy": 1.0,
          "completeness": 0.8,
          "answer_relevance_score": 0.6160381805916253,
          "is_answer_relevant": false,
          "relevance_category": "low"
        },
        {
          "example_id": "attention_004",
          "question": "How does multi-head attention work and what are its benefits?",
          "difficulty": "medium",
          "query_type": "conceptual",
          "retrieval_metrics": {
            "recall_at_k": 0.6666666666666666,
            "precision_at_k": 0.4,
            "f1_at_k": 0.5,
            "hit_rate": 1.0,
            "mrr": 1.0,
            "ndcg_at_5": 0.8303010754742701
          },
          "groundedness_score": 1.0,
          "confidence_score": 0.95,
          "has_hallucination": false,
          "final_answer": "Multi-head attention works by running several attention mechanisms (\u201cheads\u201d) in parallel on different learned projections of the same inputs, then combining their outputs:\n\n- For each head \\(i\\), the model linearly projects the input into queries, keys, and values using parameter matrices:\n  - \\(W^Q_i \\in \\mathbb{R}^{d_{model} \\times d_k}\\)\n  - \\(W^K_i \\in \\mathbb{R}^{d_{model} \\times d_k}\\)\n  - \\(W^V_i \\in \\mathbb{R}^{d_{model} \\times d_v}\\)\n\n- Each head computes Scaled Dot-Product Attention:\n  \\[\n  \\text{Attention}(QW^Q_i, KW^K_i, VW^V_i)\n  = \\text{softmax}\\left(\\frac{QW^Q_i (KW^K_i)^T}{\\sqrt{d_k}}\\right) VW^V_i\n  \\]\n\n- The outputs of all heads \\(\\text{head}_1, \\dots, \\text{head}_h\\) are concatenated and projected once more:\n  \\[\n  \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W^O\n  \\]\n  where \\(W^O \\in \\mathbb{R}^{h d_v \\times d_{model}}\\).\n\nIn the described setup, the model uses \\(h = 8\\) heads with \\(d_k = d_v = d_{model}/h = 64\\). Because each head operates on a reduced dimension, the total computational cost is similar to a single-head attention mechanism with full dimensionality.\n\nBenefits (as stated in the context):\n\n- Multi-head attention allows the model to **jointly attend to information from different representation subspaces at different positions**. With only a single attention head, averaging inhibits this capability.\n- Using multiple heads with reduced dimensionality keeps the **computational cost similar** to that of single-head attention with full dimensionality, while providing the above representational benefit.",
          "ground_truth_answer": "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Instead of performing a single attention function, multi-head attention projects the queries, keys, and values h times with different learned linear projections. Each head performs attention independently, and the outputs are concatenated and linearly transformed. This mechanism allows the model to capture different types of relationships and attend to different parts of the representation space simultaneously, improving the model's ability to focus on different aspects of the input.",
          "semantic_similarity": 0.9,
          "factual_accuracy": 1.0,
          "completeness": 0.9,
          "answer_relevance_score": 0.7662804297024822,
          "is_answer_relevant": true,
          "relevance_category": "medium"
        },
        {
          "example_id": "attention_005",
          "question": "Explain the complete forward pass through the Transformer encoder, including all sub-layers and their connections.",
          "difficulty": "hard",
          "query_type": "procedural",
          "retrieval_metrics": {
            "recall_at_k": 0.5,
            "precision_at_k": 0.4,
            "f1_at_k": 0.4444444444444445,
            "hit_rate": 1.0,
            "mrr": 0.3333333333333333,
            "ndcg_at_5": 0.41338034030572435
          },
          "groundedness_score": 0.967741935483871,
          "confidence_score": 0.75,
          "has_hallucination": false,
          "final_answer": "The Transformer encoder processes an input sequence as follows, layer by layer:\n\n1. **Input to the encoder**  \n   - The input sequence is first mapped to a **sequence of embedding vectors** (e.g., token embeddings).  \n   - In one described variant, **learnable 1D positional embeddings** are added, and \u201cthe resulting sequence of embedding vectors serves as input to the encoder\u201d (ViT excerpt).\n\n2. **Encoder structure**  \n   - The encoder is \u201ccomposed of a stack of \\(N = 6\\) identical layers.\u201d  \n   - All sub-layers in the model, as well as the embedding layers, produce outputs of the same dimension \\(d_{\\text{model}} = 512\\).  \n\n3. **Composition of each encoder layer**  \n   Each encoder layer has **two sub-layers**:\n\n   1. **Multi-head self-attention sub-layer**  \n      - This sub-layer performs **multi-head self-attention** on the sequence:  \n        \u201cIn a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\u201d\n      - In the ViT-style encoder, this is called a **multiheaded self-attention (MSA)** block.\n\n   2. **Position-wise feed-forward sub-layer**  \n      - The second sub-layer is a \u201csimple, **position-wise fully connected feed-forward network**.\u201d\n\n4. **Residual connections and Layer Normalization**  \n   - For **each** of the two sub-layers, the encoder uses:\n     - A **residual connection** around the sub-layer.\n     - Followed by **Layer Normalization**.  \n   - This is explicitly:  \n     \\[\n     \\text{output} = \\text{LayerNorm}(x + \\text{Sublayer}(x)),\n     \\]\n     where \\(x\\) is the input to the sub-layer, and \\(\\text{Sublayer}(x)\\) is either:\n       - the output of the multi-head self-attention sub-layer, or  \n       - the output of the position-wise feed-forward sub-layer.  \n   - In the ViT description, \u201cLayernorm (LN) is applied **before every block**, and residual connections **after every block**,\u201d but the standard Transformer description given specifies the combined form above.\n\n5. **Forward pass through one encoder layer (summarized)**  \n   Let \\(x\\) be the input sequence to a layer:\n\n   1. **Self-attention block**  \n      - Compute self-attention on \\(x\\): \\(\\text{SA}(x)\\).  \n      - Apply residual and layer norm:\n        \\[\n        x^{(1)} = \\text{LayerNorm}(x + \\text{SA}(x)).\n        \\]\n\n   2. **Feed-forward block**  \n      - Apply the position-wise feed-forward network to \\(x^{(1)}\\): \\(\\text{FFN}(x^{(1)})\\).  \n      - Apply residual and layer norm:\n        \\[\n        x^{(2)} = \\text{LayerNorm}(x^{(1)} + \\text{FFN}(x^{(1)})).\n        \\]\n\n   - The output \\(x^{(2)}\\) becomes the input \\(x\\) for the next encoder layer.\n\n6. **Forward pass through the full encoder stack**  \n   - Start from the embedded (and possibly position-embedded) input sequence.  \n   - Pass it through the stack of \\(N = 6\\) identical layers, each performing:\n     1. Multi-head self-attention with residual + LayerNorm.\n     2. Position-wise feed-forward with residual + LayerNorm.  \n   - The output of the final layer is the **encoder output**, where each position has attended to all positions in the input sequence through the stacked self-attention layers.",
          "ground_truth_answer": "The Transformer encoder consists of N=6 identical layers. Each layer has two sub-layers: (1) a multi-head self-attention mechanism, and (2) a position-wise fully connected feed-forward network. A residual connection is applied around each sub-layer, followed by layer normalization. The forward pass works as follows: First, the input embeddings are added with positional encodings. Then, for each of the 6 encoder layers: the input passes through multi-head self-attention (allowing positions to attend to all positions), the output is added to the input via residual connection and normalized, then it passes through a feed-forward network (two linear transformations with ReLU activation), and again a residual connection and normalization are applied. The output of the final encoder layer is then passed to the decoder.",
          "semantic_similarity": 0.9,
          "factual_accuracy": 1.0,
          "completeness": 0.9,
          "answer_relevance_score": 0.6973837186648041,
          "is_answer_relevant": false,
          "relevance_category": "low"
        }
      ],
      "total_examples": 5,
      "successful_evaluations": 5,
      "tier": "premium",
      "tier_config": {
        "name": "Premium",
        "daily_cost": 12060,
        "quality_narrative": "88-92%",
        "models": "GPT-5.1 + GPT-5-mini + GPT-5-nano",
        "targets": {
          "f1_at_k": [
            0.35,
            0.5
          ],
          "groundedness": [
            0.95,
            1.0
          ],
          "confidence": [
            0.85,
            0.95
          ]
        }
      },
      "execution_time_seconds": 517.8454933166504,
      "examples_evaluated": 5,
      "avg_time_per_example": 103.56909866333008
    }
  }
}