{
  "timestamp": "2025-11-19T12:28:05.426862",
  "test_type": "tier_comparison",
  "tiers_evaluated": [
    "budget",
    "balanced",
    "premium"
  ],
  "results": {
    "budget": {
      "retrieval_metrics": {
        "recall_at_k": 0.4,
        "precision_at_k": 0.24000000000000005,
        "f1_at_k": 0.29603174603174603,
        "hit_rate": 0.8,
        "mrr": 0.7,
        "ndcg_at_5": 0.45706528414331454
      },
      "generation_metrics": {
        "avg_groundedness": 0.9538461538461538,
        "avg_confidence": 0.8400000000000001,
        "hallucination_rate": 0.2,
        "avg_semantic_similarity": 0.798,
        "avg_factual_accuracy": 0.748,
        "avg_completeness": 0.6819999999999999,
        "avg_answer_relevance": 0.785240259352093
      },
      "per_difficulty_breakdown": {
        "easy": {
          "recall_at_k": 0.25,
          "precision_at_k": 0.1,
          "f1_at_k": 0.14285714285714288,
          "hit_rate": 0.5,
          "mrr": 0.5,
          "ndcg_at_5": 0.35195904451706733
        },
        "medium": {
          "recall_at_k": 0.5,
          "precision_at_k": 0.30000000000000004,
          "f1_at_k": 0.375,
          "hit_rate": 1.0,
          "mrr": 0.75,
          "ndcg_at_5": 0.4575752688685675
        },
        "hard": {
          "recall_at_k": 0.5,
          "precision_at_k": 0.4,
          "f1_at_k": 0.4444444444444445,
          "hit_rate": 1.0,
          "mrr": 1.0,
          "ndcg_at_5": 0.6662577939453029
        }
      },
      "per_query_type_breakdown": {
        "factual": {
          "recall_at_k": 0.25,
          "precision_at_k": 0.1,
          "f1_at_k": 0.14285714285714288,
          "hit_rate": 0.5,
          "mrr": 0.5,
          "ndcg_at_5": 0.35195904451706733
        },
        "conceptual": {
          "recall_at_k": 0.5,
          "precision_at_k": 0.30000000000000004,
          "f1_at_k": 0.375,
          "hit_rate": 1.0,
          "mrr": 0.75,
          "ndcg_at_5": 0.4575752688685675
        },
        "procedural": {
          "recall_at_k": 0.5,
          "precision_at_k": 0.4,
          "f1_at_k": 0.4444444444444445,
          "hit_rate": 1.0,
          "mrr": 1.0,
          "ndcg_at_5": 0.6662577939453029
        }
      },
      "per_example_results": [
        {
          "example_id": "attention_001",
          "question": "How many attention heads are used in the base Transformer model?",
          "difficulty": "easy",
          "query_type": "factual",
          "retrieval_metrics": {
            "recall_at_k": 0.5,
            "precision_at_k": 0.2,
            "f1_at_k": 0.28571428571428575,
            "hit_rate": 1.0,
            "mrr": 1.0,
            "ndcg_at_5": 0.7039180890341347
          },
          "groundedness_score": 1.0,
          "confidence_score": 0.85,
          "has_hallucination": false,
          "final_answer": "According to the retrieved information, the base Transformer model, referred to as BERTBASE, utilizes 12 attention heads (A=12) as stated in the second document. This model has 12 layers (L=12) and a hidden size of 768 (H=768).",
          "ground_truth_answer": "The base Transformer model uses 8 attention heads (h=8). This is specified in the model configuration where the base model has dmodel=512 and uses 8 parallel attention heads, with each head operating on dk=dv=64 dimensions.",
          "semantic_similarity": 0.15,
          "factual_accuracy": 0.0,
          "completeness": 0.0,
          "answer_relevance_score": 0.7293710048464815,
          "is_answer_relevant": true,
          "relevance_category": "medium"
        },
        {
          "example_id": "attention_002",
          "question": "What is the dimensionality of the model (dmodel) in the base Transformer?",
          "difficulty": "easy",
          "query_type": "factual",
          "retrieval_metrics": {
            "recall_at_k": 0.0,
            "precision_at_k": 0.0,
            "f1_at_k": 0.0,
            "hit_rate": 0.0,
            "mrr": 0.0,
            "ndcg_at_5": 0.0
          },
          "groundedness_score": 1.0,
          "confidence_score": 0.9,
          "has_hallucination": false,
          "final_answer": "The dimensionality of the model (dmodel) in the base Transformer is 512, as stated in the retrieved context. Specifically, this information can be found in Table 3 of the document.",
          "ground_truth_answer": "The base Transformer model uses dmodel=512. This is the dimensionality of the input and output of all sub-layers in the model, including the attention layers and feed-forward networks.",
          "semantic_similarity": 0.95,
          "factual_accuracy": 1.0,
          "completeness": 0.6,
          "answer_relevance_score": 0.9246463762389351,
          "is_answer_relevant": true,
          "relevance_category": "high"
        },
        {
          "example_id": "attention_003",
          "question": "What is self-attention and how does it differ from traditional attention?",
          "difficulty": "medium",
          "query_type": "conceptual",
          "retrieval_metrics": {
            "recall_at_k": 0.3333333333333333,
            "precision_at_k": 0.2,
            "f1_at_k": 0.25,
            "hit_rate": 1.0,
            "mrr": 1.0,
            "ndcg_at_5": 0.3393978490514598
          },
          "groundedness_score": 1.0,
          "confidence_score": 0.65,
          "has_hallucination": false,
          "final_answer": "Self-attention is a mechanism where the keys, values, and queries all come from the same input, allowing each position in the sequence to attend to all other positions. This contrasts with traditional attention mechanisms, which often involve separate sources for queries and keys/values, typically seen in encoder-decoder architectures. \n\nIn the context of the Transformer model, self-attention is utilized in both the encoder and decoder. In the encoder, self-attention layers allow each position to attend to all positions from the previous layer. Similarly, in the decoder, self-attention layers enable each position to attend to all positions up to and including that position, preventing leftward attention (according to the retrieved information from the provided context).\n\nWhile traditional attention mechanisms often rely on recurrent networks to capture dependencies, self-attention allows for these dependencies to be modeled without regard to their distance, thereby enhancing parallelization and efficiency (as described in the first document).\n\nHowever, the provided context does not elaborate extensively on how self-attention specifically differs from all forms of traditional attention beyond its implementation details. Thus, some nuances may be missing.",
          "ground_truth_answer": "Self-attention is an attention mechanism where the queries, keys, and values all come from the same source sequence, allowing the model to relate different positions within a single sequence. Unlike traditional attention mechanisms that attend from one sequence to another (like encoder-decoder attention), self-attention computes relationships between all positions in the same sequence, enabling the model to capture dependencies regardless of their distance in the sequence.",
          "semantic_similarity": 0.95,
          "factual_accuracy": 0.82,
          "completeness": 0.95,
          "answer_relevance_score": 0.6998561586153995,
          "is_answer_relevant": false,
          "relevance_category": "low"
        },
        {
          "example_id": "attention_004",
          "question": "How does multi-head attention work and what are its benefits?",
          "difficulty": "medium",
          "query_type": "conceptual",
          "retrieval_metrics": {
            "recall_at_k": 0.6666666666666666,
            "precision_at_k": 0.4,
            "f1_at_k": 0.5,
            "hit_rate": 1.0,
            "mrr": 0.5,
            "ndcg_at_5": 0.5757526886856752
          },
          "groundedness_score": 1.0,
          "confidence_score": 0.9,
          "has_hallucination": false,
          "final_answer": "Multi-head attention works by allowing the model to jointly attend to information from different representation subspaces at different positions. Specifically, it involves multiple attention heads that process the input in parallel, rather than relying on a single attention mechanism. The output from each attention head is concatenated and projected to produce the final values. \n\nThe formula for multi-head attention is given as follows:\n\n\\[ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W_O \\]\n\nwhere each head is computed as:\n\n\\[ \\text{head}_i = \\text{Attention}(Q W_{Q_i}, K W_{K_i}, V W_{V_i}) \\]\n\nHere, \\(W_{Q_i}\\), \\(W_{K_i}\\), and \\(W_{V_i}\\) are parameter matrices for queries, keys, and values, respectively, and \\(W_O\\) is a parameter matrix for the output.\n\nThe benefits of multi-head attention include:\n\n1. **Diverse Representation**: By using multiple heads, the model can attend to information from different representation subspaces, capturing various aspects of the input data.\n2. **Parallelization**: The parallel nature of multi-head attention allows for greater computational efficiency compared to single-head attention, especially since the total computational cost remains similar to that of single-head attention with full dimensionality.\n\nThis information is detailed in the context from the retrieved documents, particularly in the discussions on multi-head attention and its applications within the Transformer architecture (Document 2 and Document 3).",
          "ground_truth_answer": "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Instead of performing a single attention function, multi-head attention projects the queries, keys, and values h times with different learned linear projections. Each head performs attention independently, and the outputs are concatenated and linearly transformed. This mechanism allows the model to capture different types of relationships and attend to different parts of the representation space simultaneously, improving the model's ability to focus on different aspects of the input.",
          "semantic_similarity": 0.99,
          "factual_accuracy": 0.92,
          "completeness": 1.0,
          "answer_relevance_score": 0.7700668779976325,
          "is_answer_relevant": true,
          "relevance_category": "medium"
        },
        {
          "example_id": "attention_005",
          "question": "Explain the complete forward pass through the Transformer encoder, including all sub-layers and their connections.",
          "difficulty": "hard",
          "query_type": "procedural",
          "retrieval_metrics": {
            "recall_at_k": 0.5,
            "precision_at_k": 0.4,
            "f1_at_k": 0.4444444444444445,
            "hit_rate": 1.0,
            "mrr": 1.0,
            "ndcg_at_5": 0.6662577939453029
          },
          "groundedness_score": 0.7692307692307693,
          "confidence_score": 0.9,
          "has_hallucination": true,
          "final_answer": "The complete forward pass through the Transformer encoder involves several steps and components, as outlined below:\n\n1. **Input Representation**: The input sequence is first transformed into embeddings. Each input token is represented as a vector of dimension \\(d_{model} = 512\\). The model adds positional embeddings to these token embeddings to retain information about the positions of the tokens in the sequence.\n\n2. **Stack of Encoder Layers**: The encoder consists of a stack of \\(N = 6\\) identical layers. Each layer contains two main sub-layers:\n   - **Multi-Head Self-Attention Mechanism**: This sub-layer allows each position in the input sequence to attend to all positions in the previous layer of the encoder. The output of this mechanism is computed using queries, keys, and values that all come from the same input (the output of the previous encoder layer).\n   - **Feed-Forward Network**: This is a simple, position-wise fully connected network applied to each position separately and identically. \n\n3. **Residual Connections and Layer Normalization**: Each sub-layer in the encoder employs a residual connection around it, followed by layer normalization. Mathematically, this can be expressed as:\n   \\[\n   \\text{output} = \\text{LayerNorm}(x + \\text{Sublayer}(x))\n   \\]\n   where \\(\\text{Sublayer}(x)\\) is the output from the respective sub-layer.\n\n4. **Repetition of Layers**: The process of attention followed by the feed-forward network, along with residual connections and layer normalization, is repeated for each of the six layers in the encoder.\n\nIn summary, the forward pass through the Transformer encoder involves embedding the input tokens, processing them through a series of six identical layers of self-attention and feed-forward networks, while applying residual connections and normalization at each step. This architecture allows for the effective modeling of dependencies in the input sequence.\n\nThis explanation is based on the information provided in the retrieved documents, particularly from \"Attention Is All You Need.pdf.\"",
          "ground_truth_answer": "The Transformer encoder consists of N=6 identical layers. Each layer has two sub-layers: (1) a multi-head self-attention mechanism, and (2) a position-wise fully connected feed-forward network. A residual connection is applied around each sub-layer, followed by layer normalization. The forward pass works as follows: First, the input embeddings are added with positional encodings. Then, for each of the 6 encoder layers: the input passes through multi-head self-attention (allowing positions to attend to all positions), the output is added to the input via residual connection and normalized, then it passes through a feed-forward network (two linear transformations with ReLU activation), and again a residual connection and normalization are applied. The output of the final encoder layer is then passed to the decoder.",
          "semantic_similarity": 0.95,
          "factual_accuracy": 1.0,
          "completeness": 0.86,
          "answer_relevance_score": 0.8022608790620163,
          "is_answer_relevant": true,
          "relevance_category": "medium"
        }
      ],
      "total_examples": 5,
      "successful_evaluations": 5,
      "tier": "budget",
      "tier_config": {
        "name": "Budget",
        "daily_cost": 1200,
        "quality_narrative": "70-75%",
        "models": "All GPT-4o-mini",
        "targets": {
          "f1_at_k": [
            0.2,
            0.3
          ],
          "groundedness": [
            0.85,
            0.95
          ],
          "confidence": [
            0.65,
            0.8
          ]
        }
      },
      "execution_time_seconds": 444.940735578537,
      "examples_evaluated": 5,
      "avg_time_per_example": 88.98814711570739
    },
    "balanced": {
      "retrieval_metrics": {
        "recall_at_k": 0.3833333333333333,
        "precision_at_k": 0.2,
        "f1_at_k": 0.2587301587301588,
        "hit_rate": 0.6,
        "mrr": 0.45,
        "ndcg_at_5": 0.3920305873020813
      },
      "generation_metrics": {
        "avg_groundedness": 0.8966666666666665,
        "avg_confidence": 0.882,
        "hallucination_rate": 0.2,
        "avg_semantic_similarity": 0.756,
        "avg_factual_accuracy": 0.796,
        "avg_completeness": 0.562,
        "avg_answer_relevance": 0.6770521710885653
      },
      "per_difficulty_breakdown": {
        "easy": {
          "recall_at_k": 0.5,
          "precision_at_k": 0.2,
          "f1_at_k": 0.28571428571428575,
          "hit_rate": 0.5,
          "mrr": 0.5,
          "ndcg_at_5": 0.46927872602275644
        },
        "medium": {
          "recall_at_k": 0.3333333333333333,
          "precision_at_k": 0.2,
          "f1_at_k": 0.25,
          "hit_rate": 0.5,
          "mrr": 0.5,
          "ndcg_at_5": 0.41515053773713506
        },
        "hard": {
          "recall_at_k": 0.25,
          "precision_at_k": 0.2,
          "f1_at_k": 0.22222222222222224,
          "hit_rate": 1.0,
          "mrr": 0.25,
          "ndcg_at_5": 0.19129440899062336
        }
      },
      "per_query_type_breakdown": {
        "factual": {
          "recall_at_k": 0.5,
          "precision_at_k": 0.2,
          "f1_at_k": 0.28571428571428575,
          "hit_rate": 0.5,
          "mrr": 0.5,
          "ndcg_at_5": 0.46927872602275644
        },
        "conceptual": {
          "recall_at_k": 0.3333333333333333,
          "precision_at_k": 0.2,
          "f1_at_k": 0.25,
          "hit_rate": 0.5,
          "mrr": 0.5,
          "ndcg_at_5": 0.41515053773713506
        },
        "procedural": {
          "recall_at_k": 0.25,
          "precision_at_k": 0.2,
          "f1_at_k": 0.22222222222222224,
          "hit_rate": 1.0,
          "mrr": 0.25,
          "ndcg_at_5": 0.19129440899062336
        }
      },
      "per_example_results": [
        {
          "example_id": "attention_001",
          "question": "How many attention heads are used in the base Transformer model?",
          "difficulty": "easy",
          "query_type": "factual",
          "retrieval_metrics": {
            "recall_at_k": 1.0,
            "precision_at_k": 0.4,
            "f1_at_k": 0.5714285714285715,
            "hit_rate": 1.0,
            "mrr": 1.0,
            "ndcg_at_5": 0.9385574520455129
          },
          "groundedness_score": 1.0,
          "confidence_score": 0.9,
          "has_hallucination": false,
          "final_answer": "The base Transformer model uses 8 attention heads, as stated in the retrieved context: \"we employ h = 8 parallel attention layers, or heads.\"",
          "ground_truth_answer": "The base Transformer model uses 8 attention heads (h=8). This is specified in the model configuration where the base model has dmodel=512 and uses 8 parallel attention heads, with each head operating on dk=dv=64 dimensions.",
          "semantic_similarity": 0.9,
          "factual_accuracy": 1.0,
          "completeness": 0.33,
          "answer_relevance_score": 0.8446124443153475,
          "is_answer_relevant": true,
          "relevance_category": "medium"
        },
        {
          "example_id": "attention_002",
          "question": "What is the dimensionality of the model (dmodel) in the base Transformer?",
          "difficulty": "easy",
          "query_type": "factual",
          "retrieval_metrics": {
            "recall_at_k": 0.0,
            "precision_at_k": 0.0,
            "f1_at_k": 0.0,
            "hit_rate": 0.0,
            "mrr": 0.0,
            "ndcg_at_5": 0.0
          },
          "groundedness_score": 0.6666666666666666,
          "confidence_score": 0.9,
          "has_hallucination": true,
          "final_answer": "The dimensionality of the model (dmodel) in the base Transformer is 512, as stated in the retrieved context (Document 2).",
          "ground_truth_answer": "The base Transformer model uses dmodel=512. This is the dimensionality of the input and output of all sub-layers in the model, including the attention layers and feed-forward networks.",
          "semantic_similarity": 0.9,
          "factual_accuracy": 1.0,
          "completeness": 0.5,
          "answer_relevance_score": 0.9150206976287883,
          "is_answer_relevant": true,
          "relevance_category": "high"
        },
        {
          "example_id": "attention_003",
          "question": "What is self-attention and how does it differ from traditional attention?",
          "difficulty": "medium",
          "query_type": "conceptual",
          "retrieval_metrics": {
            "recall_at_k": 0.0,
            "precision_at_k": 0.0,
            "f1_at_k": 0.0,
            "hit_rate": 0.0,
            "mrr": 0.0,
            "ndcg_at_5": 0.0
          },
          "groundedness_score": 0.9,
          "confidence_score": 0.88,
          "has_hallucination": false,
          "final_answer": "Self-attention is a mechanism used in the Transformer architecture that allows the model to weigh the importance of different words in an input sequence when encoding a particular word. In a self-attention layer, all the keys, values, and queries come from the same source, which is the output of the previous layer in the encoder. This enables each position in the encoder to attend to all positions in the previous layer, effectively capturing relationships and contextual dependencies across the entire sequence (Document 1).\n\nIn contrast, traditional attention mechanisms, such as those found in sequence-to-sequence models, typically involve separate sources for queries and keys/values. For instance, in encoder-decoder attention layers of the Transformer, the queries come from the previous decoder layer while the memory keys and values come from the encoder's output. This allows each position in the decoder to attend over all positions in the input sequence (Document 1).\n\nThus, the key difference is that self-attention uses the same source for queries, keys, and values, making it a more integrated approach compared to traditional attention, which relies on distinct sources for different components.",
          "ground_truth_answer": "Self-attention is an attention mechanism where the queries, keys, and values all come from the same source sequence, allowing the model to relate different positions within a single sequence. Unlike traditional attention mechanisms that attend from one sequence to another (like encoder-decoder attention), self-attention computes relationships between all positions in the same sequence, enabling the model to capture dependencies regardless of their distance in the sequence.",
          "semantic_similarity": 0.98,
          "factual_accuracy": 1.0,
          "completeness": 0.98,
          "answer_relevance_score": 0.663987452586615,
          "is_answer_relevant": false,
          "relevance_category": "low"
        },
        {
          "example_id": "attention_004",
          "question": "How does multi-head attention work and what are its benefits?",
          "difficulty": "medium",
          "query_type": "conceptual",
          "retrieval_metrics": {
            "recall_at_k": 0.6666666666666666,
            "precision_at_k": 0.4,
            "f1_at_k": 0.5,
            "hit_rate": 1.0,
            "mrr": 1.0,
            "ndcg_at_5": 0.8303010754742701
          },
          "groundedness_score": 0.9166666666666666,
          "confidence_score": 0.88,
          "has_hallucination": false,
          "final_answer": "Multi-head attention works by allowing the model to jointly attend to information from different representation subspaces at different positions. It consists of several attention layers running in parallel, where each layer (or head) performs attention on different projections of the input data.\n\nThe formula for multi-head attention is given as:\n\n\\[ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W_O \\]\n\nwhere each head is computed using:\n\n\\[ \\text{head}_i = \\text{Attention}(Q W_Q^i, K W_K^i, V W_V^i) \\]\n\nThis approach allows the model to capture different aspects of the data simultaneously, as each head can focus on different parts of the input sequence.\n\nThe benefits of multi-head attention include:\n\n1. **Diverse Representation**: It allows the model to learn different representations of the data by attending to various parts of the input simultaneously.\n2. **Reduced Dimensionality**: By using multiple heads with smaller dimensions, the total computational cost remains similar to that of single-head attention with full dimensionality, making it efficient.\n3. **Improved Performance**: The joint attention across multiple heads enhances the model's ability to understand complex relationships in the data, leading to better performance in tasks such as translation and image recognition.\n\nAccording to the retrieved information, these functionalities and advantages are part of the design of the Transformer model, which employs multi-head attention in various ways, including in encoder-decoder attention layers and self-attention layers.",
          "ground_truth_answer": "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Instead of performing a single attention function, multi-head attention projects the queries, keys, and values h times with different learned linear projections. Each head performs attention independently, and the outputs are concatenated and linearly transformed. This mechanism allows the model to capture different types of relationships and attend to different parts of the representation space simultaneously, improving the model's ability to focus on different aspects of the input.",
          "semantic_similarity": 1.0,
          "factual_accuracy": 0.98,
          "completeness": 1.0,
          "answer_relevance_score": 0.7805134740142718,
          "is_answer_relevant": true,
          "relevance_category": "medium"
        },
        {
          "example_id": "attention_005",
          "question": "Explain the complete forward pass through the Transformer encoder, including all sub-layers and their connections.",
          "difficulty": "hard",
          "query_type": "procedural",
          "retrieval_metrics": {
            "recall_at_k": 0.25,
            "precision_at_k": 0.2,
            "f1_at_k": 0.22222222222222224,
            "hit_rate": 1.0,
            "mrr": 0.25,
            "ndcg_at_5": 0.19129440899062336
          },
          "groundedness_score": 1.0,
          "confidence_score": 0.85,
          "has_hallucination": false,
          "final_answer": "The provided context does not contain enough information to answer this question.",
          "ground_truth_answer": "The Transformer encoder consists of N=6 identical layers. Each layer has two sub-layers: (1) a multi-head self-attention mechanism, and (2) a position-wise fully connected feed-forward network. A residual connection is applied around each sub-layer, followed by layer normalization. The forward pass works as follows: First, the input embeddings are added with positional encodings. Then, for each of the 6 encoder layers: the input passes through multi-head self-attention (allowing positions to attend to all positions), the output is added to the input via residual connection and normalized, then it passes through a feed-forward network (two linear transformations with ReLU activation), and again a residual connection and normalization are applied. The output of the final encoder layer is then passed to the decoder.",
          "semantic_similarity": 0.0,
          "factual_accuracy": 0.0,
          "completeness": 0.0,
          "answer_relevance_score": 0.18112678689780387,
          "is_answer_relevant": false,
          "relevance_category": "low"
        }
      ],
      "total_examples": 5,
      "successful_evaluations": 5,
      "tier": "balanced",
      "tier_config": {
        "name": "Balanced",
        "daily_cost": 1800,
        "quality_narrative": "78-80%",
        "models": "Hybrid GPT-4o-mini + GPT-5-mini",
        "targets": {
          "f1_at_k": [
            0.28,
            0.38
          ],
          "groundedness": [
            0.88,
            0.98
          ],
          "confidence": [
            0.72,
            0.87
          ]
        }
      },
      "execution_time_seconds": 404.69233083724976,
      "examples_evaluated": 5,
      "avg_time_per_example": 80.93846616744995
    },
    "premium": {
      "retrieval_metrics": {
        "recall_at_k": 0.3333333333333333,
        "precision_at_k": 0.2,
        "f1_at_k": 0.24603174603174605,
        "hit_rate": 0.6,
        "mrr": 0.6,
        "ndcg_at_5": 0.3870092146235704
      },
      "generation_metrics": {
        "avg_groundedness": 0.9333333333333333,
        "avg_confidence": 0.898,
        "hallucination_rate": 0.2,
        "avg_semantic_similarity": 0.984,
        "avg_factual_accuracy": 0.99,
        "avg_completeness": 0.716,
        "avg_answer_relevance": 0.7763728123127975
      },
      "per_difficulty_breakdown": {
        "easy": {
          "recall_at_k": 0.25,
          "precision_at_k": 0.1,
          "f1_at_k": 0.14285714285714288,
          "hit_rate": 0.5,
          "mrr": 0.5,
          "ndcg_at_5": 0.23463936301137822
        },
        "medium": {
          "recall_at_k": 0.3333333333333333,
          "precision_at_k": 0.2,
          "f1_at_k": 0.25,
          "hit_rate": 0.5,
          "mrr": 0.5,
          "ndcg_at_5": 0.41515053773713506
        },
        "hard": {
          "recall_at_k": 0.5,
          "precision_at_k": 0.4,
          "f1_at_k": 0.4444444444444445,
          "hit_rate": 1.0,
          "mrr": 1.0,
          "ndcg_at_5": 0.6354662716208254
        }
      },
      "per_query_type_breakdown": {
        "factual": {
          "recall_at_k": 0.25,
          "precision_at_k": 0.1,
          "f1_at_k": 0.14285714285714288,
          "hit_rate": 0.5,
          "mrr": 0.5,
          "ndcg_at_5": 0.23463936301137822
        },
        "conceptual": {
          "recall_at_k": 0.3333333333333333,
          "precision_at_k": 0.2,
          "f1_at_k": 0.25,
          "hit_rate": 0.5,
          "mrr": 0.5,
          "ndcg_at_5": 0.41515053773713506
        },
        "procedural": {
          "recall_at_k": 0.5,
          "precision_at_k": 0.4,
          "f1_at_k": 0.4444444444444445,
          "hit_rate": 1.0,
          "mrr": 1.0,
          "ndcg_at_5": 0.6354662716208254
        }
      },
      "per_example_results": [
        {
          "example_id": "attention_001",
          "question": "How many attention heads are used in the base Transformer model?",
          "difficulty": "easy",
          "query_type": "factual",
          "retrieval_metrics": {
            "recall_at_k": 0.5,
            "precision_at_k": 0.2,
            "f1_at_k": 0.28571428571428575,
            "hit_rate": 1.0,
            "mrr": 1.0,
            "ndcg_at_5": 0.46927872602275644
          },
          "groundedness_score": 1.0,
          "confidence_score": 0.9,
          "has_hallucination": false,
          "final_answer": "The base Transformer model described in the provided context uses **8 attention heads**.",
          "ground_truth_answer": "The base Transformer model uses 8 attention heads (h=8). This is specified in the model configuration where the base model has dmodel=512 and uses 8 parallel attention heads, with each head operating on dk=dv=64 dimensions.",
          "semantic_similarity": 1.0,
          "factual_accuracy": 1.0,
          "completeness": 0.33,
          "answer_relevance_score": 0.8471100615665863,
          "is_answer_relevant": true,
          "relevance_category": "medium"
        },
        {
          "example_id": "attention_002",
          "question": "What is the dimensionality of the model (dmodel) in the base Transformer?",
          "difficulty": "easy",
          "query_type": "factual",
          "retrieval_metrics": {
            "recall_at_k": 0.0,
            "precision_at_k": 0.0,
            "f1_at_k": 0.0,
            "hit_rate": 0.0,
            "mrr": 0.0,
            "ndcg_at_5": 0.0
          },
          "groundedness_score": 1.0,
          "confidence_score": 0.98,
          "has_hallucination": false,
          "final_answer": "The dimensionality of the model (dmodel) in the base Transformer is 512.",
          "ground_truth_answer": "The base Transformer model uses dmodel=512. This is the dimensionality of the input and output of all sub-layers in the model, including the attention layers and feed-forward networks.",
          "semantic_similarity": 1.0,
          "factual_accuracy": 1.0,
          "completeness": 0.5,
          "answer_relevance_score": 0.9087373972531098,
          "is_answer_relevant": true,
          "relevance_category": "high"
        },
        {
          "example_id": "attention_003",
          "question": "What is self-attention and how does it differ from traditional attention?",
          "difficulty": "medium",
          "query_type": "conceptual",
          "retrieval_metrics": {
            "recall_at_k": 0.0,
            "precision_at_k": 0.0,
            "f1_at_k": 0.0,
            "hit_rate": 0.0,
            "mrr": 0.0,
            "ndcg_at_5": 0.0
          },
          "groundedness_score": 1.0,
          "confidence_score": 0.92,
          "has_hallucination": false,
          "final_answer": "Self-attention (also called intra-attention) is defined as:\n\n> \u201can attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.\u201d\n\nIn the Transformer:\n\n- **Self-attention in the encoder**: all keys, values, and queries come from the same place\u2014the output of the previous encoder layer\u2014so each position in the encoder can attend to all positions in that layer.\n- **Self-attention in the decoder**: similarly uses the decoder\u2019s own previous-layer outputs as keys, values, and queries, but each position can only attend to positions up to and including itself.\n\nThis differs from traditional **encoder\u2013decoder attention**, where:\n\n- The queries come from the decoder,\n- The keys and values come from the encoder output,\n\nallowing each decoder position to attend over all positions in the input sequence, rather than over positions within the same sequence.",
          "ground_truth_answer": "Self-attention is an attention mechanism where the queries, keys, and values all come from the same source sequence, allowing the model to relate different positions within a single sequence. Unlike traditional attention mechanisms that attend from one sequence to another (like encoder-decoder attention), self-attention computes relationships between all positions in the same sequence, enabling the model to capture dependencies regardless of their distance in the sequence.",
          "semantic_similarity": 1.0,
          "factual_accuracy": 1.0,
          "completeness": 1.0,
          "answer_relevance_score": 0.6757713689867723,
          "is_answer_relevant": false,
          "relevance_category": "low"
        },
        {
          "example_id": "attention_004",
          "question": "How does multi-head attention work and what are its benefits?",
          "difficulty": "medium",
          "query_type": "conceptual",
          "retrieval_metrics": {
            "recall_at_k": 0.6666666666666666,
            "precision_at_k": 0.4,
            "f1_at_k": 0.5,
            "hit_rate": 1.0,
            "mrr": 1.0,
            "ndcg_at_5": 0.8303010754742701
          },
          "groundedness_score": 1.0,
          "confidence_score": 0.94,
          "has_hallucination": false,
          "final_answer": "Multi-head attention works by running several attention layers (called \u201cheads\u201d) in parallel and then combining their results:\n\n- For each head i, the model linearly projects the same inputs into separate query, key, and value spaces using parameter matrices  \n  \\(W_i^Q \\in \\mathbb{R}^{d_{model} \\times d_k},\\; W_i^K \\in \\mathbb{R}^{d_{model} \\times d_k},\\; W_i^V \\in \\mathbb{R}^{d_{model} \\times d_v}\\).  \n- Each head computes Scaled Dot-Product Attention:\n  \\[\n  \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n  \\]\n- The outputs of all heads \\(\\text{head}_1, \\dots, \\text{head}_h\\) are concatenated and then projected with \\(W^O \\in \\mathbb{R}^{h d_v \\times d_{model}}\\):\n  \\[\n  \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\dots,\\text{head}_h) W^O\n  \\]\n\nBenefits described in the context:\n\n- Multi-head attention allows the model to **jointly attend to information from different representation subspaces at different positions**; with a single attention head, averaging inhibits this.\n- By using reduced dimensions per head (\\(d_k = d_v = d_{model}/h\\)), the **total computational cost is similar to that of single-head attention** with full dimensionality.\n- It can be applied in different ways within the Transformer:\n  - **Encoder-decoder attention**: queries from the decoder, keys/values from the encoder output, so each decoder position can attend to all input positions.\n  - **Encoder self-attention**: queries, keys, and values all from the encoder\u2019s previous layer, so each encoder position can attend to all positions in the previous encoder layer.\n  - **Decoder self-attention**: each decoder position can attend to positions in the decoder up to and including that position.",
          "ground_truth_answer": "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Instead of performing a single attention function, multi-head attention projects the queries, keys, and values h times with different learned linear projections. Each head performs attention independently, and the outputs are concatenated and linearly transformed. This mechanism allows the model to capture different types of relationships and attend to different parts of the representation space simultaneously, improving the model's ability to focus on different aspects of the input.",
          "semantic_similarity": 1.0,
          "factual_accuracy": 1.0,
          "completeness": 1.0,
          "answer_relevance_score": 0.7662824132426613,
          "is_answer_relevant": true,
          "relevance_category": "medium"
        },
        {
          "example_id": "attention_005",
          "question": "Explain the complete forward pass through the Transformer encoder, including all sub-layers and their connections.",
          "difficulty": "hard",
          "query_type": "procedural",
          "retrieval_metrics": {
            "recall_at_k": 0.5,
            "precision_at_k": 0.4,
            "f1_at_k": 0.4444444444444445,
            "hit_rate": 1.0,
            "mrr": 1.0,
            "ndcg_at_5": 0.6354662716208254
          },
          "groundedness_score": 0.6666666666666666,
          "confidence_score": 0.75,
          "has_hallucination": true,
          "final_answer": "The Transformer encoder processes an input sequence as follows:\n\n1. **Input embeddings and dimensionality**  \n   - The input tokens are mapped to embedding vectors.  \n   - All embedding layers and all encoder sub-layers produce outputs of the same dimension \\( d_{\\text{model}} = 512 \\).  \n   - This shared dimensionality is used to facilitate the residual connections.\n\n2. **Encoder structure**  \n   - The encoder is a stack of \\( N = 6 \\) identical layers.  \n   - Each encoder layer has **two sub-layers**:\n     1. A **multi-head self-attention** mechanism.  \n     2. A **position-wise fully connected feed-forward network**.\n\n3. **Within each encoder layer**  \n   Let \\( x \\) be the input to a sub-layer (either from the previous layer or from the embeddings for the first layer).\n\n   1. **First sub-layer: multi-head self-attention**\n      - This is a *self-attention* layer: all queries, keys, and values come from the same source\u2014the output of the previous layer in the encoder.\n      - Each position in the encoder can attend to all positions in the previous layer of the encoder.\n      - A residual connection is applied around this sub-layer, followed by layer normalization:\n        \\[\n        \\text{output} = \\text{LayerNorm}(x + \\text{SelfAttention}(x))\n        \\]\n\n   2. **Second sub-layer: position-wise feed-forward network**\n      - A simple, position-wise fully connected feed-forward network is applied to each position independently.\n      - Again, a residual connection is applied around this sub-layer, followed by layer normalization:\n        \\[\n        \\text{output} = \\text{LayerNorm}(x + \\text{FeedForward}(x))\n        \\]\n\n4. **Stacking of layers**  \n   - The output of the first sub-layer (self-attention + residual + LayerNorm) becomes the input to the second sub-layer (feed-forward + residual + LayerNorm) within the same layer.  \n   - The output of the second sub-layer in layer \\( i \\) becomes the input \\( x \\) to layer \\( i+1 \\).  \n   - This process is repeated through all 6 encoder layers.\n\nThe final output of the encoder is the output of the second sub-layer of the top (6th) encoder layer, after its residual connections and layer normalization.",
          "ground_truth_answer": "The Transformer encoder consists of N=6 identical layers. Each layer has two sub-layers: (1) a multi-head self-attention mechanism, and (2) a position-wise fully connected feed-forward network. A residual connection is applied around each sub-layer, followed by layer normalization. The forward pass works as follows: First, the input embeddings are added with positional encodings. Then, for each of the 6 encoder layers: the input passes through multi-head self-attention (allowing positions to attend to all positions), the output is added to the input via residual connection and normalized, then it passes through a feed-forward network (two linear transformations with ReLU activation), and again a residual connection and normalization are applied. The output of the final encoder layer is then passed to the decoder.",
          "semantic_similarity": 0.92,
          "factual_accuracy": 0.95,
          "completeness": 0.75,
          "answer_relevance_score": 0.6839628205148575,
          "is_answer_relevant": false,
          "relevance_category": "low"
        }
      ],
      "total_examples": 5,
      "successful_evaluations": 5,
      "tier": "premium",
      "tier_config": {
        "name": "Premium",
        "daily_cost": 12060,
        "quality_narrative": "88-92%",
        "models": "GPT-5.1 + GPT-5-mini + GPT-5-nano",
        "targets": {
          "f1_at_k": [
            0.35,
            0.5
          ],
          "groundedness": [
            0.95,
            1.0
          ],
          "confidence": [
            0.85,
            0.95
          ]
        }
      },
      "execution_time_seconds": 454.2442932128906,
      "examples_evaluated": 5,
      "avg_time_per_example": 90.84885864257812
    }
  }
}