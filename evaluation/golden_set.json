[
  {
    "id": "attention_001",
    "question": "How many attention heads are used in the base Transformer model?",
    "ground_truth_answer": "The base Transformer model uses 8 attention heads (h=8). This is specified in the model configuration where the base model has dmodel=512 and uses 8 parallel attention heads, with each head operating on dk=dv=64 dimensions.",
    "relevant_doc_ids": [
      "Attention Is All You Need.pdf_chunk_34",
      "Attention Is All You Need.pdf_chunk_17"
    ],
    "relevance_grades": {
      "Attention Is All You Need.pdf_chunk_34": 3,
      "Attention Is All You Need.pdf_chunk_17": 2
    },
    "source_document": "Attention Is All You Need.pdf",
    "difficulty": "easy",
    "query_type": "factual",
    "domain": "nlp",
    "expected_strategy": "keyword"
  },
  {
    "id": "attention_002",
    "question": "What is the dimensionality of the model (dmodel) in the base Transformer?",
    "ground_truth_answer": "The base Transformer model uses dmodel=512. This is the dimensionality of the input and output of all sub-layers in the model, including the attention layers and feed-forward networks.",
    "relevant_doc_ids": [
      "Attention Is All You Need.pdf_chunk_34",
      "Attention Is All You Need.pdf_chunk_17"
    ],
    "relevance_grades": {
      "Attention Is All You Need.pdf_chunk_34": 3,
      "Attention Is All You Need.pdf_chunk_17": 2
    },
    "source_document": "Attention Is All You Need.pdf",
    "difficulty": "easy",
    "query_type": "factual",
    "domain": "nlp",
    "expected_strategy": "keyword"
  },
  {
    "id": "attention_003",
    "question": "What is self-attention and how does it differ from traditional attention?",
    "ground_truth_answer": "Self-attention is an attention mechanism where the queries, keys, and values all come from the same source sequence, allowing the model to relate different positions within a single sequence. Unlike traditional attention mechanisms that attend from one sequence to another (like encoder-decoder attention), self-attention computes relationships between all positions in the same sequence, enabling the model to capture dependencies regardless of their distance in the sequence.",
    "relevant_doc_ids": [
      "Attention Is All You Need.pdf_chunk_12",
      "Attention Is All You Need.pdf_chunk_13",
      "Attention Is All You Need.pdf_chunk_14"
    ],
    "relevance_grades": {
      "Attention Is All You Need.pdf_chunk_12": 3,
      "Attention Is All You Need.pdf_chunk_13": 3,
      "Attention Is All You Need.pdf_chunk_14": 2
    },
    "source_document": "Attention Is All You Need.pdf",
    "difficulty": "medium",
    "query_type": "conceptual",
    "domain": "nlp",
    "expected_strategy": "semantic"
  },
  {
    "id": "attention_004",
    "question": "How does multi-head attention work and what are its benefits?",
    "ground_truth_answer": "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Instead of performing a single attention function, multi-head attention projects the queries, keys, and values h times with different learned linear projections. Each head performs attention independently, and the outputs are concatenated and linearly transformed. This mechanism allows the model to capture different types of relationships and attend to different parts of the representation space simultaneously, improving the model's ability to focus on different aspects of the input.",
    "relevant_doc_ids": [
      "Attention Is All You Need.pdf_chunk_16",
      "Attention Is All You Need.pdf_chunk_17",
      "Attention Is All You Need.pdf_chunk_18"
    ],
    "relevance_grades": {
      "Attention Is All You Need.pdf_chunk_16": 3,
      "Attention Is All You Need.pdf_chunk_17": 3,
      "Attention Is All You Need.pdf_chunk_18": 2
    },
    "source_document": "Attention Is All You Need.pdf",
    "difficulty": "medium",
    "query_type": "conceptual",
    "domain": "nlp",
    "expected_strategy": "semantic"
  },
  {
    "id": "attention_005",
    "question": "Explain the complete forward pass through the Transformer encoder, including all sub-layers and their connections.",
    "ground_truth_answer": "The Transformer encoder consists of N=6 identical layers. Each layer has two sub-layers: (1) a multi-head self-attention mechanism, and (2) a position-wise fully connected feed-forward network. A residual connection is applied around each sub-layer, followed by layer normalization. The forward pass works as follows: First, the input embeddings are added with positional encodings. Then, for each of the 6 encoder layers: the input passes through multi-head self-attention (allowing positions to attend to all positions), the output is added to the input via residual connection and normalized, then it passes through a feed-forward network (two linear transformations with ReLU activation), and again a residual connection and normalization are applied. The output of the final encoder layer is then passed to the decoder.",
    "relevant_doc_ids": [
      "Attention Is All You Need.pdf_chunk_10",
      "Attention Is All You Need.pdf_chunk_11",
      "Attention Is All You Need.pdf_chunk_12",
      "Attention Is All You Need.pdf_chunk_19"
    ],
    "relevance_grades": {
      "Attention Is All You Need.pdf_chunk_10": 3,
      "Attention Is All You Need.pdf_chunk_11": 3,
      "Attention Is All You Need.pdf_chunk_12": 2,
      "Attention Is All You Need.pdf_chunk_19": 2
    },
    "source_document": "Attention Is All You Need.pdf",
    "difficulty": "hard",
    "query_type": "procedural",
    "domain": "nlp",
    "expected_strategy": "semantic"
  },
  {
    "id": "bert_001",
    "question": "What does BERT stand for?",
    "ground_truth_answer": "BERT stands for Bidirectional Encoder Representations from Transformers. The name reflects the model's key innovation of using bidirectional training of Transformers, as opposed to previous models that used either left-to-right or combined left-to-right and right-to-left training.",
    "relevant_doc_ids": [
      "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_0",
      "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_1"
    ],
    "relevance_grades": {
      "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_0": 3,
      "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_1": 2
    },
    "source_document": "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf",
    "difficulty": "easy",
    "query_type": "factual",
    "domain": "nlp",
    "expected_strategy": "keyword"
  },
  {
    "id": "bert_002",
    "question": "How many parameters does BERT-Base have?",
    "ground_truth_answer": "BERT-Base has approximately 110 million parameters. The model has 12 layers (transformer blocks), a hidden size of 768, and 12 attention heads, resulting in a total of 110M parameters.",
    "relevant_doc_ids": [
      "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_12",
      "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_13"
    ],
    "relevance_grades": {
      "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_12": 3,
      "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_13": 2
    },
    "source_document": "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf",
    "difficulty": "easy",
    "query_type": "factual",
    "domain": "nlp",
    "expected_strategy": "keyword"
  },
  {
    "id": "bert_003",
    "question": "What is masked language modeling (MLM) and how does BERT use it?",
    "ground_truth_answer": "Masked Language Modeling (MLM) is BERT's primary pre-training objective where random tokens in the input are masked, and the model must predict the original tokens based on the context from both directions. Specifically, 15% of input tokens are randomly selected for masking. Of these selected tokens, 80% are replaced with [MASK], 10% are replaced with random tokens, and 10% are left unchanged. The model then predicts the original token for all masked positions. This approach allows BERT to learn deep bidirectional representations by conditioning on both left and right context, unlike traditional language models that only use left context.",
    "relevant_doc_ids": [
      "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_14",
      "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_15",
      "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_16"
    ],
    "relevance_grades": {
      "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_14": 3,
      "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_15": 3,
      "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_16": 2
    },
    "source_document": "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf",
    "difficulty": "medium",
    "query_type": "conceptual",
    "domain": "nlp",
    "expected_strategy": "semantic"
  },
  {
    "id": "bert_004",
    "question": "What is the Next Sentence Prediction (NSP) task in BERT?",
    "ground_truth_answer": "Next Sentence Prediction (NSP) is BERT's second pre-training objective, designed to help the model understand relationships between sentences. During training, the model receives pairs of sentences and must predict whether the second sentence actually follows the first in the original document. 50% of the time, the second sentence is the actual next sentence (labeled as 'IsNext'), and 50% of the time, it's a random sentence from the corpus (labeled as 'NotNext'). This task helps BERT learn relationships important for downstream tasks like question answering and natural language inference, where understanding sentence relationships is crucial.",
    "relevant_doc_ids": [
      "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_17",
      "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_18"
    ],
    "relevance_grades": {
      "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_17": 3,
      "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_18": 2
    },
    "source_document": "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf",
    "difficulty": "medium",
    "query_type": "conceptual",
    "domain": "nlp",
    "expected_strategy": "semantic"
  },
  {
    "id": "bert_005",
    "question": "How does BERT's pre-training approach differ from traditional autoregressive language models like GPT?",
    "ground_truth_answer": "BERT differs fundamentally from autoregressive language models like GPT in its use of bidirectional context. While autoregressive models like GPT are trained to predict the next token based only on previous (left) context, BERT uses masked language modeling to condition on both left and right context simultaneously. This is achieved by randomly masking tokens and predicting them based on surrounding context in both directions. Additionally, BERT uses the encoder-only Transformer architecture, while GPT uses the decoder architecture. This bidirectional training allows BERT to achieve better performance on tasks requiring deep understanding of context, though it comes at the cost of not being directly usable for text generation tasks that require left-to-right generation.",
    "relevant_doc_ids": [
      "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_5",
      "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_6",
      "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_14"
    ],
    "relevance_grades": {
      "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_5": 3,
      "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_6": 2,
      "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf_chunk_14": 3
    },
    "source_document": "BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf",
    "difficulty": "hard",
    "query_type": "comparative",
    "domain": "nlp",
    "expected_strategy": "semantic"
  },
  {
    "id": "ddpm_001",
    "question": "What is the forward diffusion process in DDPM?",
    "ground_truth_answer": "The forward diffusion process in DDPM is a fixed Markov chain that gradually adds Gaussian noise to data over T timesteps. Starting from the original data x0, noise is added according to a variance schedule βt, producing a sequence x1, x2, ..., xT. At each step t, noise is added according to q(xt|xt-1) = N(xt; √(1-βt)xt-1, βtI). The process is designed so that the final distribution q(xT|x0) approximates an isotropic Gaussian distribution.",
    "relevant_doc_ids": [
      "Denoising Diffusion Probabilistic Models.pdf_chunk_8",
      "Denoising Diffusion Probabilistic Models.pdf_chunk_9",
      "Denoising Diffusion Probabilistic Models.pdf_chunk_10"
    ],
    "relevance_grades": {
      "Denoising Diffusion Probabilistic Models.pdf_chunk_8": 3,
      "Denoising Diffusion Probabilistic Models.pdf_chunk_9": 3,
      "Denoising Diffusion Probabilistic Models.pdf_chunk_10": 2
    },
    "source_document": "Denoising Diffusion Probabilistic Models.pdf",
    "difficulty": "easy",
    "query_type": "factual",
    "domain": "generative_models",
    "expected_strategy": "semantic"
  },
  {
    "id": "ddpm_002",
    "question": "How many diffusion steps (T) are typically used in DDPM?",
    "ground_truth_answer": "DDPM typically uses T=1000 diffusion steps. This relatively large number of steps allows for a gradual noise addition process in the forward diffusion and enables high-quality sample generation during the reverse process.",
    "relevant_doc_ids": [
      "Denoising Diffusion Probabilistic Models.pdf_chunk_20",
      "Denoising Diffusion Probabilistic Models.pdf_chunk_21"
    ],
    "relevance_grades": {
      "Denoising Diffusion Probabilistic Models.pdf_chunk_20": 3,
      "Denoising Diffusion Probabilistic Models.pdf_chunk_21": 2
    },
    "source_document": "Denoising Diffusion Probabilistic Models.pdf",
    "difficulty": "easy",
    "query_type": "factual",
    "domain": "generative_models",
    "expected_strategy": "keyword"
  },
  {
    "id": "ddpm_003",
    "question": "What is the variational lower bound in DDPM and why is it important?",
    "ground_truth_answer": "The variational lower bound (also called ELBO - Evidence Lower BOund) in DDPM is the objective function used to train the model. It provides a tractable way to optimize the likelihood of the data by breaking down the reverse process into manageable KL divergence terms. The bound is expressed as the negative log-likelihood minus the KL divergence between the forward and reverse processes. This formulation allows DDPM to be trained by matching the learned reverse distributions to the tractable forward process posteriors, making training stable and effective without requiring adversarial training or complex inference procedures.",
    "relevant_doc_ids": [
      "Denoising Diffusion Probabilistic Models.pdf_chunk_11",
      "Denoising Diffusion Probabilistic Models.pdf_chunk_12",
      "Denoising Diffusion Probabilistic Models.pdf_chunk_13"
    ],
    "relevance_grades": {
      "Denoising Diffusion Probabilistic Models.pdf_chunk_11": 3,
      "Denoising Diffusion Probabilistic Models.pdf_chunk_12": 3,
      "Denoising Diffusion Probabilistic Models.pdf_chunk_13": 2
    },
    "source_document": "Denoising Diffusion Probabilistic Models.pdf",
    "difficulty": "medium",
    "query_type": "conceptual",
    "domain": "generative_models",
    "expected_strategy": "semantic"
  },
  {
    "id": "ddpm_004",
    "question": "How does the reverse diffusion process work in DDPM?",
    "ground_truth_answer": "The reverse diffusion process in DDPM is a learned Markov chain that gradually denoises data, starting from pure Gaussian noise xT and progressively removing noise to generate a sample x0. At each timestep t, the reverse process is modeled as pθ(xt-1|xt) = N(xt-1; μθ(xt, t), Σθ(xt, t)), where μθ is a neural network that predicts the mean of the Gaussian distribution. The process moves backward through time from T to 0, with the neural network learning to predict the less noisy version at each step. This reverse process is trained to match the posterior of the forward process, and during generation, it transforms random noise into realistic samples from the data distribution.",
    "relevant_doc_ids": [
      "Denoising Diffusion Probabilistic Models.pdf_chunk_8",
      "Denoising Diffusion Probabilistic Models.pdf_chunk_14",
      "Denoising Diffusion Probabilistic Models.pdf_chunk_15"
    ],
    "relevance_grades": {
      "Denoising Diffusion Probabilistic Models.pdf_chunk_8": 2,
      "Denoising Diffusion Probabilistic Models.pdf_chunk_14": 3,
      "Denoising Diffusion Probabilistic Models.pdf_chunk_15": 3
    },
    "source_document": "Denoising Diffusion Probabilistic Models.pdf",
    "difficulty": "medium",
    "query_type": "conceptual",
    "domain": "generative_models",
    "expected_strategy": "semantic"
  },
  {
    "id": "ddpm_005",
    "question": "Explain the mathematical derivation of the simplified training objective in DDPM and why it works better than the full variational bound.",
    "ground_truth_answer": "The simplified training objective in DDPM is derived from the variational lower bound but focuses on predicting the noise εt rather than the mean directly. The full variational bound can be reweighted and simplified to L_simple = E[||ε - εθ(xt, t)||²], where ε is the noise added at timestep t, and εθ is the neural network's prediction. This formulation is equivalent to denoising score matching and has several advantages: (1) It removes complicated weighting terms from the full bound that can lead to optimization difficulties, (2) It directly optimizes for the quality of individual denoising steps rather than the overall likelihood, (3) Empirically, it produces better sample quality despite being a less tight bound, (4) It provides a clearer training signal by focusing on the core task of noise prediction. The simplified objective essentially trades off some likelihood optimization for improved perceptual quality of generated samples.",
    "relevant_doc_ids": [
      "Denoising Diffusion Probabilistic Models.pdf_chunk_13",
      "Denoising Diffusion Probabilistic Models.pdf_chunk_14",
      "Denoising Diffusion Probabilistic Models.pdf_chunk_16",
      "Denoising Diffusion Probabilistic Models.pdf_chunk_17"
    ],
    "relevance_grades": {
      "Denoising Diffusion Probabilistic Models.pdf_chunk_13": 3,
      "Denoising Diffusion Probabilistic Models.pdf_chunk_14": 3,
      "Denoising Diffusion Probabilistic Models.pdf_chunk_16": 2,
      "Denoising Diffusion Probabilistic Models.pdf_chunk_17": 2
    },
    "source_document": "Denoising Diffusion Probabilistic Models.pdf",
    "difficulty": "hard",
    "query_type": "procedural",
    "domain": "generative_models",
    "expected_strategy": "semantic"
  },
  {
    "id": "cross_001",
    "question": "How is 'attention' used differently in the Transformer architecture compared to Vision Transformer (ViT)?",
    "ground_truth_answer": "While both Transformers and Vision Transformers (ViT) use the same self-attention mechanism, they differ in how they handle input modalities. The original Transformer operates on sequences of word embeddings in NLP tasks, where each token represents a word or subword. In contrast, ViT adapts this architecture for images by dividing the image into fixed-size patches (typically 16x16 pixels), flattening each patch, and treating these flattened patches as 'tokens' analogous to words. Both use multi-head self-attention to capture relationships, but ViT's innovation is in treating image patches as a sequence, allowing the same attention mechanism to work across spatial dimensions of images rather than temporal sequences of text. Additionally, ViT adds a learnable classification token [CLS] similar to BERT's approach, and uses 2D positional embeddings to retain spatial information about patch locations.",
    "relevant_doc_ids": [
      "Attention Is All You Need.pdf_chunk_12",
      "Attention Is All You Need.pdf_chunk_16",
      "AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_5",
      "AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_6"
    ],
    "relevance_grades": {
      "Attention Is All You Need.pdf_chunk_12": 2,
      "Attention Is All You Need.pdf_chunk_16": 2,
      "AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_5": 3,
      "AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_6": 3
    },
    "source_document": [
      "Attention Is All You Need.pdf",
      "AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf"
    ],
    "difficulty": "medium",
    "query_type": "comparative",
    "domain": "cross_domain",
    "expected_strategy": "hybrid"
  },
  {
    "id": "cross_002",
    "question": "How does ViT adapt the Transformer architecture for computer vision tasks?",
    "ground_truth_answer": "Vision Transformer (ViT) adapts the standard Transformer encoder architecture for images through several key modifications: (1) Image Patching: Images are divided into fixed-size patches (e.g., 16x16 pixels), which are then flattened and linearly embedded, effectively treating patches as tokens analogous to words in NLP. (2) Positional Embeddings: Since patches lose spatial information when flattened, learnable 2D positional embeddings are added to retain information about patch positions. (3) Class Token: A learnable [CLS] token is prepended to the sequence (similar to BERT), whose final representation is used for classification. (4) Pre-training Strategy: ViT is typically pre-trained on large image datasets (like ImageNet-21k or JFT-300M) before fine-tuning on specific tasks. The architecture uses the same multi-head self-attention and MLP blocks as the original Transformer, but applied to visual data rather than text sequences.",
    "relevant_doc_ids": [
      "AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_5",
      "AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_6",
      "AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_7",
      "Attention Is All You Need.pdf_chunk_10"
    ],
    "relevance_grades": {
      "AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_5": 3,
      "AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_6": 3,
      "AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf_chunk_7": 2,
      "Attention Is All You Need.pdf_chunk_10": 1
    },
    "source_document": [
      "AN IMAGE IS WORTH 16X16 WORDS - TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE.pdf",
      "Attention Is All You Need.pdf"
    ],
    "difficulty": "medium",
    "query_type": "conceptual",
    "domain": "computer_vision",
    "expected_strategy": "semantic"
  },
  {
    "id": "cross_003",
    "question": "Compare the diffusion process in DDPM versus Consistency Models.",
    "ground_truth_answer": "While both DDPM and Consistency Models are diffusion-based generative models, they differ fundamentally in their sampling approach. DDPM uses an iterative reverse diffusion process that requires hundreds or thousands of steps (typically T=1000) to gradually denoise from pure noise to a data sample, making sampling slow. Each step involves a neural network evaluation, resulting in significant computational cost. In contrast, Consistency Models learn to map any point on a diffusion trajectory directly to its origin (the clean data point), enabling single-step or few-step generation. Consistency Models achieve this through consistency training or consistency distillation, which enforces that the model's predictions are consistent across different noise levels along the same trajectory. This allows Consistency Models to generate high-quality samples in 1-2 steps instead of 1000 steps, providing a 100-1000x speedup while maintaining competitive sample quality.",
    "relevant_doc_ids": [
      "Denoising Diffusion Probabilistic Models.pdf_chunk_8",
      "Denoising Diffusion Probabilistic Models.pdf_chunk_14",
      "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_5",
      "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_6"
    ],
    "relevance_grades": {
      "Denoising Diffusion Probabilistic Models.pdf_chunk_8": 2,
      "Denoising Diffusion Probabilistic Models.pdf_chunk_14": 2,
      "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_5": 3,
      "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf_chunk_6": 3
    },
    "source_document": [
      "Denoising Diffusion Probabilistic Models.pdf",
      "Consistency Models - faster alternative to traditional diffusion models; directly mapping noise to data.pdf"
    ],
    "difficulty": "hard",
    "query_type": "comparative",
    "domain": "generative_models",
    "expected_strategy": "hybrid"
  },
  {
    "id": "cross_004",
    "question": "How does RAPTOR improve upon standard RAG retrieval strategies?",
    "ground_truth_answer": "RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval) improves upon standard RAG by introducing hierarchical document organization and multi-level retrieval. While traditional RAG retrieves similar chunks at a single granularity level (typically fixed-size chunks), RAPTOR builds a tree structure by recursively summarizing and clustering document chunks. This creates multiple levels of abstraction: leaf nodes contain original text chunks, while higher-level nodes contain summaries of clustered content. During retrieval, RAPTOR can access both specific details (from leaf nodes) and high-level thematic information (from summary nodes), enabling better handling of queries that require either detailed facts or broad conceptual understanding. This hierarchical approach is particularly effective for long documents and complex queries that benefit from multi-scale context, improving retrieval quality by 10-20% on tasks requiring integration of information across document sections.",
    "relevant_doc_ids": [
      "Expanding Horizons in RAG - Exploring and Extending the Limits of RAPTOR.pdf_chunk_3",
      "Expanding Horizons in RAG - Exploring and Extending the Limits of RAPTOR.pdf_chunk_4",
      "Expanding Horizons in RAG - Exploring and Extending the Limits of RAPTOR.pdf_chunk_5"
    ],
    "relevance_grades": {
      "Expanding Horizons in RAG - Exploring and Extending the Limits of RAPTOR.pdf_chunk_3": 3,
      "Expanding Horizons in RAG - Exploring and Extending the Limits of RAPTOR.pdf_chunk_4": 3,
      "Expanding Horizons in RAG - Exploring and Extending the Limits of RAPTOR.pdf_chunk_5": 2
    },
    "source_document": "Expanding Horizons in RAG - Exploring and Extending the Limits of RAPTOR.pdf",
    "difficulty": "medium",
    "query_type": "conceptual",
    "domain": "rag",
    "expected_strategy": "semantic"
  },
  {
    "id": "cross_005",
    "question": "Compare the generative modeling approaches of DDPM and WGAN-GP. What are the fundamental differences?",
    "ground_truth_answer": "DDPM (Denoising Diffusion Probabilistic Models) and WGAN-GP (Wasserstein GAN with Gradient Penalty) represent fundamentally different paradigms for generative modeling. DDPM uses a diffusion process: it gradually adds noise to data (forward process) and learns to reverse this process (backward/generative process), generating samples through iterative denoising over many steps (typically 1000). Training is stable and uses a simple L2 loss between predicted and actual noise. In contrast, WGAN-GP uses adversarial training with two neural networks: a generator that creates samples and a critic (discriminator) that distinguishes real from fake samples. The Wasserstein distance with gradient penalty provides stable training by enforcing Lipschitz constraints on the critic. Key differences: (1) Training: DDPM uses likelihood-based training while WGAN-GP uses adversarial min-max optimization, (2) Sampling: DDPM requires many iterative steps while GANs generate in a single forward pass, (3) Stability: DDPM training is generally more stable while GANs can suffer from mode collapse, (4) Diversity: DDPM tends to produce more diverse samples while GANs can generate sharper images but with potentially less diversity.",
    "relevant_doc_ids": [
      "Denoising Diffusion Probabilistic Models.pdf_chunk_8",
      "Denoising Diffusion Probabilistic Models.pdf_chunk_14",
      "Improved Training of Wasserstein GANs.pdf_chunk_3",
      "Improved Training of Wasserstein GANs.pdf_chunk_4",
      "Improved Training of Wasserstein GANs.pdf_chunk_5"
    ],
    "relevance_grades": {
      "Denoising Diffusion Probabilistic Models.pdf_chunk_8": 2,
      "Denoising Diffusion Probabilistic Models.pdf_chunk_14": 2,
      "Improved Training of Wasserstein GANs.pdf_chunk_3": 3,
      "Improved Training of Wasserstein GANs.pdf_chunk_4": 3,
      "Improved Training of Wasserstein GANs.pdf_chunk_5": 2
    },
    "source_document": [
      "Denoising Diffusion Probabilistic Models.pdf",
      "Improved Training of Wasserstein GANs.pdf"
    ],
    "difficulty": "hard",
    "query_type": "comparative",
    "domain": "generative_models",
    "expected_strategy": "semantic"
  }
]
